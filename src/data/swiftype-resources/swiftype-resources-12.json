{
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-application-gateway-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45828,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53195,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52876,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-containers-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.4582,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53189,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.5287,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.4582,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53189,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.5287,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-cost-management-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45807,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52866,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-data-factory-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45807,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52866,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45807,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Cost Management monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Event data"
      ],
      "title": "Azure Cost Management monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "a4cf35715fefc93b9938c32e7133deb1aaca9928",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-cost-management-monitoring-integration/",
      "published_at": "2021-05-22T06:16:43Z",
      "updated_at": "2021-03-16T04:36:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Cost Management data to New Relic. Here we explain how to activate the integration and what data it collects. Features New Relic's Azure Cost Management integration collects your accumulated monthly expenditures for the Azure resources in your subscription, grouped by service, location, resource group, or tag. Our integration allows you to track how costs evolve for each billing period and detect unexpected trends, or alert on accumulated costs for the current billing period. Activate integration Follow standard procedures to activate your Azure service in New Relic. In some cases, the Azure Cost Management integration requires you to add an additional role to fetch billing data: In the Azure Portal Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add. From the Role dropdown, select Billing Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Cost Management integration: Default polling frequency: 1 hour Maximum recommended polling frequency: 1 hour Resolution: 1 day Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureCostManagementSample event type. The provider metadata value indicates how the daily cost is grouped: AzureCostLocation: Costs are grouped by location. AzureCostService: Costs are grouped by cloud service. AzureCostResourceGroup: Costs are grouped by resource group. AzureCostTag: Costs are grouped by a resource tag, indicated in the label.<tag_key> metadata of the same event. Untagged resources will not be collected. In order to collect costs grouped by tag, specify the tag key in the integration settings. Costs are estimates, as Microsoft can make updates to the cost data up until the invoice for the billing period is generated. To get the most accurate value of the accumulated cost in the billing period, use the max() function in your NRQL queries. Event data Here are the metrics and metadata reported by the Azure Cost Management integration. For more information about how data is structured and reported to New Relic, see Understand and use integration data. Metric Description currency USD cost Amount spent for the day of the event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.70009,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cost Management monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cost Management monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Cost Management data to New Relic. Here we explain how to activate the integration and what data it collects. Features New Relic&#x27;s <em>Azure</em> Cost Management integration collects your accumulated monthly expenditures"
      },
      "id": "603ea1d0e7b9d230242a07e9"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45795,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.5286,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    },
    {
      "sections": [
        "Azure Cost Management monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Event data"
      ],
      "title": "Azure Cost Management monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "a4cf35715fefc93b9938c32e7133deb1aaca9928",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-cost-management-monitoring-integration/",
      "published_at": "2021-05-22T06:16:43Z",
      "updated_at": "2021-03-16T04:36:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Cost Management data to New Relic. Here we explain how to activate the integration and what data it collects. Features New Relic's Azure Cost Management integration collects your accumulated monthly expenditures for the Azure resources in your subscription, grouped by service, location, resource group, or tag. Our integration allows you to track how costs evolve for each billing period and detect unexpected trends, or alert on accumulated costs for the current billing period. Activate integration Follow standard procedures to activate your Azure service in New Relic. In some cases, the Azure Cost Management integration requires you to add an additional role to fetch billing data: In the Azure Portal Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add. From the Role dropdown, select Billing Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Cost Management integration: Default polling frequency: 1 hour Maximum recommended polling frequency: 1 hour Resolution: 1 day Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureCostManagementSample event type. The provider metadata value indicates how the daily cost is grouped: AzureCostLocation: Costs are grouped by location. AzureCostService: Costs are grouped by cloud service. AzureCostResourceGroup: Costs are grouped by resource group. AzureCostTag: Costs are grouped by a resource tag, indicated in the label.<tag_key> metadata of the same event. Untagged resources will not be collected. In order to collect costs grouped by tag, specify the tag key in the integration settings. Costs are estimates, as Microsoft can make updates to the cost data up until the invoice for the billing period is generated. To get the most accurate value of the accumulated cost in the billing period, use the max() function in your NRQL queries. Event data Here are the metrics and metadata reported by the Azure Cost Management integration. For more information about how data is structured and reported to New Relic, see Understand and use integration data. Metric Description currency USD cost Amount spent for the day of the event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.70009,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cost Management monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cost Management monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Cost Management data to New Relic. Here we explain how to activate the integration and what data it collects. Features New Relic&#x27;s <em>Azure</em> Cost Management integration collects your accumulated monthly expenditures"
      },
      "id": "603ea1d0e7b9d230242a07e9"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-postgresql-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45795,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53178,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.5286,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-event-hub-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45782,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53174,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52853,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-express-route-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45782,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53174,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52853,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-firewalls-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.4577,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53168,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52847,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-front-door-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.4577,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53168,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52847,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-functions-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.4577,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53168,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52847,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-key-vault-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45758,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53162,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-load-balancer-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45758,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53162,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-logic-apps-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45746,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53156,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52838,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-machine-learning-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45746,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53156,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52838,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-power-bi-dedicated-capacities-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45734,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53151,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52832,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-redis-cache-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45734,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53151,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52832,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-bus-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45724,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53146,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52826,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45724,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53146,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52826,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45724,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53146,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52826,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-managed-instances-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45712,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.5314,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.5282,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-storage-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45712,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.5314,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.5282,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machine-scale-sets-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.457,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53134,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52815,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machines-scale-sets-monitoring-integration": [
    {
      "sections": [
        "Azure virtual machine scale sets monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Virtual machine scale sets ScaleSet data"
      ],
      "title": "Azure virtual machine scale sets monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2b25b6720032817e09a6e844210f020b3a4fc98b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machine-scale-sets-monitoring-integration/",
      "published_at": "2021-05-21T23:39:49Z",
      "updated_at": "2021-03-16T04:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Azure virtual machine scale sets data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure virtual machine scale sets integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. You can query and explore your data using the following event type: Entity Event type Provider ScaleSet AzureVirtualMachineScaleSetSample AzureVirtualMachineScaleSet For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure virtual machine scale sets data for ScaleSet. Virtual machine scale sets ScaleSet data Metric Unit Description cpuPercent Percent The percentage of allocated compute units that are currently in use by the Virtual Machine(s) networkInBytes Bytes The number of billable bytes received on all network interfaces by the Virtual Machine(s) (Incoming Traffic) networkOutBytes Bytes The number of billable bytes out on all network interfaces by the Virtual Machine(s) (Outgoing Traffic) diskReadBytes Bytes Bytes read from disk during monitoring period diskWriteBytes Bytes Bytes written to disk during monitoring period diskReadOperationsCountPerSecond CountPerSecond Disk Read IOPS diskWriteOperationsCountPerSecond CountPerSecond Disk Write IOPS cpuCreditsRemaining Count Total number of credits available to burst cpuCreditsConsumed Count Total number of credits consumed by the Virtual Machine dataDiskReadBytesCountPerSecond CountPerSecond Bytes/Sec read from a single disk during monitoring period dataDiskWriteBytesCountPerSecond CountPerSecond Bytes/Sec written to a single disk during monitoring period dataDiskReadOperationsCountPerSecond CountPerSecond Read IOPS from a single disk during monitoring period dataDiskWriteOperationsCountPerSecond CountPerSecond Write IOPS from a single disk during monitoring period dataDiskQueueDepth Count Data Disk Queue Depth(or Queue Length) osDiskReadBytesCountPerSecond CountPerSecond Bytes/Sec read from a single disk during monitoring period for OS disk osDiskWriteBytesCountPerSecond CountPerSecond Bytes/Sec written to a single disk during monitoring period for OS disk osDiskReadOperationsCountPerSecond CountPerSecond Read IOPS from a single disk during monitoring period for OS disk osDiskWriteOperationsCountPerSecond CountPerSecond Write IOPS from a single disk during monitoring period for OS disk osDiskQueueDepth Count OS Disk Queue Depth(or Queue Length) inboundFlows Count Inbound Flows are number of current flows in the inbound direction (traffic going into the VM) outboundFlows Count Outbound Flows are number of current flows in the outbound direction (traffic going out of the VM) inboundFlowsMaximumCreationRateCountPerSecond CountPerSecond The maximum creation rate of inbound flows (traffic going into the VM) outboundFlowsMaximumCreationRateCountPerSecond CountPerSecond The maximum creation rate of outbound flows (traffic going out of the VM) premiumDataDiskCacheReadHitPercent Percent Premium Data Disk Cache Read Hit premiumDataDiskCacheReadMissPercent Percent Premium Data Disk Cache Read Miss premiumOSDiskCacheReadHitPercent Percent Premium OS Disk Cache Read Hit premiumOSDiskCacheReadMissPercent Percent Premium OS Disk Cache Read Miss networkInTotalBytes Bytes The number of bytes received on all network interfaces by the Virtual Machine(s) (Incoming Traffic) networkOutTotalBytes Bytes The number of bytes out on all network interfaces by the Virtual Machine(s) (Outgoing Traffic)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1266.466,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>virtual</em> <em>machine</em> <em>scale</em> <em>sets</em> <em>monitoring</em> <em>integration</em>",
        "sections": "<em>Azure</em> <em>virtual</em> <em>machine</em> <em>scale</em> <em>sets</em> <em>monitoring</em> <em>integration</em>",
        "tags": "Microsoft <em>Azure</em> <em>integrations</em>",
        "body": " data. Metric data This <em>integration</em> collects <em>Azure</em> <em>virtual</em> machine <em>scale</em> <em>sets</em> data for <em>ScaleSet</em>. <em>Virtual</em> machine <em>scale</em> <em>sets</em> <em>ScaleSet</em> data Metric Unit Description cpuPercent Percent The percentage of allocated compute units that are currently in use by the <em>Virtual</em> Machine(s) networkInBytes Bytes"
      },
      "id": "603ea1cfe7b9d2a8342a0819"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 235.25934,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs <em>monitoring</em> <em>integration</em>",
        "sections": "<em>Azure</em> VMs <em>monitoring</em> <em>integration</em>",
        "tags": "Microsoft <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>monitoring</em> provides an <em>integration</em> for Microsoft <em>Azure</em> <em>Virtual</em> <em>Machines</em> (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this <em>integration</em> and describes the data that can be captured. Features New Relic&#x27;s <em>integration</em>"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "VMware vSphere monitoring integration",
        "Why it matters",
        "Compatibility and requirements",
        "Important",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Tarball installation (advanced)",
        "Configure the integration",
        "Enable and configure performance metrics (Beta)",
        "Caution",
        "Tip",
        "Collect vSphere events (Beta)",
        "Collect snapshots data (Beta)",
        "Collect vSphere tags (Beta)",
        "Filter resources by tags (Beta)",
        "Example configuration",
        "Update your integration",
        "View and use data",
        "Metric data",
        "VSphereHost",
        "VSphereVm",
        "VSphereDatastore",
        "VSphereDatacenter",
        "VSphereResourcePool",
        "VSphereCluster",
        "VSphereSnapshotVm"
      ],
      "title": "VMware vSphere monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "bf1de0c493c7d9c977daab0d1445b16f0a0b9248",
      "image": "https://docs.newrelic.com/static/a5b511bbd58392771e7ee8472579bc01/8c557/infrastructure-ohi-vmware-vsphere_0.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-vsphere-monitoring-integration/",
      "published_at": "2021-05-22T06:11:08Z",
      "updated_at": "2021-05-16T07:21:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's VMware vSphere integration helps you understand the health and performance of your vSphere environment. You can: Query data to get insights on the performance on your hypervisors, virtual machines, and more. Go from high level views down to the most granular data. vSphere data visualized in a New Relic dashboard includes operating systems, status, average CPU and memory consumption, and more. Our integration uses the vSphere API to collect metrics and events generated by all vSphere's components, and forwards the data to our platform via the infrastructure agent. Why it matters With our vSphere integration you can: Instrument and monitor multiple vSphere instances using the same account. Collect data on snapshots, VMs, hosts, resource pools, clusters, and datastores, including tags. Monitor the health of your hypervisors and VMs using our charts and dashboards. Use the data retrieved to monitor key performance and key capacity scaling indicators. Set alerts based on any metrics collected from vCenter. Create workloads to group resources and focus on key data. You can create workloads using data collected via the vSphere integration. Compatibility and requirements Our integration is compatible with VMware vSphere 6.5 or higher. Before installing the integration, make sure that you meet the following requirements: Infrastructure agent installed on a host vCenter service account having at least read-only global permissions with the propagate to children option checked Important Large environments: In environments with more than 800 virtual machines, the integration cannot report all data and may fail. We offer a workaround that will preserve all metrics and events, but it will disable entity registration. To apply the workaround, add the following environment variable to the configuration file: EVENTS: true and METRICS: true Copy Install and activate To install the vSphere integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-vsphere. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp vsphere-config.yml.sample vsphere-config.yml Copy Edit the vsphere-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-vsphere MSI installer image from: download.newrelic.com/infrastructure_agent/windows/integrations/nri-vsphere/nri-vsphere-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-vsphere-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp vsphere-config.yml.sample vsphere-config.yml Copy Edit the vsphere-config.yml file as described in the configuration settings. Restart the infrastructure agent. Tarball installation (advanced) You can also install the integration from a tarball file. This gives you full control over the installation and configuration process. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. To configure the vSphere integration, you must define the URL of the vSphere API endpoint, and your vSphere username and password. For configuration examples, see the sample configuration files. Some vSphere integration features are optional and can be enabled via configuration settings. In addition, with secrets management, you can configure on-host integrations with New Relic's infrastructure monitoring agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. Enable and configure performance metrics (Beta) Performance metrics provide a better understanding of the current status of VMware resources and can be collected in addition to the metrics collected by default;and included in the samples;described at the bottom of the page. All metrics collected are included in the corresponding sample with the perf. prefix attached to the name. For example, net.packetsRx.summation is collected and sent as perf.net.packetsRx.summation. To collect vSphere performance metrics, use the ENABLE_VSPHERE_PERF_METRICS environment variable. Data is collected according to the settings in the vsphere-performance.metrics configuration file. You can override the location of the performance metrics config file using PERF_METRIC_FILE environment variable. Notice that the integration follows VMware's data collection levels (1 to 4). When ENABLE_VSPHERE_PERF_METRICS is set, all level 1 metrics are collected. The data collection level of the performance metrics collected can be modified using PERF_LEVEL. Each metric in the config file can be commented out and new ones can be added if needed. Caution Collection of performance data can increase the load in vCenter and the time needed by to collect data. We recommended to only include the metrics you need in the configuration file. To fine-tune data collection, the number of entities and metrics retrieved per request can be modified using BATCH_SIZE_PERF_ENTITIES and BATCH_SIZE_PERF_METRICS. Tip For more information on vSphere performance metrics, see the VMware documentation. Collect vSphere events (Beta) To collect vSphere events, use the ENABLE_VSPHERE_EVENTS environment variable. The integration collects events between the current time and the last fetched event for each datacenter. It stores the information regarding the last fetched event in a cache that is updated after each execution. Events are only available if the integration is connected to a vCenter and not directly to an ESXi host. The number of events collected per request can be tuned by modifying EVENTS_PAGE_SIZE, which is set to 100 by default. Events are available in the Events page and can be queried via NRQL as InfrastructureEvent under vSphereEvent. Here is an example of vSphere events data: \"summary\": \"User dcui@127.0.0.1 logged out (login time: Tuesday, 14 July, 2020 08:32:09 AM, number of API invocations: 0, user agent: VMware-client/6.5.0)\", \"vSphereEvent.computeResource\": \"cluster1\", \"vSphereEvent.datacenter\": \"Prod Datacenter\", \"vSphereEvent.date\": \"Tue, 14 Jul 2020 09:03:51 UTC\", \"vSphereEvent.host\": \"192.168.0.230\", \"vSphereEvent.userName\": \"dcui\" Copy Collect snapshots data (Beta) To collect snapshot data, use the ENABLE_VSPHERE_SNAPSHOTS environment variable. Snapshot data can be found in VSphereSnapshotVmSample. Collected data covers total and unique space occupied by disk and memory files, snapshot tree, and creation time. You can use this information to create NRQL queries, dashboards, and alerts, since it's linked to the corresponding virtual machine entity. Collect vSphere tags (Beta) To collect vSphere tags, use the ENABLE_VSPHERE_TAGS environment variable. Tags are available as attributes in the corresponding entity sample as label.tagCategory:tagName. If two tags of the same category are assigned to a resource, they are added to a unique attribute separated by a pipe character. For example: label.tagCategory:tagName|tagName2. Tags can be used to run NRQL queries, filter entities in the New Relic Explorer, and to create dashboards and alerts. Filter resources by tags (Beta) Resource filtering allows you to specify which resources you want to monitor by declaring a set of tags that resources must have in order to be monitored. Resources require a match on any (one or more) of the filter tags in order to be included. If none of the resource tags match any of the filter tags, no information about that resource is sent to New Relic. To use filtering resources by tag you need to have the ENABLE_VSPHERE_TAGS environment variable enabled. A tag filter expression is a space-separated list of pairs of strings with the format category=name. For example, to only retrieve resources with a tag category region and include regions us and eu use a filter expression like: region=us region=eu INCLUDE_TAGS: > region=us region=eu Copy To enable resource filtering by tag, edit your integration configuration file and add the option INCLUDE_TAGS with the filter expression you want. Caution Note that datacenter resources acting as the root of the resource tree MUST have tags attached AND match the filter expression in order for other child resources to be fetched. Important If you connect the integration directly to the ESXi host, vCenter data is not available (for example, events, tags, or datacenter metadata). Example configuration Here are examples of the vSphere integration configuration, including performance metrics: vsphere-config.yml.sample (Linux) vsphere-win-config.yml.sample (Windows) vsphere-performance.metrics (Performance metrics) For more information, see our documentation about the general structure of on-host integration configurations. Important The configuration option inventory_source is not compatible with this integration. Update your integration On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. View and use data Data from this service is reported to an integration dashboard. You can query this data for troubleshooting purposes or to create charts and dashboards. vSphere data is attached to these event types: VSphereHostSample VSphereClusterSample VSphereVmSample VSphereDatastoreSample VSphereDatacenterSample VSphereResourcePoolSample VSphereSnapshotVmSample Performance data is enabled and configured separately (see Enable and configure performance metrics). For more on how to view and use your data, see Understand integration data. Metric data The vSphere integration provides metric data attached to the following New Relic events: VSphereHost VSphereVm VSphereDatastore VSphereDatacenter VSphereResourcePool VSphereCluster VSphereSnapshotVm VSphereHost Name Description cpu.totalMHz Sum of the MHz for all the individual cores on the host cpu.coreMHz Speed of the CPU cores cpu.available Amount of free CPU MHz in the host cpu.overallUsage CPU usage across all cores on the host in MHz cpu.percent Percentage of CPU utilization in the host cpu.cores Number of physical CPU cores on the host. Physical CPU cores are the processors contained by a CPU package cpu.threads Number of physical CPU threads on the host disk.totalMiB Total capacity of disks mounted in host, in MiB mem.free Amount of available memory in the host, in MiB mem.usage Amount of used memory in the host, in MiB mem.size Total memory capacity of the host, in MiB vmCount Number of virtual machines in the host hypervisorHostname Name of the host uuid The hardware BIOS identification datacenterName Name of the datacenter related to the host clusterName Name of the cluster related to the host resourcePoolNameList List of names of the resource pools related to the host datastoreNameList List of names of datastores related to the host datacenterLocation Datacenter location networkNameList List of names of networks related to the host overallStatus gray: Status is unknown green: Entity is OK yellow: Entity might have a problem red: Entity definitely has a problem connectionState The host connection state: connected: Connected to the server. For ESX Server, this is the default setting. disconnected: The user has explicitly taken the host down. VirtualCenter does not expect to receive heartbeats from the host. The next time a heartbeat is received, the host is moved to the connected state again and an event is logged. notResponding: VirtualCenter is not receiving heartbeats from the server. The state automatically changes to connected once heartbeats are received again. This state is typically used to trigger an alarm on the host. inMaintenanceMode The flag to indicate whether or not the host is in maintenance mode. This flag is set when the host has entered the maintenance mode. It is not set during the entering phase of maintenance mode. inQuarantineMode The flag to indicate whether or not the host is in quarantine mode. InfraUpdateHa will recommend to set this flag based on the HealthUpdates received by the HealthUpdateProviders configured for the cluster. A host that is reported as degraded will be recommended to enter quarantine mode, while a host that is reported as healthy will be recommended to exit quarantine mode. Execution of these recommended actions will set this flag. Hosts in quarantine mode will be avoided by vSphere DRS as long as the increased consolidation in the cluster does not negatively affect VM performance. powerState The host power state: poweredOff: The host was specifically powered off by the user through VirtualCenter. This state is not a cetain state, because after VirtualCenter issues the command to power off the host, the host might crash, or kill all the processes but fail to power off. poweredOn: The host is powered on. A host that is entering standby mode entering is also in this state. standBy: The host was specifically put in standby mode, either explicitly by the user or automatically by DPM. This state is not a certain state, because after VirtualCenter issues the command to put the host in standby state, the host might crash, or kill all the processes but fail to power off. A host that is exiting standby mode s also in this state. unknown: If the host is disconnected or notResponding, we know its power state, so the host is marked as unknown. standbyMode The host’s standby mode. The property is only populated by vCenter server. If queried directly from the ESX host, the property is unset. entering: The host is entering standby mode. exiting: The host is exiting standby mode. in: The host is in standby mode. none: The host is not in standby mode, and it is not in the process of entering or exiting standby mode. cryptoState Encryption state of the host. Valid values are enumerated by the CryptoState type: incapable: The host is not safe for receiving sensitive material. prepared: The host is prepared for receiving sensitive material but does not have a host key set yet. safe: The host is crypto safe and has a host key set. bootTime The time when the host was booted. VSphereVm Name Description mem.size Memory size of the virtual machine, in MiB mem.usage Guest memory utilization statistics, in MiB. This is also known as active guest memory. The value can range between 0 and the configured memory size of the virtual machine. Valid while the virtual machine is running. mem.free Guest memory available, in MiB. The value can range between 0 and the configured memory size of the virtual machine. Valid while the virtual machine is running. mem.ballooned The size of the balloon driver in the virtual machine, in MiB. The host will inflate the balloon driver to reclaim physical memory from the virtual machine. This is a sign that there is memory pressure on the host. mem.swapped The portion of memory, in MiB, that is granted to this virtual machine from the host's swap space. This is a sign that there is memory pressure on the host. mem.swappedSsd The amount of memory swapped to fast disk device such as SSD, in MiB cpu.allocationLimit Resource limits for CPU, in MHz. If set to -1, there is no fixed allocation limit. cpu.overallUsage Basic CPU performance statistics, in MHz. Valid while the virtual machine is running. cpu.hostUsagePercent Percent of the host CPU used by the virtual machine. In case a limit is configured, the percentage is calculated by taking the limit as the total. cpu.cores Number of processors in the virtual machine disk.totalMiB Total storage space, committed to this virtual machine across all datastores, in MiB ipAddress Primary guest IP address, if available ipAddresses List of IPs associated with the VM (except ipAddress). A pipe or vertical bar character (|) is used as a separator. connectionState Indicates whether or not the virtual machine is available for management: connected: Server has access to the virtual machine. disconnected: Server is currently disconnected from the virtual machine, since its host is disconnected. inaccessible: One or more of the virtual machine configuration files are inaccessible. invalid: The virtual machine configuration format is invalid. orphaned: The virtual machine is no longer registered on its associated host. powerState The current power state of the virtual machine: poweredOff, poweredOn, or suspended. guestHeartbeatStatus gray: Status is unknown. green: Entity is OK. yellow: Entity might have a problem. red: Entity definitely has a problem. operatingSystem Operating system of the virtual machine guestFullName Guest operating system full name, if available from guest tools hypervisorHostname Name of the host where the virtual machine is running instanceUuid Unique identification of the virtual machine datacenterName Name of the datacenter clusterName Name of the cluster resourcePoolNameList List of names of the resource pools datastoreNameList List of names of datastores networkNameList List of names of networks datacenterLocation Datacenter location overallStatus gray: Status is unknown. green: Entity is OK. yellow: Entity might have a problem. red: Entity definitely has a problem. disk.suspendMemory Size of the snapshot file (bytes). disk.suspendMemoryUnique Size of the snapshot file, unique blocks (bytes). disk.totalUncommittedMiB Additional storage space potentially used by this virtual machine on all datastores. Essentially an aggregate of the property uncommitted across all datastores that this virtual machine is located on (Mebibytes). disk.totalUnsharedMiB Total storage space occupied by the virtual machine across all datastores, that is not shared with any other virtual machine (Mebibytes). mem.hostUsage Host memory usage (Mebibytes). resourcePoolName Resource Pool Name. vmConfigName Vm Config Name. vmHostname Vm Hostname. VSphereDatastore Name Description capacity Maximum capacity of this datastore, in GiB, if accessible is true freeSpace Available space of this datastore, in GiB, if accessible is true uncommitted Total additional storage space, potentially used by all virtual machines on this datastore, in GiB, if accessible is true vmCount Number of virtual machines attached to the datastore datacenterLocation Datacenter location datacenterName Datacenter name hostCount Number of hosts attached to the datastore overallStatus gray: Status is unknown. green: Entity is OK. yellow: Entity might have a problem. red: Entity definitely has a problem. accessible Connectivity status of the datastore. If this is set to false, the datastore is not accessible. url Unique locator for the datastore, if accessible is true fileSystemType Type of file system volume, such as VMFS or NFS name Name of the datastore nas.remoteHost Host that runs the NFS/CIFS server nas.remotePath Remote path of NFS/CIFS mount point VSphereDatacenter Name Description datastore.totalUsedGiB Total used space in the datastores, in GiB datastore.totalFreeGiB Total free space in the datastores, in GiB datastore.totalGiB Total size of the datastores, in GiB cpu.cores Total CPU count per datacenter cpu.overallUsagePercentage Total CPU usage, in percentage cpu.overallUsage Total CPU usage, in MHz cpu.totalMHz Total CPU capacity, in MHz mem.usage Total memory usage, in MiB mem.size Total memory, in MiB mem.usagePercentage Total memory usage as percentage clusters Total cluster count per datacenter resourcePools Total resource pools per datacenter datastores Total datastores per datacenter networks Total network adapter count per datacenter overallStatus gray: Status is unknown green: Entity is OK yellow: Entity might have a problem red: Entity definitely has a problem hostCount Total host system count per datacenter vmCount Total virtual machines count per datacenter VSphereResourcePool Name Description cpu.TotalMHz Resource pool CPU total capacity, in MHz cpu.overallUsage Resource pool CPU usage, in MHz mem.size Resource pool total memory reserved, in MiB mem.usage Resource pool memory usage, in MiB mem.free Resource pool memory available, in MiB mem.ballooned Size of the balloon driver in the resource pool, in MiB mem.swapped Portion of memory, in MiB, that is granted to this resource pool from the host's swap space vmCount Number of virtual machines in the resource pool overallStatus gray: Status is unknown. green: Entity is OK. yellow: Entity might have a problem. red: Entity definitely has a problem. resourcePoolName Name of the resource pool datacenterLocation Datacenter location datacenterName Name of the datacenter clusterName Name of the cluster VSphereCluster Name Description cpu.totalEffectiveMHz Effective CPU resources, in MHz, available to virtual machines. This is the aggregated effective resource level from all running hosts. Hosts that are in maintenance mode or are unresponsive are not counted. Resources used by the VMware Service Console are not included in the aggregate. This value represents the amount of resources available for the root resource pool for running virtual machines. cpu.totalMHz Aggregated CPU resources of all hosts, in MHz. It does not filter out cpu used by system or related to hosts under maintenance. cpu.cores Number of physical CPU cores. Physical CPU cores are the processors contained by a CPU package. cpu.threads Aggregated number of CPU threads. mem.size Aggregated memory resources of all hosts, in MiB. It does not filter out memory used by system or related to hosts under maintenance. mem.effectiveSize Effective memory resources, in MiB, available to run virtual machines. This is the aggregated effective resource level from all running hosts. Hosts that are in maintenance mode or are unresponsive are not counted. Resources used by the VMware Service Console are not included in the aggregate. This value represents the amount of resources available for the root resource pool for running virtual machines. effectiveHosts Total number of effective hosts. This number exclude hosts under maintenance. hosts Total number of hosts overallStatus gray: Status is unknown. green: Entity is OK. yellow: Entity might have a problem. red: Entity definitely has a problem. datastoreList List of datastore used by the cluster. A pipe or vertical bar character (|) is used as a separator. hostList List of hosts belonging to the cluster. A pipe or vertical bar character (|) is used as a separator. networkList List of networks attached to the cluster. A pipe or vertical bar character (|) is used as a separator. drsConfig.vmotionRate Threshold for generated ClusterRecommendations. DRS generates only those recommendations that are above the specified vmotionRate. Ratings vary from 1 to 5. This setting applies to manual, partiallyAutomated, and fullyAutomated DRS clusters. dasConfig.restartPriorityTimeout Maximum time the lower priority VMs should wait for the higher priority VMs to be ready (Seconds). datacenterName Datacenter name. datacenterLocation Datacenter location. drsConfig.enabled Flag indicating whether or not the service is enabled. drsConfig.enableVmBehaviorOverrides Flag that dictates whether DRS Behavior overrides for individual virtual machines (ClusterDrsVmConfigInfo) are enabled. drsConfig.defaultVmBehavior Specifies the cluster-wide default DRS behavior for virtual machines. You can override the default behavior for a virtual machine by using the ClusterDrsVmConfigInfo object. dasConfig.enabled Flag to indicate whether or not vSphere HA feature is enabled. dasConfig.admissionControlEnabled Flag that determines whether strict admission control is enabled dasConfig.isolationResponse Indicates whether or not the virtual machine should be powered off if a host determines that it is isolated from the rest of the compute resource. dasConfig.restartPriority Restart priority for a virtual machine. dasConfig.hostMonitoring Determines whether HA restarts virtual machines after a host fails. dasConfig.vmMonitoring Level of HA Virtual Machine Health Monitoring Service. dasConfig.vmComponentProtecting This property indicates if vSphere HA VM Component Protection service is enabled. dasConfig.hbDatastoreCandidatePolicy The policy on what datastores will be used by vCenter Server to choose heartbeat datastores: allFeasibleDs, allFeasibleDsWithUserPreference, userSelectedDs VSphereSnapshotVm Name Description snapshotTreeInfo Tree info for the snapshot. Es: Cluster:Vm:Snapshot1:Snapshot2 name Snapshot name creationTime Snapshot creation time powerState The power state of the virtual machine when this snapshot was taken snapshotId The unique identifier that distinguishes this snapshot from other snapshots of the virtual machine quiesced Flag to indicate whether or not the snapshot was created with the \"quiesce\" option, ensuring a consistent state of the file system backupManifest The relative path from the snapshotDirectory pointing to the backup manifest. Available for certain quiesced snapshots only description Description of the snapshot replaySupported Flag to indicate whether this snapshot is associated with a recording session on the virtual machine that can be replayed totalMemoryInDisk Total size of memory in disk. totalUniqueMemoryInDisk Total size of the file corresponding to the file blocks that were allocated uniquely to store memory. In other words, if the underlying storage supports sharing of file blocks across disk files, the property corresponds to the size of the file blocks that were allocated only in context of this file, i.e. it does not include shared blocks that were allocated in other files. This property will be unset if the underlying implementation is unable to compute this information. totalDisk Total size of snapshot files in disk totalUniqueDisk Total size of the file corresponding to the file blocks that were allocated uniquely to store snapshot data in disk. In other words, if the underlying storage supports sharing of file blocks across disk files, the property corresponds to the size of the file blocks that were allocated only in context of this file, i.e. it does not include shared blocks that were allocated in other files. This property will be unset if the underlying implementation is unable to compute this information. datastorePathDisk Disk file path in the datastore datastorePathMemory Memory file path in the datastore",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.54826,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware vSphere <em>monitoring</em> <em>integration</em>",
        "sections": "VMware vSphere <em>monitoring</em> <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": "New Relic&#x27;s VMware vSphere <em>integration</em> helps you understand the health and performance of your vSphere environment. You can: Query data to get insights on the performance on your hypervisors, <em>virtual</em> <em>machines</em>, and more. Go from high level views down to the most granular data. vSphere data"
      },
      "id": "6044e44f196a67abca960f59"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-network-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.45688,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53128,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.5281,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration": [
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53128,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.5281,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    },
    {
      "sections": [
        "Azure Cost Management monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Event data"
      ],
      "title": "Azure Cost Management monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "a4cf35715fefc93b9938c32e7133deb1aaca9928",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-cost-management-monitoring-integration/",
      "published_at": "2021-05-22T06:16:43Z",
      "updated_at": "2021-03-16T04:36:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Cost Management data to New Relic. Here we explain how to activate the integration and what data it collects. Features New Relic's Azure Cost Management integration collects your accumulated monthly expenditures for the Azure resources in your subscription, grouped by service, location, resource group, or tag. Our integration allows you to track how costs evolve for each billing period and detect unexpected trends, or alert on accumulated costs for the current billing period. Activate integration Follow standard procedures to activate your Azure service in New Relic. In some cases, the Azure Cost Management integration requires you to add an additional role to fetch billing data: In the Azure Portal Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add. From the Role dropdown, select Billing Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Cost Management integration: Default polling frequency: 1 hour Maximum recommended polling frequency: 1 hour Resolution: 1 day Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureCostManagementSample event type. The provider metadata value indicates how the daily cost is grouped: AzureCostLocation: Costs are grouped by location. AzureCostService: Costs are grouped by cloud service. AzureCostResourceGroup: Costs are grouped by resource group. AzureCostTag: Costs are grouped by a resource tag, indicated in the label.<tag_key> metadata of the same event. Untagged resources will not be collected. In order to collect costs grouped by tag, specify the tag key in the integration settings. Costs are estimates, as Microsoft can make updates to the cost data up until the invoice for the billing period is generated. To get the most accurate value of the accumulated cost in the billing period, use the max() function in your NRQL queries. Event data Here are the metrics and metadata reported by the Azure Cost Management integration. For more information about how data is structured and reported to New Relic, see Understand and use integration data. Metric Description currency USD cost Amount spent for the day of the event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.70006,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Cost Management monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Cost Management monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Cost Management data to New Relic. Here we explain how to activate the integration and what data it collects. Features New Relic&#x27;s <em>Azure</em> Cost Management integration collects your accumulated monthly expenditures"
      },
      "id": "603ea1d0e7b9d230242a07e9"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.4568,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-05-22T07:06:34Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.53123,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-05-21T15:20:45Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.52805,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 154.58734,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Tip",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-05-22T08:56:01Z",
      "updated_at": "2021-03-13T03:33:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as New Relic APM's .NET support for Azure. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.4345,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as New Relic APM&#x27;s .NET support for <em>Azure</em>. Tip To use <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "82db3eae120c4318365cf0d0e5bfee69930b969f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2021-05-22T08:56:01Z",
      "updated_at": "2021-03-13T03:47:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.24368,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "6044e560196a671d6f960f72"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 154.58728,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Tip",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-05-22T08:56:01Z",
      "updated_at": "2021-03-13T03:33:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as New Relic APM's .NET support for Azure. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.4345,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as New Relic APM&#x27;s .NET support for <em>Azure</em>. Tip To use <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-05-22T08:54:43Z",
      "updated_at": "2021-03-13T03:44:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 138.73721,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 154.58728,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-05-22T08:54:43Z",
      "updated_at": "2021-03-13T03:44:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 138.73721,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "82db3eae120c4318365cf0d0e5bfee69930b969f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2021-05-22T08:56:01Z",
      "updated_at": "2021-03-13T03:47:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.24368,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "6044e560196a671d6f960f72"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-05-21T18:18:57Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 154.58722,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Tip",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-05-22T08:56:01Z",
      "updated_at": "2021-03-13T03:33:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as New Relic APM's .NET support for Azure. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.4345,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as New Relic APM&#x27;s .NET support for <em>Azure</em>. Tip To use <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-05-22T08:54:43Z",
      "updated_at": "2021-03-13T03:44:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 138.73721,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    }
  ],
  "/docs/integrations/mlops-integrations/algorithmia-mlops-integration": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/style-guide/processes-procedures/update-left-navigation-pane/",
      "sections": [
        "Update left-navigation pane",
        "The configuration file",
        "Tip",
        "Configuration file options",
        "Use nav YAML to generate auto index content",
        "Nav requirements for auto index pages",
        "Example"
      ],
      "published_at": "2021-05-21T16:38:01Z",
      "title": "Update left-navigation pane",
      "updated_at": "2021-05-21T16:38:01Z",
      "type": "docs",
      "external_id": "a05e38602a1eb152749923691d251ef2c99f05a9",
      "document_type": "page",
      "popularity": 1,
      "body": "Navigation for docs.newrelic.com is stored in YAML files located in the /src/nav/ directory. Each top-level navigation should have its own configuration file. You'll most likely edit a version of this file in your branch and merge it into the develop branch. The configuration file As an example, here's a snippet of the agents.yml navigation configuration. Note that the file has indentation that corresponds to the level of the navigation hierarchy. Be sure to imitate the same spacing when you make changes: title: Agents path: /docs/agents pages: - title: Manage APM agents path: /docs/agents/manage-apm-agents pages: - title: Agent data path: /docs/agents/manage-apm-agents/agent-data pages: - title: Real time streaming path: /docs/agents/manage-apm-agents/agent-data/real-time-streaming - title: Custom instrumentation path: /docs/agents/manage-apm-agents/agent-data/custom-instrumentation - title: Agent attributes path: /docs/agents/manage-apm-agents/agent-data/agent-attributes - title: Custom events path: /docs/agents/manage-apm-agents/agent-data/collect-custom-events - title: Custom metrics path: /docs/agents/manage-apm-agents/agent-data/collect-custom-metrics - title: Manage errors path: /docs/agents/manage-apm-agents/agent-data/manage-errors-apm-collect-ignore-or-mark-expected - title: Link Kubernetes path: /docs/agents/manage-apm-agents/agent-data/link-your-applications-kubernetes - title: App naming pages: - title: Name your application path: /docs/agents/manage-apm-agents/app-naming/name-your-application - title: Use multiple names for an app path: /docs/agents/manage-apm-agents/app-naming/use-multiple-names-app Copy Tip Navigation nesting is currently limited to a maximum of six levels deep. Please reach out to the engineering team if that is not enough. Configuration file options When you're changing the navigation, keep in mind that each node in the .yml configuration file can have the following properties: Key Required? Description title yes The text that is shown in the navigation. path * no The URL path to the page. Do not use trailing slashes. * Adding a path is required to generate auto index page content. See path requirements below. children no Any sub-navigation nodes. When the user goes to a page, we determine which section of the site they are on and load the appropriate .yml file to populate the sidebar navigation. The navigation for the homepage is an aggregate of all the top-level pages. Tip Each category has its own index.md page (list of pages for that category). When updating the navigation, you may also want to update these pages to reflect the new information architecture more correctly. Use nav YAML to generate auto index content On build we automatically generate an index page for each sub-directory in src/content that does not already have an index.mdx file and that has at least one .mdx or .md file in it. When creating the page we look through each nav YAML file for a reference to the path to know which nav data to use for the left nav and the auto index page content. If an index page path is in more than one nav YAML file, we use the nav that matches its top level path with the beginning of index page path. Nav requirements for auto index pages In order to populate the auto index page with nav YAML data, you must add the index page path somewhere in the most relevant nav YAML file. This helps the auto index page plugin determine which nav to attach to the index page, but also helps the <IndexContents /> component know which section to display on the index page itself. Each nav item must reference a directory path in src/content that contains at least one .md or .mdx file (otherwise the auto index plugin does not create a page for it). Example If this auto index page is used in full-stack-observability.yml and integrations.yml, we will attach the integrations.yml nav to it because: the top level path in the integrations.yml matches the beginning of the auto index page path: title: Integrations path: /docs/integrations Copy matches this auto index page path: /docs/integrations /amazon-integrations/aws-integrations-list Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 47.506344,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " a directory path in src&#x2F;content that contains at least one .md or .mdx file (otherwise the auto index plugin does not create a page for it). Example If this auto index page is used in full-stack-observability.yml and <em>integrations</em>.yml, we will attach the <em>integrations</em>.yml nav to it because: the top level path"
      },
      "id": "60422077196a67ad56a83dc8"
    },
    {
      "sections": [
        "Uninstall infrastructure integrations",
        "Cloud integrations",
        "AWS",
        "Azure",
        "Google Cloud Platform (GCP)",
        "On-host integrations",
        "Apache",
        "Cassandra",
        "Kubernetes",
        "MySQL",
        "NGINX",
        "Redis",
        "StatsD",
        "Moving away from the integrations package",
        "Uninstall package"
      ],
      "title": "Uninstall infrastructure integrations",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Update or uninstall"
      ],
      "external_id": "1e9232193cbf71bdbe1a6c6d0374ed0d6b7e7b0f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/uninstall-infrastructure-integrations/",
      "published_at": "2021-05-21T18:13:21Z",
      "updated_at": "2021-05-16T10:05:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Uninstalling the infrastructure agent does not directly affect any of your infrastructure Integrations: if you uninstall the agent, your integrations will remain. Similarly, if you disable or uninstall your integrations, the infrastructure agent will remain. To uninstall any of your integrations, follow the procedure corresponding to the type of integration. Cloud integrations AWS You can disable infrastructure AWS integrations and still retain the connection between your AWS account and New Relic. We recommend not to disable your EC2 and EBS integrations because those add important metadata to your infrastructure data. If you want to... Do this Disable one or more AWS service integrations To disable services while keeping your AWS account linked to New Relic: From one.newrelic.com > Infrastructure, select AWS > Manage services. From your Edit AWS account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all AWS integrations To disconnect your AWS account completely from New Relic, you need to unlink your AWS account. This disables all New Relic integrations associated with that AWS account. Go to one.newrelic.com > Infrastructure > AWS > Manage services. From your Edit AWS account page, select Unlink this account. Save your changes. Sign in to AWS and select Services > IAM > Roles. Select the checkbox for the role you want to delete, then select Role Actions > Delete Role. Unlinking your AWS account will disable the trust relationship set up via your ARN. Azure If you want to... Do this Disable one or more Azure service integrations To disable services while keeping your Azure account linked to New Relic: Go to one.newrelic.com > Infrastructure > Azure > Manage services. From your Edit Azure account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all Azure integrations To disconnect your Azure account completely from New Relic, you need to unlink your Azure account. This requires being either the user who registered the app or an administrator. This procedure will disable all New Relic integrations associated with that Azure account. Go to one.newrelic.com > Infrastructure > Azure > Manage services. From your Edit Azure account page, select Unlink this account. Save your changes. Sign in to Azure and go into All Services > Identity > App registrations, or go to Azure Active Directory service and select App registrations. Find the registered app (the recommended name is NewRelic-Integrations). To see the full list of available apps, select the dropdown menu beside the search field and select All apps. Select the app and, on the panel that opens, select Delete. Google Cloud Platform (GCP) If you want to... Do this Disable one or more GCP service integrations To disable services while keeping your GCP account linked to New Relic: From one.newrelic.com > Infrastructure > GCP > Manage services. From your Edit GCP account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all GCP integrations To disconnect your GCP account completely from New Relic, you need to unlink your GCP account. This disables all New Relic integrations associated with that GCP account. If you registered the GCP project using a User account, follow these steps. Go to one.newrelic.com > Infrastructure > GCP > Manage services. From your Edit GCP account page, select Unlink this account. Save your changes. If you registered the GCP project using a Service account, follow these steps. If you are deleting a custom role, be aware that this role may be used for other purposes besides New Relic access. Sign in to New Relic and go to Infrastructure > Integrations > Google Cloud Platform. For a standard (non-custom) user role, select Manage Services for the account you want to remove. Copy the value of User and save it. OR For a custom user role, go to IAM > admin > Roles, search for the role, select it, and select DELETE. You are now finished and can skip the remaining steps. Standard (non-custom) user role: Sign in to Google Cloud and select the correct project in the Select a project box. From the navigation menu, select IAM & admin > IAM. Search for and select the user value you saved, then select REMOVE. On-host integrations If you used the integrations package, see the integrations package instructions. Here are some examples: Apache Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-apache yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-apache zypper (SLES) sudo zypper -n remove nri-apache Cassandra Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-cassandra yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-cassandra zypper (SLES) sudo zypper -n remove nri-cassandra Kubernetes Each cluster will have a single node where kubectl is running. To uninstall the Kubernetes integration, use the following command on each of these nodes: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy MySQL Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-mysql yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-mysql zypper (SLES) sudo zypper -n remove nri-mysql NGINX Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-nginx yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-nginx zypper (SLES) sudo zypper -n remove nri-nginx Redis Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-redis yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-redis zypper (SLES) sudo zypper -n remove nri-redis StatsD cd /path/to/statsd npm uninstall @newrelic/statsd-infra-backend From the StatsD config.js, remove the \"@newrelic/statsd-infra-backend\" entry from the list of backends. Restart StatsD. Moving away from the integrations package While it is still possible to use the integrations package, we recommend removing it completely and working with integrations on an individual basis. The last integration package contains the following versions of the integrations: Apache 1.1.2 Cassandra 2.0.3 MySQL 1.1.5 Nginx 1.0.2 Redis 1.0.1 If you remove the integrations package and want to continue using the related on-host integrations, you will need to install them one by one. To uninstall the package and re-install your integrations: Remove the integrations package by following these instructions. The config files from the old integrations will not be deleted, so you won’t have to configure them again. Uninstall package Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove newrelic-infra-integrations sudo apt-get autoremove yum (Amazon Linux, CentOS, or RHEL) sudo yum remove newrelic-infra-integrations sudo yum autoremove zypper (SLES) sudo zypper -n remove newrelic-infra-integrations --clean-deps Copy Install your integrations one by one following these instructions. To replicate the integrations package, you will need to install all the available integrations again.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 45.91655,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Uninstall infrastructure <em>integrations</em>",
        "sections": "Uninstall infrastructure <em>integrations</em>",
        "body": "Uninstalling the infrastructure agent does not directly affect any of your infrastructure <em>Integrations</em>: if you uninstall the agent, your <em>integrations</em> will remain. Similarly, if you disable or uninstall your <em>integrations</em>, the infrastructure agent will remain. To uninstall any of your <em>integrations</em>"
      },
      "id": "603ea831196a67fc6fa83db5"
    },
    {
      "sections": [
        "NerdGraph tutorial: Configure cloud integrations",
        "Requirements",
        "Access the NerdGraph GraphiQL explorer",
        "Query examples",
        "Available provider accounts",
        "Specific provider account information",
        "Specific integration data from a specific cloud provider",
        "List of enabled cloud accounts",
        "Specific linked account data",
        "Enabled cloud integrations for all linked accounts",
        "Specific cloud integration data for a specific linked account",
        "Mutation examples",
        "Link an account",
        "Link an Amazon AWS account",
        "Link an Amazon AWS account using CloudWatch Metric Streams",
        "Link a Microsoft Azure account",
        "Link a Google Cloud Platform (GCP) project",
        "Rename one or more cloud accounts",
        "Enable an integration in a cloud account",
        "Enable an integration in multiple cloud accounts",
        "Enable multiple integrations in multiple cloud accounts",
        "Modify an integration's configuration (regions, polling intervals, etc.)",
        "Disable (remove) an integration",
        "Unlink account",
        "Caution",
        "Enable an Amazon AWS integration",
        "Send query to fetch account data",
        "Link AWS provider account",
        "Enable Amazon AWS SQS integration",
        "Enable integration in multiple provider accounts",
        "Change polling interval for Amazon AWS integration",
        "Update the polling interval",
        "Disable Amazon AWS integration",
        "Disable the SQS integration"
      ],
      "title": "NerdGraph tutorial: Configure cloud integrations",
      "type": "docs",
      "tags": [
        "APIs",
        "NerdGraph",
        "Examples"
      ],
      "external_id": "9b0b3a78186946732433d21aa149fc36e14bb2d8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/nerdgraph/examples/nerdgraph-cloud-integrations-api-tutorial/",
      "published_at": "2021-05-21T14:08:28Z",
      "updated_at": "2021-05-16T14:30:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document provides examples of how to use New Relic NerdGraph to query and modify your cloud integration configuration data, including Amazon AWS, Microsoft Azure, and Google Cloud Platform (GCP). Using the NerdGraph GraphiQL explorer, you can also query NRQL data. These examples for querying cloud integration configuration data use GraphQL queries and mutations: Queries: requests that are intended to only fetch data Mutations: requests that create or update data on the server Requirements Before querying cloud integration data with NerdGraph, ensure you have: Followed the instructions to connect cloud integrations with New Relic. Created an API key. Access the NerdGraph GraphiQL explorer To access the NerdGraph GraphiQL explorer: Go to api.newrelic.com/graphiql. Add any of the following examples. Query examples Queries are requests that are intended to only fetch data (no side effects). Queries in NerdGraph are not static, meaning that you can ask for more or less data depending on your needs. For each query, you can specify exactly what data you want to retrieve, as long as it is supported by the schema. Available provider accounts This query returns a list of all provider accounts available in your infrastructure data. Depending on the provider, additional properties can be requested. For example, for GCP, you can also ask for the serviceAccountId property, which is needed when linking a new GCP project to New Relic. Anonymous: { actor { account(id: <NR_ACCOUNT_ID>) { cloud { providers { id name slug ... on CloudGcpProvider { serviceAccountId } } } } } } Copy Named: query cloudProviders { actor { account(id: <NR_ACCOUNT_ID>) { cloud { providers { id name slug } } } } } Copy Specific provider account information This query returns information about a specific provider account for your Amazon AWS integration. The properties id, name, slug are requested, along with a list of integrations available to be monitored. { actor { account(id: <NR_ACCOUNT_ID>) { cloud { provider(slug: \"aws\") { id slug name services { id slug name } } } } } } Copy Specific integration data from a specific cloud provider This query returns information about a specific cloud service integration of a provider. In this example, the integration is the Amazon AWS ALB monitoring integration and the provider is AWS. The properties id, name, slug, and isAllowed are requested with the available configuration parameters. { actor { account(id: <NR_ACCOUNT_ID>) { cloud { provider(slug: \"aws\") { service(slug: \"alb\") { id name slug isEnabled } } } } } } Copy List of enabled cloud accounts This query returns the list of cloud accounts enabled with your New Relic account. (Your cloud account associates your New Relic account and a specific provider account with your integration.) You can enable multiple cloud provider accounts in the same New Relic account, even with the same cloud provider. { actor { account(id: <NR_ACCOUNT_ID>) { cloud { linkedAccounts { id name createdAt provider { id name } } } } } } Copy Specific linked account data This query returns information about a linked account, including the properties name, providerId, and a list of the cloud integrations enabled for monitoring. { actor { account(id: <NR_ACCOUNT_ID>) { cloud { linkedAccount(id: <LINKED_CLOUD_ACCOUNT_ID>) { name provider { id name } integrations { id name createdAt updatedAt } } } } } } Copy Enabled cloud integrations for all linked accounts This query returns all monitored integrations for all the provider cloud accounts. { actor { account(id: <NR_ACCOUNT_ID>) { cloud { linkedAccounts { name provider { id name } integrations { id name service { id name } createdAt updatedAt } } } } } } Copy Specific cloud integration data for a specific linked account This query returns information about a specific integration from a specific linked account. { actor { account(id: <NR_ACCOUNT_ID>) { cloud { linkedAccount(id: <LINKED_CLOUD_ACCOUNT_ID>) { name provider { id name } integration(id: <INTEGRATION_ID>) { id name service { id name } createdAt updatedAt } } } } } } Copy Mutation examples Mutations are requests that are intended to have side effects, such as creating or updating data on the server. Mutations require the keyword mutation and the name of the mutation. NerdGraph mutations are restricted to a subset of all possible mutations. Link an account This mutation allows linking cloud provider accounts to a New Relic account, creating one or more linked accounts. It can link one specific cloud provider account (for example aws) to the New Relic account or multiple cloud provider accounts to one New Relic account. Required: The parameter <PROVIDER_ACCOUNT_NAME> is required and cannot be empty. It must be unique in your New Relic account. Other parameters are specific to the provider (AWS, GCP, and Azure) and are also required. In the following sections, you can see which parameters are required for each provider account. After linking an account the createdAt and updatedAt values are equal. mutation { cloudLinkAccount( accounts: { accountId: <NR_ACCOUNT_ID>, aws: [{ name: <PROVIDER_ACCOUNT_NAME>, <other_params> }] azure: [{ name: <PROVIDER_ACCOUNT_NAME>, <other_params> }] gcp: [{ name: <PROVIDER_ACCOUNT_NAME>, <other_params> }] } ) { linkedAccounts { id name authLabel createdAt updatedAt } } } } Copy Link an Amazon AWS account This mutation links an Amazon AWS provider account to your New Relic account. mutation { cloudLinkAccount( accountId: <NR_ACCOUNT_ID>, accounts: { aws: [{ name: <PROVIDER_ACCOUNT_NAME>, arn: <AWS_ROLE_ARN> }] } ) { linkedAccounts { id name authLabel createdAt updatedAt } } } } Copy Link an Amazon AWS account using CloudWatch Metric Streams This mutation links an Amazon AWS account sending data through CloudWatch Metric Streams to your New Relic account. mutation { cloudLinkAccount( accountId: <NR_ACCOUNT_ID>, accounts: { aws: [{ name: <PROVIDER_ACCOUNT_NAME>, arn: <AWS_ROLE_ARN>, metricCollectionMode: PUSH }] } ) { linkedAccounts { id name authLabel createdAt updatedAt } } } } Copy Link a Microsoft Azure account This mutation links a Microsoft Azure cloud subscription to the New Relic account. mutation { cloudLinkAccount( accountId: <NR_ACCOUNT_ID>, accounts: { azure: [{ name: <PROVIDER_ACCOUNT_NAME>, applicationId: <azure_application_id>, clientSecret: <azure_application_key>, tenantId: <azure_tenant_id>, subscriptionId: <azure_subscription_id> }] } ) { linkedAccounts { id name authLabel createdAt updatedAt } } } Copy Link a Google Cloud Platform (GCP) project This mutation links a GCP project to the New Relic account. mutation { cloudLinkAccount( accountId: <NR_ACCOUNT_ID>, accounts: { gcp: [{ name: <PROVIDER_ACCOUNT_NAME>, projectId: <GCP_PROJECT_ID> }] } ) { linkedAccounts { id name authLabel createdAt updatedAt } } } Copy Rename one or more cloud accounts This mutation allows you to rename one or more linked provider accounts. The name parameter is required, cannot be empty, and must be unique within your New Relic account. mutation { cloudRenameAccount( accountId: <NR_ACCOUNT_ID>, accounts: [ { id: <linked_cloud_account_id_1>, name: <new_provider_account_name> }, { id: <linked_cloud_account_id_2>, name: <new_provider_account_name> } ] ) { linkedAccounts { id name } } } Copy Enable an integration in a cloud account This mutation allows you to enable the monitoring of one or more specific cloud integrations in an existing cloud account. With this mutation, New Relic records data for the enabled integration from the provider account. For each provider account you have access to different input parameters, matching each available service. mutation { cloudConfigureIntegration ( accountId: <NR_ACCOUNT_ID>, integrations: { <provider_slug> : { <integration_slug>: [{ linkedAccountId: <LINKED_CLOUD_ACCOUNT_ID>, <other_parameters> }] } } ) { integrations { id name integration { id slug } ... on SqsIntegration { awsRegions } } } } Copy Enable an integration in multiple cloud accounts If you have many provider accounts linked, you can enable the same integration in the many cloud accounts at the same time. For the output of the operation, you can use GraphQL fragments for integration specific configuration parameters. mutation { cloudConfigureIntegration ( accountId: <NR_ACCOUNT_ID>, integrations: { <provider_slug> : { <integration_slug> : [ { linkedAccountId: <linked_cloud_account_id_1> }, { linkedAccountId: <linked_cloud_account_id_2> } ] } } ) { integrations { id name integration { id name } ... on SqsIntegration { awsRegions } } } } Copy Enable multiple integrations in multiple cloud accounts If you have multiple cloud accounts linked, you can also enable multiple integrations in multiple linked cloud accounts at the same time. For the output of the operation, you can use GraphQL fragments to ask for integration specific configuration parameters. mutation { cloudConfigureIntegration ( accountId: <NR_ACCOUNT_ID>, integrations: { <provider_slug_1>: { <integration_slug_1>: [ { linkedAccountId: <linked_cloud_account_id_1> } ] <integration_slug_2>: [ { linkedAccountId: <linked_cloud_account_id_2> } ] }, <provider_slug_2>: { <integration_slug_3>: [ { linkedAccountId: <linked_cloud_account_id_3>}, { linkedAccountId: <linked_cloud_account_id_4>} ] } } ) { integrations { id name service { id name } ... on SqsIntegration { awsRegions } } } } Copy Modify an integration's configuration (regions, polling intervals, etc.) This mutation also allows you to modify one or more cloud integrations and change one or more configuration parameters. Each service will have specific parameters that you can modify. For parameters of a type list (for example, awsRegion) supply the full list. For the output of the operation, you can use GraphQL fragments to ask for integration specific configuration parameters. mutation { cloudConfigureIntegration ( accountId: <NR_ACCOUNT_ID>, integrations: { <provider_slug>: { <integration_slug>: [{ linkedAccountId: <LINKED_CLOUD_ACCOUNT_ID>, metricsPollingInterval: <new_polling_interval>, <parameter_1>: <value_1>, <parameter_N>: <value_N>, }] } } ) { integrations { id name service { id slug } ... on SqsIntegration { metricsPollingInterval, <parameter_1>, <parameter_N> } } errors { type message } } } Copy Disable (remove) an integration This mutation allows you to disable an integration and stop data collection for the specific cloud integration. mutation { cloudDisableIntegration ( accountId: <NR_ACCOUNT_ID>, integrations: { : { : [ { linkedAccountId: <LINKED_CLOUD_ACCOUNT_ID> } ] } } ) { disabledIntegrations { id name authLabel provider { id } } errors { type message } } }</integration_slug></provider_slug> Copy Unlink account This mutation allows you to unlink cloud provider accounts from New Relic account. Caution This action can not be undone. However, you can link the account again, but account history will still be lost. mutation { cloudUnlinkAccount ( accountId: <NR_ACCOUNT_ID>, accounts: { { linkedAccountId: <LINKED_CLOUD_ACCOUNT_ID> } } ) { unlinkedAccounts { id name } errors { type message } } }</integration_slug></provider_slug> Copy Enable an Amazon AWS integration This example uses an Amazon AWS SQS integration and assumes you have connected an AWS account to New Relic. To enable an Amazon AWS integration: Send query to fetch account data Send a query to fetch data about the account, specifically available providers and already created provider accounts: { actor { account(id: <NR_ACCOUNT_ID>) { cloud { providers { id name slug } linkedAccounts { name integrations { id name } } } } } } Copy Link AWS provider account Link an AWS provider account, if there is not one already linked or if you want to link another AWS account: Use your New Relic account identifier in the <NR_ACCOUNT_ID> parameter. Provide a name for the provider account in the <PROVIDER_ACCOUNT_NAME>. Include the ARN of the AWS role used to fetch data from your AWS account. mutation { cloudLinkAccount( accountId: <NR_ACCOUNT_ID>, accounts: { aws: [{ name: <PROVIDER_ACCOUNT_NAME>, arn: <AWS_ROLE_ARN> }] } ) { linkedAccounts { id name authLabel createdAt updatedAt } errors { type message } } } Copy Enable Amazon AWS SQS integration Use your New Relic account ID in the <NR_ACCOUNT_ID> parameter and the ID of the provider account in the <LINKED_CLOUD_ACCOUNT_ID> parameter value. mutation { cloudConfigureIntegration ( accountId: <NR_ACCOUNT_ID>, integrations: { aws: { sqs: [ { linkedAccountId: <LINKED_CLOUD_ACCOUNT_ID> } ] } } ) { integrations { id name service { id name } } errors { type message } } } Copy Enable integration in multiple provider accounts If you have multiple accounts with the same provider account, you can enable the same integration in multiple provider accounts at the same time. Use your New Relic account ID in the <NR_ACCOUNT_ID> parameter and the ID of the provider accounts in the <LINKED_CLOUD_ACCOUNT_ID_n> parameter value. mutation { cloudConfigureIntegration ( accountId: <NR_ACCOUNT_ID>, integrations: { aws: { sqs: [ { linkedAccountId: <LINKED_CLOUD_ACCOUNT_ID_1> }, { linkedAccountId: <LINKED_CLOUD_ACCOUNT_ID_2>, configuration_param_1: value_1, configuration_param_2: value_2 } ] } } }) { integrations { id name service { id name } } errors { type message } } } Copy Change polling interval for Amazon AWS integration This example uses an Amazon AWS SQS integration and assumes you have connected an AWS account to New Relic. To change the polling interval of an AWS integration: Update the polling interval To update the polling interval for an Amazon AWS SQS integration, use your New Relic account ID in the <NR_ACCOUNT_ID> parameter and the id of the linked provider account in the <LINKED_ACCOUNT_ID> parameter value: mutation { cloudConfigureIntegration( accountId: <NR_ACCOUNT_ID>, integrations: { aws : { sqs: [ { linkedAccountId: <LINKED_CLOUD_ACCOUNT_ID>, metricsPollingInterval: 300 } ] } } ) { integrations { id name service { id slug } ... on SqsIntegration { metricsPollingInterval } } errors { type message } } } Copy Disable Amazon AWS integration This example uses an Amazon AWS SQS integration and assumes you have connected an AWS account to New Relic. To disable an AWS integration: Disable the SQS integration Use your New Relic account identifier in the <NR_ACCOUNT_ID> parameter and the ID of the linked cloud account the <LINKED_ACCOUNT_ID> parameter value. mutation { cloudDisableIntegration ( accountId: <NR_ACCOUNT_ID>, integrations: { aws: { sqs : [ { linkedAccountId: <LINKED_CLOUD_ACCOUNT_ID> } ] } } ) { disabledIntegrations { id accountId name } errors { type message } } } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 43.60373,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "NerdGraph tutorial: Configure cloud <em>integrations</em>",
        "sections": "NerdGraph tutorial: Configure cloud <em>integrations</em>",
        "body": " to connect cloud <em>integrations</em> with New Relic. Created an API key. Access the NerdGraph GraphiQL explorer To access the NerdGraph GraphiQL explorer: Go to api.newrelic.com&#x2F;graphiql. Add any of the following examples. Query examples Queries are requests that are intended to only fetch data (no side"
      },
      "id": "6044293864441f1bdc378f05"
    }
  ],
  "/docs/integrations/new-relic-integrations/getting-started/integration-dashboards-infrastructure-beta-transition-guide": [
    {
      "sections": [
        "Transition to New Relic One from Insights",
        "Important",
        "Features",
        "Improved query abilities",
        "Improved visualizations",
        "Steps for a successful transition"
      ],
      "title": "Transition to New Relic One from Insights",
      "type": "docs",
      "tags": [
        "New Relic One",
        "Use New Relic One",
        "Core concepts"
      ],
      "external_id": "4af99cd8030909a71d21a359a60af5ac93b93a66",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/core-concepts/transition-new-relic-one-insights/",
      "published_at": "2021-05-22T00:14:35Z",
      "updated_at": "2021-05-22T00:14:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important As of April 12, 2021, we're upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. Released in 2014, New Relic Insights was our original way to create custom queries, charts, and dashboards. With New Relic One, we have modernized the experience for you to access, analyze, and visualize your data. New Relic One offers an improved charts and dashboards experience, and it provides a platform where we can more rapidly bring new innovations to you. This transition guide can help you understand: What are some of the new and improved features you get with New Relic One charts, dashboards, and queries Why it's easy to transition to New Relic One What to know and considerations when you make the switch How to get the most out of using New Relic One Features You can scroll down to the transition details, but first here are some features we've added that show how New Relic One dashboards are a clear improvement over Insights dashboards. Improved query abilities With New Relic One, you get: Ability to query many accounts from the same widget: New Relic One lets you query across all your associated accounts in one place. Better querying and charting experiences: Query access is available globally, no matter where you are in New Relic One. This includes a \"basic\" query mode that doesn't require knowledge of NRQL. Improved query experience: You can query both the Metric data type and metric timeslice data. Easy customization: Every visualization now has the query accessible. You can augment any curated chart just by changing the NRQL query. Improved visualizations Not only can you select a wide range of visualization options, you can also add more to your dashboards: Better display options: Make your data easier to understand by using visualizations other than dense, line-heavy charts. New Relic One also offers a better TV mode. Facet linking: You can filter your dashboards by faceted attributes, making your dashboards more interactive and easy to use. There's also support for cases. Learn more. More charts or widgets in an area: Insights restricted you to a 3-across limit. Now you can display up to 12 across your dashboard, providing increased data density along with improved tooltips and tracking across charts. Easier creation of multi-page dashboards: Insights referred to these as data apps. Your Insights data apps are preserved as multi-page dashboards in New Relic One. Chart consistency and flexibility: Dashboards include facet color consistency across widgets and faster loading times for more performant dashboards. Also, you can add any chart type to a dashboard in New Relic One! The New Relic Insights UI has served our users well for many years, but it's time to give you an even better experience. Join us and make the switch to New Relic One! Steps for a successful transition The transition to New Relic One has two parts: the UI and mobile app experience (April 12, 2021) and the Dashboard API (July 2021). Insights functionality Transition to New Relic One UI We have already taken care of your transition from Insights to New Relic One for you! As of April 12, 2021, your old Insights web URLs redirect automatically to New Relic One. We recommend that you familiarize yourself with the new UI features available to you, as described in this transition guide. If you need to view any Insights charts embedded in other websites, go to one.newrelic.com > More > Manage Data. (These older embedded charts will continue to function as expected.) Mobile apps Your Insights mobile app is deprecated as of April 11, 2021. Go to the Google Play Store 2 or Apple App store. Delete your old Insights mobile app, and download the New Relic One mobile app. tvOS apps and large displays New Relic's tvOS app is still available. No action is needed by you at this time. Some New Relic customers with the original pricing model may have set up dashboards on wall screens for restricted users with kiosk mode. No action is required for you to continue to view these dashboards. APIs In July of 2021, the Insights Dashboard API will be deprecated and replaced with NerdGraph functionality. For more on this change, and tips on how to migrate, see NerdGraph API for dashboards. Partnership accounts This applies only if your account is one of the few using our partnership account structure to deliver New Relic services to your direct customers. In this situation, the Insights EOL will not affect your customers’ pricing. This is simply an EOL for the UI, not an EOL for the account type. Questions If you have questions about the transition, please comment in our Explorers Hub post. Or, if you work with an account team, they will be happy to help you.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.9056,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Transition</em> to New Relic One from <em>Insights</em>",
        "sections": "<em>Transition</em> to New Relic One from <em>Insights</em>",
        "body": " experience, and it provides a platform where we can more rapidly bring new innovations to you. This <em>transition</em> <em>guide</em> can help you understand: What are some of the new and improved features you get with New Relic One charts, <em>dashboards</em>, and queries Why it&#x27;s easy to <em>transition</em> to New Relic One What to know"
      },
      "id": "6044171164441f454a378ee2"
    },
    {
      "sections": [
        "Introduction to dashboards",
        "Tip",
        "Why it matters",
        "Transitioning from Insights",
        "Get started with dashboards",
        "Create a dashboard",
        "Import a dashboard",
        "Clone a dashboard",
        "Delete a dashboard",
        "Mark a dashboard as favorite",
        "Search and sort dashboards",
        "Dashboard permissions",
        "Organize your dashboards with tags",
        "Key visual tools",
        "Consistent chart coloring",
        "Correlated needle",
        "Data scrubber",
        "Brush to zoom",
        "Custom visualizations"
      ],
      "title": "Introduction to dashboards",
      "type": "docs",
      "tags": [
        "Query your data",
        "Explore and query data",
        "Dashboards"
      ],
      "external_id": "caf20070eae1529315d1e0642bd2f853e2872b77",
      "image": "https://docs.newrelic.com/static/c9724f76b9c3ad86f9a22abab501a2af/c1b63/dashboards_intro.png",
      "url": "https://docs.newrelic.com/docs/query-your-data/explore-query-data/dashboards/introduction-dashboards/",
      "published_at": "2021-05-22T05:25:43Z",
      "updated_at": "2021-05-22T05:25:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Envision your data as a complex system of roads: you need to navigate the signs and signals along the way to quickly see and make meaning of the information you collect. New Relic One dashboards gather and chart the specific data you want to see, the way you want to see it, from anywhere in the New Relic platform. Tip To use dashboards and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Why it matters With New Relic One dashboards you can customize and understand the data you collect. Explore your data and correlate connected sources with tailored, user-friendly charts, and quickly learn the state of your system and applications for faster, more efficient troubleshooting. Use dashboards to: Drive insight with custom, high-density interactive visualizations with a consistent UI. View dashboards across your organization using cross-account search. Chart all the events and attributes from everywhere across our platform. For more information, see Data collection. Add custom attributes or send custom event types to most events in order to better understand your business, and see specific details about how your customers interact with your platform, such as page views, host transactions, etc. Manage your charts and dashboards easily using our quick-access CRUD menus and editing options. Explore and contextualize data with advanced tooltips and zoom-in functions to monitor what your systems are doing in real time. Search your dashboards for attributes and metrics. Send data to your dashboards using our APIs. Transitioning from Insights Switching to using New Relic One dashboards from our deprecated Insights dashboards? See our transition guide. If you're using the Insights Dashboard API, we have have a migration guide that will help you transition to using the new API. Get started with dashboards To access dashboards, go to one.newrelic.com and click on Dashboards on the top navigation menu. In the dashboards index, you can view all the dashboards and data apps associated with your New Relic account. This includes the dashboards you've created within the New Relic One platform as well as the dashboards built in Insights. From the top bar, quickly access our explorer as well as all New Relic capabilities, such as APM, Browser and Infrastructure monitoring, Logs, or Applied Intelligence. You can also use the core New Relic One features such as Search or Query your data that are available across the platform. For each dashboard, the index displays the following information: Favorite status, indicated by a star Name: The name of the dashboard Account: The account the dashboard belongs to Created by: The user who created the dashboard Last edited: When the dashboard was last modified Created on: When the dashboard was created Here you can carry out the following actions: Create a dashboard You can easily create a dashboard in New Relic One from the dashboards index by selecting the + Create a dashboard button located at the top-right corner of the dashboards index. Name your dashboard. Names are searchable, so we recommend giving it a meaningful name (your service or application, for instance) using words that will help you locate your dashboard easily. Select the account the dashboard belongs to. Choose carefully because this action cannot be modified. Press Create to continue, or Cancel to return to the index. Tip By default a dashboard is created with Public - Read and write permissions. You can edit them from the settings menu once you access the dashboard. Alternatively, you can also create a new dashboard: By cloning an existing dashboard. From any chart: Copy any chart from any dashboard to a new or an existing dashboard. From the data explorer or the query builder: Add any chart you create from our querying features to a new or an existing dashboard. From the explorer: Take any custom view from the entity manager over to dashboards. To organize dashboards with multiple pages, see Add pages to a dashboard. Import a dashboard You can import a dashboard as JSON by selecting the Import a dashboard button located at the top-right corner of the dashboards index: Paste the JSON code. By default, the dashboard belongs to the same account as the original dashboard you’re importing. Select a different account if you want to change it. By default, the new dashboard has the same rights as the original dashboard you’re importing. Select different rights if you want to change them. Tip See how to obtain a dashboard’s JSON. Clone a dashboard Clone any dashboard by clicking the Clone dashboard button that appears when you hover over any dashboard row in the index. You can clone any dashboard regardless of your permission levels. The dashboard is automatically copied and the clone is added to the index. Access the new dashboard by clicking on the message that pops up on your screen. The cloned dashboard is named like the original dashboard followed by the word \"copy\". For example, if you clone a dashboard named this is my dashboard, the clone will be created as this is my dashboard copy. The clone has Public - Read and write permissions. You can edit the name and other properties of the dashboard, like the permissions, at any time. Tip The index displays dashboards according to sorting. To quickly find your cloned dashboard, sort the dashboards by creation date. The new dashboard appears on top. Delete a dashboard To delete a dashboard, hover over the dashboard row at the index until the Delete button appears. You can only delete a dashboard if you created it, or if it has Public - Read and write permissions. For more information, see the permissions information. You can also delete a dashboard from the settings panel of the dashboard. Mark a dashboard as favorite Clicking the star icon next to a dashboard toggles on or off the favorites. When you favorite a dashboard, it’s grouped with other favorite dashboards at the top of the list, and appears on the New Relic One home page. To remove a dashboard from your favorites, select the star icon again. New Relic One doesn’t retrieve favorited dashboards from Insights. Learn how to make the transition from Insights to New Relic One. Search and sort dashboards You can search dashboards by dashboard name and author using the search box above the index. You can also sort the dashboards in the index. By default, dashboards you edited recently are at the top of the index in both the favorited and non-favorited sections. To change this order, you can sort both sections by any of the columns in the index, your most recent sort is displayed next time you access New Relic One. Dashboard permissions Dashboards have three types of permissions: Public - Read and write: All users have full rights to the dashboard. Public - Read only: All users are able to see the dashboard, but only you have full rights to work with the dashboard. Other users can access the dashboard but are not able to edit or delete it, although they can clone it. Private: Only you can see the dashboard. When you create a dashboard using the Create a dashboard button or by cloning another dashboard, it will have Public - Read and write rights by default. Access the new dashboard to change this setting. Organize your dashboards with tags You can add tags using our NerdGraph, our tagging API. You can also filter your dashboards by tags, which you can use to identify users, accounts, locations, etc. Click on the tag filter to see the available tags, you can easily select one or more tags from the list to narrow down the dashboards in the index. Key visual tools Dashboards offer intuitive visualization features and tools for advanced data exploration and fast troubleshooting. Consistent chart coloring So that you can quickly see and correlate your data, facets that you apply to more than one chart in a dashboard have a consistent facet color across all the charts. Correlated needle When you mouse over one chart, the correlated needle overlays across all charts or data points in the dashboard at the same time. The tooltip provides the relevant data points from the selected facet, such as maximum and minimum values in a line chart. It also highlights the selected attribute in a pie chart. Data scrubber The chart scrubber helps you select a data point or facet in a chart when the chart is too crowded and facets are too close to each other. Mouse along the needle to smoothly select the adjacent facets and view their associated data points. You can also lighten a heavily populated chart by unselecting one or more of the attributes that appear in the UI. Brush to zoom Drag to select a time segment on any chart and you automatically zoom to that time period on all the charts in the dashboard. The time picker reflects the new period on display in the dashboard. You can return to the default or any other time settings at any time. Custom visualizations You can also make custom visualizations for your dashboards. These enable you to include information from any data source. To learn more about working with custom visualizations, see Build a custom visualization for dashboards and Add custom visualizations to your dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 145.26443,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Introduction</em> to <em>dashboards</em>",
        "sections": "<em>Introduction</em> to <em>dashboards</em>",
        "tags": "<em>Dashboards</em>",
        "body": " <em>guide</em>. If you&#x27;re using the Insights <em>Dashboard</em> API, we have have a migration <em>guide</em> that will help you <em>transition</em> to using the new API. Get started with <em>dashboards</em> To access <em>dashboards</em>, go to one.newrelic.com and click on <em>Dashboards</em> on the top navigation menu. In the <em>dashboards</em> index, you can view all"
      },
      "id": "603ec16028ccbc8d07eba78d"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/release-notes/infrastructure-release-notes/cloud-integration-release-notes/changes-curated-dashboards/",
      "sections": [
        "Changes in curated dashboards",
        "Other changes",
        "New"
      ],
      "published_at": "2021-05-22T10:19:03Z",
      "title": "Changes in curated dashboards",
      "updated_at": "2021-03-16T16:57:40Z",
      "type": "docs",
      "external_id": "e8bcff90d46de24933446830d551bcab22a2da5c",
      "document_type": "release_notes",
      "popularity": 1,
      "body": "Changes in curated dashboards New Relic Infrastructure integrations will not automatically create new dashboards in New Relic Insights when an integration is enabled. Instead, curated dashboards for On-host and Cloud integrations data will be embedded in the New Relic Infrastructure UI, and they can be reached from the following links: On-host integrations: https://infrastructure.newrelic.com/accounts/<your_account_ID>/integrations/onHostIntegrations AWS integrations: https://infrastructure.newrelic.com/accounts/<your_account_ID>/integrations/aws Azure integrations: https://infrastructure.newrelic.com/accounts/<your_account_ID>/integrations/azure GCP integrations: https://infrastructure.newrelic.com/accounts/<your_account_ID>/integrations/gcp Please refer to the transition guide and the Infrastructure integration dashboards and charts documentation for more details about the new pre-built integration dashboards. Note the existing curated dashboards that had been automatically created in New Relic Insights through the InfrastructurePro@newrelic.com user are still available. These dashboards won't be automatically updated anymore, but now you can edit and remove them. By default, they have the same name as the dashboard in New Relic Infrastructure, that takes this format: For on-host integrations: <Integration name> For cloud integrations: <Integration name> - <Linked account name> Other changes The former systemErrors metric in the AWS DynamoDB integration is now reported in several different metrics which represent the total number of requests that generate an HTTP 500 status per operation type. Please refer to AWS DynamoDB monitoring integration for details. New The AWS RDS integration now provides new metrics for RDS database instances and Amazon Aurora clusters. Check AWS RDS monitoring integration for details. The Azure App Service integration now provides new metrics to monitor connections and performance of Web Apps. Check Azure App Service monitoring integration for details.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 145.00542,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Changes <em>in</em> curated <em>dashboards</em>",
        "sections": "Changes <em>in</em> curated <em>dashboards</em>",
        "body": ":&#x2F;&#x2F;<em>infrastructure</em>.newrelic.com&#x2F;accounts&#x2F;&lt;your_account_ID&gt;&#x2F;integrations&#x2F;azure GCP integrations: https:&#x2F;&#x2F;<em>infrastructure</em>.newrelic.com&#x2F;accounts&#x2F;&lt;your_account_ID&gt;&#x2F;integrations&#x2F;gcp Please refer to the <em>transition</em> <em>guide</em> and the <em>Infrastructure</em> <em>integration</em> <em>dashboards</em> and charts documentation for more details about the new pre"
      },
      "id": "603e8dcbe7b9d203892a07e2"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/dropwizard/dropwizard-reporter": [
    {
      "sections": [
        "Introduction to New Relic's open source telemetry integrations",
        "Tip",
        "Types of integrations",
        "How they work"
      ],
      "title": "Introduction to New Relic's open source telemetry integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "Get started"
      ],
      "external_id": "239889ec292525fcfd6b417d243943ea7b3e0529",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations/",
      "published_at": "2021-05-22T08:58:28Z",
      "updated_at": "2021-03-16T06:12:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides open source integrations that report telemetry data from telemetry tools to your New Relic account. Tip To use telemetry integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Types of integrations We have open source integrations that report data from OpenCensus, OpenTelemetry, DropWizard, Prometheus, and more. With these solutions, you can aggregate all your telemetry data in one place: the New Relic platform. See our list of open source telemetry integrations (to browse all New Relic solutions, see our integrations page). How they work These integrations were built using our Telemetry SDKs, which are open-source language-specific libraries for reporting metrics, trace data, and other telemetry data to New Relic. If our pre-built integrations don't meet your needs, you can use the Telemetry SDKs to build your own telemetry tools. Under the hood, data reported by these solutions are ingested via our data ingest APIs. For example, metrics reported by the DropWizard exporter are ingested via the Metric API, so to understand how to query and chart that type of data, you could read Query metric data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 219.74356,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "sections": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to 100GB of data for free each month. Forever. Types of <em>integrations</em> We have <em>open</em> <em>source</em> <em>integrations</em> that report data from <em>Open</em>Census, <em>OpenTelemetry</em>, <em>DropWizard</em>, Prometheus, and more. With these solutions, you can aggregate all your <em>telemetry</em> data in one place: the New Relic platform. See our list"
      },
      "id": "603e95ab28ccbc036aeba789"
    },
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed Tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Metrics Explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/623057d780c245a4a67ec46372991610/5f4af/explorer_filter_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-16T00:19:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Open one.newrelic.com, and go to explorer. Using the filter bar at the top, narrow your searches: Limit results to OpenTelemetry entities with the following: After the OpenTelemetry filter, you can add another filter for your service based on the service.name attributes you've used. For example, to find CartService: Click the service you want to know more about, and if you need help understanding the data, see the explanations below. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed Tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once you’re in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in the response time of your application. From the transaction summary page, you can get a list of transactions by selecting See transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals captures calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On this page, you can see total errors, as well as charts showing error count and error rate. Metrics Explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.3212,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.57361,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/elixir/elixir-open-source-agent": [
    {
      "sections": [
        "Roku open-source agent",
        "Tip",
        "Get started",
        "For more help"
      ],
      "title": "Roku open-source agent",
      "type": "docs",
      "tags": [
        "Agents",
        "Open-source licensed agents",
        "Open-source licensed agents"
      ],
      "external_id": "f0982a0ff96c8a85683bf3ef27a2e4cde85ff274",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/roku/roku-open-source-video-agent/",
      "published_at": "2021-05-26T22:21:40Z",
      "updated_at": "2021-04-27T11:09:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Monitor Roku behavior with New Relic using the Roku open-source agent. The agent contains two parts, to capture two separate categories of Roku behavior: App events like app starts and HTTP requests Video playback within the app Tip This agent is released as open source on GitHub. A change log is also available there for the latest updates. Get started For requirements, installation, and configuration information, see the Open Source Roku Agent README on GitHub. Visit New Relic’s Roku repository on GitHub for questions about installation, usage, or other topics. Report issues or bugs as an issue in the GitHub repository. For more help Recommendations for learning more: Browse New Relic's Explorers Hub for community discussions about the open-source Roku agent. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 423.5448,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Roku <em>open</em>-<em>source</em> <em>agent</em>",
        "sections": "Roku <em>open</em>-<em>source</em> <em>agent</em>",
        "tags": "<em>Open</em>-<em>source</em> <em>licensed</em> <em>agents</em>",
        "body": "Monitor Roku behavior with New Relic using the Roku <em>open</em>-<em>source</em> <em>agent</em>. The <em>agent</em> contains two parts, to capture two separate categories of Roku behavior: App events like app starts and HTTP requests Video playback within the app Tip This <em>agent</em> is released as <em>open</em> <em>source</em> on GitHub. A change log"
      },
      "id": "6087f0ff64441f618a9d8533"
    },
    {
      "image": "https://docs.newrelic.com/static/a0af3af7014ff88dc8c265d4d162da52/ae694/whats-new-rss.png",
      "url": "https://docs.newrelic.com/whats-new/2021/05/rss/",
      "sections": [
        "Keep track of What's New, security bulletins, and more with RSS!",
        "Get a link to the feeds"
      ],
      "published_at": "2021-05-26T09:45:59Z",
      "title": "Keep track of What's New, security bulletins, and more with RSS!",
      "updated_at": "2021-05-26T09:45:59Z",
      "type": "docs",
      "external_id": "6fd1f849e6612473675cfafb7f193fc6dd2a3281",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "Have you heard? RSS is back! And we're on board. We've long had release note feeds for agents and downloadable software from the release notes page. Lately, we've seen lots of requests to add RSS for What's New and our security bulletins, so we've done just that! Now you can subscribe to all the newness New Relic has to offer. You can plug those feeds into your feed reader, of course, but you can also automatically post to a channel with Slack's built in RSS app or Microsoft Team's RSS app. Or go crazy with IFTTT or Zapier for all manner of RSS-based automation. (Want to create a Zapier workflow that turns your smart lightbulbs green every time we release a new Node.js agent? Go for it!) We also release weekly Nerdlog episodes on YouTube (in case you miss our interactive livestream every Thursday at 12 p.m. PT on Twitch) to ensure you know about the latest products and features directly from the product managers and engineers at New Relic who built them, and Nerd Bytes episodes on YouTube for quick tips, tricks, and ideas about New Relic and observability. We've got RSS for those too! Get a link to the feeds What's New: https://docs.newrelic.com/whats-new/feed.xml Or find the feed from the RSS icon on What's New. Security bulletins: https://docs.newrelic.com/docs/security/new-relic-security/security-bulletins/feed.xml Or find the feed from the RSS link on the security bulletins page. Nerdlog: https://www.youtube.com/feeds/videos.xml?playlist_id=PLmhYj7Jl81JGOEHV7zUVfa_iGNyOfMGNh Nerd Bytes: https://www.youtube.com/feeds/videos.xml?playlist_id=PLmhYj7Jl81JEV-llIDkCVC05tD7fbOv_b Agents: We've got too many agents to list all the links! You can find a link by picking your agent from the release notes page and then clicking the RSS icon.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 32.848682,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": "=PLmhYj7Jl81JGOEHV7zUVfa_iGNyOfMGNh Nerd Bytes: https:&#x2F;&#x2F;www.youtube.com&#x2F;feeds&#x2F;videos.xml?playlist_id=PLmhYj7Jl81JEV-llIDkCVC05tD7fbOv_b <em>Agents</em>: We&#x27;ve got too many <em>agents</em> to list all the links! You can find a link by picking your <em>agent</em> from the release notes page and then clicking the RSS icon."
      },
      "id": "60ae18d7196a673bdee2f5f7"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/style-guide/processes-procedures/update-left-navigation-pane/",
      "sections": [
        "Update left-navigation pane",
        "The configuration file",
        "Tip",
        "Configuration file options",
        "Use nav YAML to generate auto index content",
        "Nav requirements for auto index pages",
        "Example"
      ],
      "published_at": "2021-05-21T16:38:01Z",
      "title": "Update left-navigation pane",
      "updated_at": "2021-05-21T16:38:01Z",
      "type": "docs",
      "external_id": "a05e38602a1eb152749923691d251ef2c99f05a9",
      "document_type": "page",
      "popularity": 1,
      "body": "Navigation for docs.newrelic.com is stored in YAML files located in the /src/nav/ directory. Each top-level navigation should have its own configuration file. You'll most likely edit a version of this file in your branch and merge it into the develop branch. The configuration file As an example, here's a snippet of the agents.yml navigation configuration. Note that the file has indentation that corresponds to the level of the navigation hierarchy. Be sure to imitate the same spacing when you make changes: title: Agents path: /docs/agents pages: - title: Manage APM agents path: /docs/agents/manage-apm-agents pages: - title: Agent data path: /docs/agents/manage-apm-agents/agent-data pages: - title: Real time streaming path: /docs/agents/manage-apm-agents/agent-data/real-time-streaming - title: Custom instrumentation path: /docs/agents/manage-apm-agents/agent-data/custom-instrumentation - title: Agent attributes path: /docs/agents/manage-apm-agents/agent-data/agent-attributes - title: Custom events path: /docs/agents/manage-apm-agents/agent-data/collect-custom-events - title: Custom metrics path: /docs/agents/manage-apm-agents/agent-data/collect-custom-metrics - title: Manage errors path: /docs/agents/manage-apm-agents/agent-data/manage-errors-apm-collect-ignore-or-mark-expected - title: Link Kubernetes path: /docs/agents/manage-apm-agents/agent-data/link-your-applications-kubernetes - title: App naming pages: - title: Name your application path: /docs/agents/manage-apm-agents/app-naming/name-your-application - title: Use multiple names for an app path: /docs/agents/manage-apm-agents/app-naming/use-multiple-names-app Copy Tip Navigation nesting is currently limited to a maximum of six levels deep. Please reach out to the engineering team if that is not enough. Configuration file options When you're changing the navigation, keep in mind that each node in the .yml configuration file can have the following properties: Key Required? Description title yes The text that is shown in the navigation. path * no The URL path to the page. Do not use trailing slashes. * Adding a path is required to generate auto index page content. See path requirements below. children no Any sub-navigation nodes. When the user goes to a page, we determine which section of the site they are on and load the appropriate .yml file to populate the sidebar navigation. The navigation for the homepage is an aggregate of all the top-level pages. Tip Each category has its own index.md page (list of pages for that category). When updating the navigation, you may also want to update these pages to reflect the new information architecture more correctly. Use nav YAML to generate auto index content On build we automatically generate an index page for each sub-directory in src/content that does not already have an index.mdx file and that has at least one .mdx or .md file in it. When creating the page we look through each nav YAML file for a reference to the path to know which nav data to use for the left nav and the auto index page content. If an index page path is in more than one nav YAML file, we use the nav that matches its top level path with the beginning of index page path. Nav requirements for auto index pages In order to populate the auto index page with nav YAML data, you must add the index page path somewhere in the most relevant nav YAML file. This helps the auto index page plugin determine which nav to attach to the index page, but also helps the <IndexContents /> component know which section to display on the index page itself. Each nav item must reference a directory path in src/content that contains at least one .md or .mdx file (otherwise the auto index plugin does not create a page for it). Example If this auto index page is used in full-stack-observability.yml and integrations.yml, we will attach the integrations.yml nav to it because: the top level path in the integrations.yml matches the beginning of the auto index page path: title: Integrations path: /docs/integrations Copy matches this auto index page path: /docs/integrations /amazon-integrations/aws-integrations-list Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 31.880196,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": "&#x2F;manage-apm-<em>agents</em> pages: - title: <em>Agent</em> data path: &#x2F;docs&#x2F;<em>agents</em>&#x2F;manage-apm-<em>agents</em>&#x2F;<em>agent</em>-data pages: - title: Real time streaming path: &#x2F;docs&#x2F;<em>agents</em>&#x2F;manage-apm-<em>agents</em>&#x2F;<em>agent</em>-data&#x2F;real-time-streaming - title: Custom instrumentation path: &#x2F;docs&#x2F;<em>agents</em>&#x2F;manage-apm-<em>agents</em>&#x2F;<em>agent</em>-data&#x2F;custom-instrumentation"
      },
      "id": "60422077196a67ad56a83dc8"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed Tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Metrics Explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/623057d780c245a4a67ec46372991610/5f4af/explorer_filter_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-16T00:19:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Open one.newrelic.com, and go to explorer. Using the filter bar at the top, narrow your searches: Limit results to OpenTelemetry entities with the following: After the OpenTelemetry filter, you can add another filter for your service based on the service.name attributes you've used. For example, to find CartService: Click the service you want to know more about, and if you need help understanding the data, see the explanations below. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed Tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once you’re in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in the response time of your application. From the transaction summary page, you can get a list of transactions by selecting See transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals captures calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On this page, you can see total errors, as well as charts showing error count and error rate. Metrics Explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 209.23468,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " your applications, as well as detailed information, such as distributed tracing. To <em>get</em> <em>started</em> with the explorer: <em>Open</em> one.newrelic.com, and go to explorer. Using the filter bar at the top, narrow your searches: Limit results to <em>OpenTelemetry</em> entities with the following: After the <em>OpenTelemetry</em>"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.76445,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick <em>start</em>",
        "sections": "<em>OpenTelemetry</em> quick <em>start</em>",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with <em>OpenTelemetry</em> To <em>get</em> <em>started</em>, you instrument your service with <em>OpenTelemetry</em>. <em>OpenTelemetry</em> has language-specific products"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Tip",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Collector (recommended)",
        "OTLP (pre-release)",
        "What's next?"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-05-22T09:01:08Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can send OpenTelemetry data to New Relic and analyze the results in our observability platform. This introduction can help you decide if you want to get started with OpenTelemetry. If you're already familiar with OpenTelemetry and want to jump into the setup, go to our OpenTelemetry quick start. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can send data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Tip For more, see our blog posts on OpenTelemetry. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start sending data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic APM agents In general, New Relic APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic Here are two basic ways you could set up OpenTelemetry with New Relic: use the OpenTelemetry collector (recommended), or use the native OTLP endpoint (pre-release). Each approach assumes you set up instrumentation in your code, but they differ in how the data is gathered and sent to New Relic. If you are interested in tracing, we recommend that you instrument as many services as you can so that you get the most benefit from distributed tracing. There are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Tip We discuss some variations on these basic approaches in our OpenTelemetry architecture recipes. For a detailed discussion about the architecture of OpenTelemetry itself, see our blog. Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. Collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. In this workflow, note the alternate path if you set up New Relic Infinite Tracing: Here are some advantages of this approach: The OpenTelemetry collector can perform processing operations on telemetry data before it is sent to the backend, including filtering attributes, excluding traces, and sampling. The OpenTelemetry collector can receive telemetry data in many formats and forward it on to New Relic’s backend, including Prometheus, Zipkin, Jaeger, AWS X-Ray, OpenCensus, and much more. The OpenTelemetry collector can collect system-level metrics from the host where it is running. OTLP (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector, or send us data directly from your service. If you are interested, let us know by completing this form. Here are some advantages to operating without an OpenTelemetry collector: There is no need to deploy a collector alongside your service. Telemetry data is sent directly from the instrumented service to New Relic. If something goes wrong, and telemetry data does not arrive, there is one less component to debug. What's next? If you want to set up OpenTelemetry with New Relic, the next step is to check out our OpenTelemetry quick start. If you prefer to use a New Relic APM agent, go to Introduction to APM.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.97408,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "You can send <em>OpenTelemetry</em> data to New Relic and analyze the results in our observability platform. This introduction can help you decide if you want to <em>get</em> <em>started</em> with <em>OpenTelemetry</em>. If you&#x27;re already familiar with <em>OpenTelemetry</em> and want to jump into the setup, go to our <em>OpenTelemetry</em> quick <em>start</em>"
      },
      "id": "603e81ba196a67304da83dab"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/istio/istio-adapter": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed Tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Metrics Explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/623057d780c245a4a67ec46372991610/5f4af/explorer_filter_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-16T00:19:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Open one.newrelic.com, and go to explorer. Using the filter bar at the top, narrow your searches: Limit results to OpenTelemetry entities with the following: After the OpenTelemetry filter, you can add another filter for your service based on the service.name attributes you've used. For example, to find CartService: Click the service you want to know more about, and if you need help understanding the data, see the explanations below. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed Tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once you’re in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in the response time of your application. From the transaction summary page, you can get a list of transactions by selecting See transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals captures calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On this page, you can see total errors, as well as charts showing error count and error rate. Metrics Explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.35794,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.4663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    },
    {
      "sections": [
        "OpenTelemetry architecture recipes",
        "Vendor-neutral option",
        "Tip",
        "Standalone Service",
        "Sidecar/Daemon/Agent",
        "Tail-based sampling",
        "Infinite Tracing with the OpenTelemetry collector (recommended)",
        "Use the Tail Sampling Processor with the OpenTelemetry collector"
      ],
      "title": "OpenTelemetry architecture recipes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "873790aff8692d6fbe627827b197d491ed769908",
      "image": "https://docs.newrelic.com/static/25aef685a3066a189f0cc5ec0a40eb4d/1d69c/collector_standalone_overview.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-architecture-recipes/",
      "published_at": "2021-05-22T09:01:11Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is powerful because of its flexibility, but this flexibility can be overwhelming as you consider ways to implement it. Here are some architectural recipes to give you you some context as you explore OpenTelemetry. Vendor-neutral option Use this recipe if you don’t want to add any vendor-specific code to your applications. As a best practice to maximize value and interoperability, we recommend the OpenTelemetry collector as the main ingredient of this recipe. The OpenTelemetry collector proxies data between instrumented code and backend services. Tip We have a pre-release program that allows you to use the OTLP exporter in the OpenTelemetry collector. If you are interested, let us know by completing this form. The OpenTelemetry collector has one or more pipelines. Each pipeline includes: A receiver that accepts telemetry data coming from instrumentation. Zero or more processors that perform computations or mutations on the telemetry data. One or more exporters that send telemetry data downstream. New Relic (along with many other community members) contributes its exporter to the opentelemetry-collector-contrib project. You can enable and configure the exporters in this project without changing instrumented code. Some example exporters are: The New Relic exporter that sends metric and trace data to New Relic. A logging exporter that logs every data point to STDOUT. A Prometheus exporter that exposes a Prometheus endpoint to be scraped by a downstream Prometheus system. The OpenTelemetry Collector (with exporters) is available as a pre-built docker image. Two common collector deployments are the standalone service and sidecar. We cover them briefly here, but if you need more detail, see the OpenTelemetry documentation. Standalone Service When you set up the collector as a standalone service, OpenTelemetry calls this a gateway: Sidecar/Daemon/Agent If you set up the collector as a sidecar, OpenTelemetry calls this an agent: Tail-based sampling By default, OpenTelemetry uses head-based sampling, which means traces are randomly sampled up front before details about the completed traces are known. As an alternative, you can configure OpenTelemetry to use tail-based sampling, which analyzes all your traces after they're completed. Infinite Tracing with the OpenTelemetry collector (recommended) If you want to use tail-based sampling, set up the collector to send every span to a trace observer in New Relic Edge where they're analyzed. For more information about this option for tail-based sampling, see Introduction to Infinite Tracing. In this approach, the collector and the trace observer handle the data before it's sent to New Relic: Use the Tail Sampling Processor with the OpenTelemetry collector Another option for tail-based sampling is to configure the OpenTelemetry collector to use the Tail Sampling Processor to sample traces prior to sending to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 152.81772,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> architecture recipes",
        "sections": "<em>OpenTelemetry</em> architecture recipes",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is powerful because of its flexibility, but this flexibility can be overwhelming as you consider ways to implement it. Here are some architectural recipes to give you you some context as you explore <em>OpenTelemetry</em>. Vendor-neutral option Use this recipe if you don’t want to add any"
      },
      "id": "6044e5e028ccbc04a232a725"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/kamon/kamon-reporter": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed Tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Metrics Explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/623057d780c245a4a67ec46372991610/5f4af/explorer_filter_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-16T00:19:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Open one.newrelic.com, and go to explorer. Using the filter bar at the top, narrow your searches: Limit results to OpenTelemetry entities with the following: After the OpenTelemetry filter, you can add another filter for your service based on the service.name attributes you've used. For example, to find CartService: Click the service you want to know more about, and if you need help understanding the data, see the explanations below. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed Tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once you’re in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in the response time of your application. From the transaction summary page, you can get a list of transactions by selecting See transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals captures calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On this page, you can see total errors, as well as charts showing error count and error rate. Metrics Explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.3211,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.57355,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    },
    {
      "sections": [
        "OpenTelemetry architecture recipes",
        "Vendor-neutral option",
        "Tip",
        "Standalone Service",
        "Sidecar/Daemon/Agent",
        "Tail-based sampling",
        "Infinite Tracing with the OpenTelemetry collector (recommended)",
        "Use the Tail Sampling Processor with the OpenTelemetry collector"
      ],
      "title": "OpenTelemetry architecture recipes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "873790aff8692d6fbe627827b197d491ed769908",
      "image": "https://docs.newrelic.com/static/25aef685a3066a189f0cc5ec0a40eb4d/1d69c/collector_standalone_overview.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-architecture-recipes/",
      "published_at": "2021-05-22T09:01:11Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is powerful because of its flexibility, but this flexibility can be overwhelming as you consider ways to implement it. Here are some architectural recipes to give you you some context as you explore OpenTelemetry. Vendor-neutral option Use this recipe if you don’t want to add any vendor-specific code to your applications. As a best practice to maximize value and interoperability, we recommend the OpenTelemetry collector as the main ingredient of this recipe. The OpenTelemetry collector proxies data between instrumented code and backend services. Tip We have a pre-release program that allows you to use the OTLP exporter in the OpenTelemetry collector. If you are interested, let us know by completing this form. The OpenTelemetry collector has one or more pipelines. Each pipeline includes: A receiver that accepts telemetry data coming from instrumentation. Zero or more processors that perform computations or mutations on the telemetry data. One or more exporters that send telemetry data downstream. New Relic (along with many other community members) contributes its exporter to the opentelemetry-collector-contrib project. You can enable and configure the exporters in this project without changing instrumented code. Some example exporters are: The New Relic exporter that sends metric and trace data to New Relic. A logging exporter that logs every data point to STDOUT. A Prometheus exporter that exposes a Prometheus endpoint to be scraped by a downstream Prometheus system. The OpenTelemetry Collector (with exporters) is available as a pre-built docker image. Two common collector deployments are the standalone service and sidecar. We cover them briefly here, but if you need more detail, see the OpenTelemetry documentation. Standalone Service When you set up the collector as a standalone service, OpenTelemetry calls this a gateway: Sidecar/Daemon/Agent If you set up the collector as a sidecar, OpenTelemetry calls this an agent: Tail-based sampling By default, OpenTelemetry uses head-based sampling, which means traces are randomly sampled up front before details about the completed traces are known. As an alternative, you can configure OpenTelemetry to use tail-based sampling, which analyzes all your traces after they're completed. Infinite Tracing with the OpenTelemetry collector (recommended) If you want to use tail-based sampling, set up the collector to send every span to a trace observer in New Relic Edge where they're analyzed. For more information about this option for tail-based sampling, see Introduction to Infinite Tracing. In this approach, the collector and the trace observer handle the data before it's sent to New Relic: Use the Tail Sampling Processor with the OpenTelemetry collector Another option for tail-based sampling is to configure the OpenTelemetry collector to use the Tail Sampling Processor to sample traces prior to sending to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.12851,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> architecture recipes",
        "sections": "<em>OpenTelemetry</em> architecture recipes",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is powerful because of its flexibility, but this flexibility can be overwhelming as you consider ways to implement it. Here are some architectural recipes to give you you some context as you explore <em>OpenTelemetry</em>. Vendor-neutral option Use this recipe if you don’t want to add any"
      },
      "id": "6044e5e028ccbc04a232a725"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/micrometer/micrometer-metrics-registry": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed Tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Metrics Explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/623057d780c245a4a67ec46372991610/5f4af/explorer_filter_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-16T00:19:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Open one.newrelic.com, and go to explorer. Using the filter bar at the top, narrow your searches: Limit results to OpenTelemetry entities with the following: After the OpenTelemetry filter, you can add another filter for your service based on the service.name attributes you've used. For example, to find CartService: Click the service you want to know more about, and if you need help understanding the data, see the explanations below. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed Tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once you’re in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in the response time of your application. From the transaction summary page, you can get a list of transactions by selecting See transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals captures calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On this page, you can see total errors, as well as charts showing error count and error rate. Metrics Explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.35788,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.46628,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    },
    {
      "sections": [
        "OpenTelemetry architecture recipes",
        "Vendor-neutral option",
        "Tip",
        "Standalone Service",
        "Sidecar/Daemon/Agent",
        "Tail-based sampling",
        "Infinite Tracing with the OpenTelemetry collector (recommended)",
        "Use the Tail Sampling Processor with the OpenTelemetry collector"
      ],
      "title": "OpenTelemetry architecture recipes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "873790aff8692d6fbe627827b197d491ed769908",
      "image": "https://docs.newrelic.com/static/25aef685a3066a189f0cc5ec0a40eb4d/1d69c/collector_standalone_overview.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-architecture-recipes/",
      "published_at": "2021-05-22T09:01:11Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is powerful because of its flexibility, but this flexibility can be overwhelming as you consider ways to implement it. Here are some architectural recipes to give you you some context as you explore OpenTelemetry. Vendor-neutral option Use this recipe if you don’t want to add any vendor-specific code to your applications. As a best practice to maximize value and interoperability, we recommend the OpenTelemetry collector as the main ingredient of this recipe. The OpenTelemetry collector proxies data between instrumented code and backend services. Tip We have a pre-release program that allows you to use the OTLP exporter in the OpenTelemetry collector. If you are interested, let us know by completing this form. The OpenTelemetry collector has one or more pipelines. Each pipeline includes: A receiver that accepts telemetry data coming from instrumentation. Zero or more processors that perform computations or mutations on the telemetry data. One or more exporters that send telemetry data downstream. New Relic (along with many other community members) contributes its exporter to the opentelemetry-collector-contrib project. You can enable and configure the exporters in this project without changing instrumented code. Some example exporters are: The New Relic exporter that sends metric and trace data to New Relic. A logging exporter that logs every data point to STDOUT. A Prometheus exporter that exposes a Prometheus endpoint to be scraped by a downstream Prometheus system. The OpenTelemetry Collector (with exporters) is available as a pre-built docker image. Two common collector deployments are the standalone service and sidecar. We cover them briefly here, but if you need more detail, see the OpenTelemetry documentation. Standalone Service When you set up the collector as a standalone service, OpenTelemetry calls this a gateway: Sidecar/Daemon/Agent If you set up the collector as a sidecar, OpenTelemetry calls this an agent: Tail-based sampling By default, OpenTelemetry uses head-based sampling, which means traces are randomly sampled up front before details about the completed traces are known. As an alternative, you can configure OpenTelemetry to use tail-based sampling, which analyzes all your traces after they're completed. Infinite Tracing with the OpenTelemetry collector (recommended) If you want to use tail-based sampling, set up the collector to send every span to a trace observer in New Relic Edge where they're analyzed. For more information about this option for tail-based sampling, see Introduction to Infinite Tracing. In this approach, the collector and the trace observer handle the data before it's sent to New Relic: Use the Tail Sampling Processor with the OpenTelemetry collector Another option for tail-based sampling is to configure the OpenTelemetry collector to use the Tail Sampling Processor to sample traces prior to sending to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 152.8177,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> architecture recipes",
        "sections": "<em>OpenTelemetry</em> architecture recipes",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is powerful because of its flexibility, but this flexibility can be overwhelming as you consider ways to implement it. Here are some architectural recipes to give you you some context as you explore <em>OpenTelemetry</em>. Vendor-neutral option Use this recipe if you don’t want to add any"
      },
      "id": "6044e5e028ccbc04a232a725"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opencensus/opencensus-exporter": [
    {
      "sections": [
        "Introduction to New Relic's open source telemetry integrations",
        "Tip",
        "Types of integrations",
        "How they work"
      ],
      "title": "Introduction to New Relic's open source telemetry integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "Get started"
      ],
      "external_id": "239889ec292525fcfd6b417d243943ea7b3e0529",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations/",
      "published_at": "2021-05-22T08:58:28Z",
      "updated_at": "2021-03-16T06:12:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides open source integrations that report telemetry data from telemetry tools to your New Relic account. Tip To use telemetry integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Types of integrations We have open source integrations that report data from OpenCensus, OpenTelemetry, DropWizard, Prometheus, and more. With these solutions, you can aggregate all your telemetry data in one place: the New Relic platform. See our list of open source telemetry integrations (to browse all New Relic solutions, see our integrations page). How they work These integrations were built using our Telemetry SDKs, which are open-source language-specific libraries for reporting metrics, trace data, and other telemetry data to New Relic. If our pre-built integrations don't meet your needs, you can use the Telemetry SDKs to build your own telemetry tools. Under the hood, data reported by these solutions are ingested via our data ingest APIs. For example, metrics reported by the DropWizard exporter are ingested via the Metric API, so to understand how to query and chart that type of data, you could read Query metric data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 302.76242,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "sections": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to 100GB of data for free each month. Forever. Types of <em>integrations</em> We have <em>open</em> <em>source</em> <em>integrations</em> that report data from <em>OpenCensus</em>, <em>OpenTelemetry</em>, DropWizard, Prometheus, and more. With these solutions, you can aggregate all your <em>telemetry</em> data in one place: the New Relic platform. See our list"
      },
      "id": "603e95ab28ccbc036aeba789"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Tip",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Collector (recommended)",
        "OTLP (pre-release)",
        "What's next?"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-05-22T09:01:08Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can send OpenTelemetry data to New Relic and analyze the results in our observability platform. This introduction can help you decide if you want to get started with OpenTelemetry. If you're already familiar with OpenTelemetry and want to jump into the setup, go to our OpenTelemetry quick start. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can send data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Tip For more, see our blog posts on OpenTelemetry. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start sending data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic APM agents In general, New Relic APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic Here are two basic ways you could set up OpenTelemetry with New Relic: use the OpenTelemetry collector (recommended), or use the native OTLP endpoint (pre-release). Each approach assumes you set up instrumentation in your code, but they differ in how the data is gathered and sent to New Relic. If you are interested in tracing, we recommend that you instrument as many services as you can so that you get the most benefit from distributed tracing. There are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Tip We discuss some variations on these basic approaches in our OpenTelemetry architecture recipes. For a detailed discussion about the architecture of OpenTelemetry itself, see our blog. Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. Collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. In this workflow, note the alternate path if you set up New Relic Infinite Tracing: Here are some advantages of this approach: The OpenTelemetry collector can perform processing operations on telemetry data before it is sent to the backend, including filtering attributes, excluding traces, and sampling. The OpenTelemetry collector can receive telemetry data in many formats and forward it on to New Relic’s backend, including Prometheus, Zipkin, Jaeger, AWS X-Ray, OpenCensus, and much more. The OpenTelemetry collector can collect system-level metrics from the host where it is running. OTLP (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector, or send us data directly from your service. If you are interested, let us know by completing this form. Here are some advantages to operating without an OpenTelemetry collector: There is no need to deploy a collector alongside your service. Telemetry data is sent directly from the instrumented service to New Relic. If something goes wrong, and telemetry data does not arrive, there is one less component to debug. What's next? If you want to set up OpenTelemetry with New Relic, the next step is to check out our OpenTelemetry quick start. If you prefer to use a New Relic APM agent, go to Introduction to APM.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.41443,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": ", and as more vendors move to support this <em>open</em> standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/intro-custom-data/",
      "sections": [
        "Introduction to custom data",
        "Options for custom data"
      ],
      "published_at": "2021-05-21T14:32:42Z",
      "title": "Introduction to custom data",
      "updated_at": "2021-05-15T10:42:42Z",
      "type": "docs",
      "external_id": "e62230f7bdaa3f3bfbde0560f0ee3b6e68c4be40",
      "document_type": "page",
      "popularity": 1,
      "body": "To get the most out of New Relic, you may need or want to report custom data to help you get insights about your environment and the unique challenges your organization faces. Options for custom data We have a wide variety of agents and integrations that bring in various types of data. But you may need to bring in data that isn't reported by default. Options for bringing in custom data include: Use our open source telemetry integrations. We have integrations for reporting data from OpenCensus, OpenTelemetry, DropWizard, Prometheus, and more. Configure our agents to send custom data. If you're using one of our APM agents, browser agents, or mobile agents, you can configure them to send custom data. To learn more, see the docs for the tool you're using. Create your own solution for reporting telemetry data with our language-specific Telemetry SDKs. Or use the underlying APIs directly: Metric API, Trace API, Event API, Log API. Report data from your operating system or network. If you're using our infrastructure agent, you might want to use our Flex integration, which lets you use simple config files to report data. Build a New Relic One application. You can build an app that lives on our platform, that can be shared with your team or the public, and that uses your own JavaScript UI functionality. You can analyze New Relic data, or bring in whatever data you want.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 278.23965,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " need to bring in data that isn&#x27;t reported by default. Options for bringing in custom data include: Use our <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>. We have <em>integrations</em> for reporting data from <em>OpenCensus</em>, <em>OpenTelemetry</em>, DropWizard, Prometheus, and more. Configure our agents to send custom data. If you&#x27;re"
      },
      "id": "609fa5a264441f731bd2a1cf"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed Tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Metrics Explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/623057d780c245a4a67ec46372991610/5f4af/explorer_filter_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-16T00:19:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Open one.newrelic.com, and go to explorer. Using the filter bar at the top, narrow your searches: Limit results to OpenTelemetry entities with the following: After the OpenTelemetry filter, you can add another filter for your service based on the service.name attributes you've used. For example, to find CartService: Click the service you want to know more about, and if you need help understanding the data, see the explanations below. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed Tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once you’re in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in the response time of your application. From the transaction summary page, you can get a list of transactions by selecting See transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals captures calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On this page, you can see total errors, as well as charts showing error count and error rate. Metrics Explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.9986,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 258.46017,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    },
    {
      "sections": [
        "Troubleshoot the OpenTelemetry exporter",
        "Install",
        "Traces",
        "Trace sampling",
        "Metrics",
        "Caution",
        "Errors"
      ],
      "title": "Troubleshoot the OpenTelemetry exporter",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "524e7f170806e42fddc8dee79c8da64b1640b8fb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/troubleshooting/troubleshoot-opentelemetry-exporter/",
      "published_at": "2021-05-22T16:00:43Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is an emerging standard for service-level instrumentation. Because it has not yet reached 1.0 for any language, the OpenTelemetry SDK, APIs, and instrumentation are subject to change. If you're using New Relic's OpenTelemetry exporter, but you're not seeing the data you expect, this guide should help. OpenTelemetry currently supports two telemetry types: distributed traces and metrics. Through the New Relic UI, you can see your traces and metrics, as well as a view that focuses on errors. Learn more about how to find your data. Install To send your data through the OpenTelemetry collector, configure your OpenTelemetry SDK service.name as a resource and configure the SDK to send data to the collector. Then configure the collector with the New Relic exporter with your Insert API key. Consult the collector documentation if you're having trouble sending data through the collector. Once OpenTelemetry is configured with the API key and service name, you should be able to find your service in the explorer. Select your service to view its summary page. Traces When viewing your OpenTelemetry data, on the left side of the UI you can select Distributed tracing to see distributed traces. If you don’t see traces: Ensure your service was reporting during the time window. Ensure you've added OpenTelemetry API calls to create trace spans. If you're using OpenTelemetry auto-instrumentation, ensure the instrumentation supports the libraries and frameworks you're using. Traces are the most mature telemetry type in OpenTelemetry, so you'll find the richest performance information from traces. Trace sampling Our trace UI will only reflect the data sent to us. This means that throughput and other counts may not reflect the unsampled data. You can configure sampling application-side using the OpenTelemetry SDKs. If you enable our Infinite Tracing feature, you have the option to send us all of your trace data and use New Relic sampling. Note that New Relic may rate-limit trace data. Metrics When viewing your OpenTelemetry data, on the left side of the UI you can select Metrics explorer to visualize metrics and create dashboards. If you don’t see metrics, you may need to add OpenTelemetry API calls to add the metrics that you care about. Caution The OpenTelemetry standard for metric naming is still being defined and to date most instrumentation includes trace spans but not metrics. We expect that more instrumentation will include metrics in the coming months. Errors When viewing your OpenTelemetry data, on the left side of the UI you can select Errors to understand what errors your service is generating. The count of errors is based on how many traces have a span with the attribute error.stack. If you don’t see the errors listed that you expect to see, check that your instrumentation sets the following three attributes: error.stack error.msg error.type In order for the error to be listed, a span must have an attribute that starts with error. Caution The OpenTelemetry standard for representing errors in traces and metrics is still being defined so these attribute names are subject to change. We expect the specification to be frozen very soon.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.28845,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Troubleshoot the <em>OpenTelemetry</em> exporter",
        "sections": "Troubleshoot the <em>OpenTelemetry</em> exporter",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is an emerging standard for service-level instrumentation. Because it has not yet reached 1.0 for any language, the <em>OpenTelemetry</em> SDK, APIs, and instrumentation are subject to change. If you&#x27;re using New Relic&#x27;s <em>OpenTelemetry</em> exporter, but you&#x27;re not seeing the data you expect"
      },
      "id": "603e897de7b9d2ca9b2a07d6"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-advanced-configuration": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed Tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Metrics Explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/623057d780c245a4a67ec46372991610/5f4af/explorer_filter_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-16T00:19:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Open one.newrelic.com, and go to explorer. Using the filter bar at the top, narrow your searches: Limit results to OpenTelemetry entities with the following: After the OpenTelemetry filter, you can add another filter for your service based on the service.name attributes you've used. For example, to find CartService: Click the service you want to know more about, and if you need help understanding the data, see the explanations below. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed Tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once you’re in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in the response time of your application. From the transaction summary page, you can get a list of transactions by selecting See transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals captures calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On this page, you can see total errors, as well as charts showing error count and error rate. Metrics Explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.99854,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 258.46014,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Tip",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Collector (recommended)",
        "OTLP (pre-release)",
        "What's next?"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-05-22T09:01:08Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can send OpenTelemetry data to New Relic and analyze the results in our observability platform. This introduction can help you decide if you want to get started with OpenTelemetry. If you're already familiar with OpenTelemetry and want to jump into the setup, go to our OpenTelemetry quick start. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can send data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Tip For more, see our blog posts on OpenTelemetry. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start sending data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic APM agents In general, New Relic APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic Here are two basic ways you could set up OpenTelemetry with New Relic: use the OpenTelemetry collector (recommended), or use the native OTLP endpoint (pre-release). Each approach assumes you set up instrumentation in your code, but they differ in how the data is gathered and sent to New Relic. If you are interested in tracing, we recommend that you instrument as many services as you can so that you get the most benefit from distributed tracing. There are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Tip We discuss some variations on these basic approaches in our OpenTelemetry architecture recipes. For a detailed discussion about the architecture of OpenTelemetry itself, see our blog. Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. Collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. In this workflow, note the alternate path if you set up New Relic Infinite Tracing: Here are some advantages of this approach: The OpenTelemetry collector can perform processing operations on telemetry data before it is sent to the backend, including filtering attributes, excluding traces, and sampling. The OpenTelemetry collector can receive telemetry data in many formats and forward it on to New Relic’s backend, including Prometheus, Zipkin, Jaeger, AWS X-Ray, OpenCensus, and much more. The OpenTelemetry collector can collect system-level metrics from the host where it is running. OTLP (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector, or send us data directly from your service. If you are interested, let us know by completing this form. Here are some advantages to operating without an OpenTelemetry collector: There is no need to deploy a collector alongside your service. Telemetry data is sent directly from the instrumented service to New Relic. If something goes wrong, and telemetry data does not arrive, there is one less component to debug. What's next? If you want to set up OpenTelemetry with New Relic, the next step is to check out our OpenTelemetry quick start. If you prefer to use a New Relic APM agent, go to Introduction to APM.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.51141,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": ", and as more vendors move to support this <em>open</em> standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end"
      },
      "id": "603e81ba196a67304da83dab"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-architecture-recipes": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed Tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Metrics Explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/623057d780c245a4a67ec46372991610/5f4af/explorer_filter_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-16T00:19:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Open one.newrelic.com, and go to explorer. Using the filter bar at the top, narrow your searches: Limit results to OpenTelemetry entities with the following: After the OpenTelemetry filter, you can add another filter for your service based on the service.name attributes you've used. For example, to find CartService: Click the service you want to know more about, and if you need help understanding the data, see the explanations below. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed Tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once you’re in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in the response time of your application. From the transaction summary page, you can get a list of transactions by selecting See transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals captures calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On this page, you can see total errors, as well as charts showing error count and error rate. Metrics Explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.99854,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 258.46014,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Tip",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Collector (recommended)",
        "OTLP (pre-release)",
        "What's next?"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-05-22T09:01:08Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can send OpenTelemetry data to New Relic and analyze the results in our observability platform. This introduction can help you decide if you want to get started with OpenTelemetry. If you're already familiar with OpenTelemetry and want to jump into the setup, go to our OpenTelemetry quick start. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can send data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Tip For more, see our blog posts on OpenTelemetry. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start sending data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic APM agents In general, New Relic APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic Here are two basic ways you could set up OpenTelemetry with New Relic: use the OpenTelemetry collector (recommended), or use the native OTLP endpoint (pre-release). Each approach assumes you set up instrumentation in your code, but they differ in how the data is gathered and sent to New Relic. If you are interested in tracing, we recommend that you instrument as many services as you can so that you get the most benefit from distributed tracing. There are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Tip We discuss some variations on these basic approaches in our OpenTelemetry architecture recipes. For a detailed discussion about the architecture of OpenTelemetry itself, see our blog. Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. Collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. In this workflow, note the alternate path if you set up New Relic Infinite Tracing: Here are some advantages of this approach: The OpenTelemetry collector can perform processing operations on telemetry data before it is sent to the backend, including filtering attributes, excluding traces, and sampling. The OpenTelemetry collector can receive telemetry data in many formats and forward it on to New Relic’s backend, including Prometheus, Zipkin, Jaeger, AWS X-Ray, OpenCensus, and much more. The OpenTelemetry collector can collect system-level metrics from the host where it is running. OTLP (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector, or send us data directly from your service. If you are interested, let us know by completing this form. Here are some advantages to operating without an OpenTelemetry collector: There is no need to deploy a collector alongside your service. Telemetry data is sent directly from the instrumented service to New Relic. If something goes wrong, and telemetry data does not arrive, there is one less component to debug. What's next? If you want to set up OpenTelemetry with New Relic, the next step is to check out our OpenTelemetry quick start. If you prefer to use a New Relic APM agent, go to Introduction to APM.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.51141,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": ", and as more vendors move to support this <em>open</em> standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end"
      },
      "id": "603e81ba196a67304da83dab"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed Tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Metrics Explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/623057d780c245a4a67ec46372991610/5f4af/explorer_filter_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-16T00:19:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Open one.newrelic.com, and go to explorer. Using the filter bar at the top, narrow your searches: Limit results to OpenTelemetry entities with the following: After the OpenTelemetry filter, you can add another filter for your service based on the service.name attributes you've used. For example, to find CartService: Click the service you want to know more about, and if you need help understanding the data, see the explanations below. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed Tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once you’re in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in the response time of your application. From the transaction summary page, you can get a list of transactions by selecting See transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals captures calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On this page, you can see total errors, as well as charts showing error count and error rate. Metrics Explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.99844,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Tip",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Collector (recommended)",
        "OTLP (pre-release)",
        "What's next?"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-05-22T09:01:08Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can send OpenTelemetry data to New Relic and analyze the results in our observability platform. This introduction can help you decide if you want to get started with OpenTelemetry. If you're already familiar with OpenTelemetry and want to jump into the setup, go to our OpenTelemetry quick start. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can send data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Tip For more, see our blog posts on OpenTelemetry. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start sending data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic APM agents In general, New Relic APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic Here are two basic ways you could set up OpenTelemetry with New Relic: use the OpenTelemetry collector (recommended), or use the native OTLP endpoint (pre-release). Each approach assumes you set up instrumentation in your code, but they differ in how the data is gathered and sent to New Relic. If you are interested in tracing, we recommend that you instrument as many services as you can so that you get the most benefit from distributed tracing. There are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Tip We discuss some variations on these basic approaches in our OpenTelemetry architecture recipes. For a detailed discussion about the architecture of OpenTelemetry itself, see our blog. Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. Collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. In this workflow, note the alternate path if you set up New Relic Infinite Tracing: Here are some advantages of this approach: The OpenTelemetry collector can perform processing operations on telemetry data before it is sent to the backend, including filtering attributes, excluding traces, and sampling. The OpenTelemetry collector can receive telemetry data in many formats and forward it on to New Relic’s backend, including Prometheus, Zipkin, Jaeger, AWS X-Ray, OpenCensus, and much more. The OpenTelemetry collector can collect system-level metrics from the host where it is running. OTLP (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector, or send us data directly from your service. If you are interested, let us know by completing this form. Here are some advantages to operating without an OpenTelemetry collector: There is no need to deploy a collector alongside your service. Telemetry data is sent directly from the instrumented service to New Relic. If something goes wrong, and telemetry data does not arrive, there is one less component to debug. What's next? If you want to set up OpenTelemetry with New Relic, the next step is to check out our OpenTelemetry quick start. If you prefer to use a New Relic APM agent, go to Introduction to APM.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.51141,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": ", and as more vendors move to support this <em>open</em> standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "sections": [
        "Troubleshoot the OpenTelemetry exporter",
        "Install",
        "Traces",
        "Trace sampling",
        "Metrics",
        "Caution",
        "Errors"
      ],
      "title": "Troubleshoot the OpenTelemetry exporter",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "524e7f170806e42fddc8dee79c8da64b1640b8fb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/troubleshooting/troubleshoot-opentelemetry-exporter/",
      "published_at": "2021-05-22T16:00:43Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is an emerging standard for service-level instrumentation. Because it has not yet reached 1.0 for any language, the OpenTelemetry SDK, APIs, and instrumentation are subject to change. If you're using New Relic's OpenTelemetry exporter, but you're not seeing the data you expect, this guide should help. OpenTelemetry currently supports two telemetry types: distributed traces and metrics. Through the New Relic UI, you can see your traces and metrics, as well as a view that focuses on errors. Learn more about how to find your data. Install To send your data through the OpenTelemetry collector, configure your OpenTelemetry SDK service.name as a resource and configure the SDK to send data to the collector. Then configure the collector with the New Relic exporter with your Insert API key. Consult the collector documentation if you're having trouble sending data through the collector. Once OpenTelemetry is configured with the API key and service name, you should be able to find your service in the explorer. Select your service to view its summary page. Traces When viewing your OpenTelemetry data, on the left side of the UI you can select Distributed tracing to see distributed traces. If you don’t see traces: Ensure your service was reporting during the time window. Ensure you've added OpenTelemetry API calls to create trace spans. If you're using OpenTelemetry auto-instrumentation, ensure the instrumentation supports the libraries and frameworks you're using. Traces are the most mature telemetry type in OpenTelemetry, so you'll find the richest performance information from traces. Trace sampling Our trace UI will only reflect the data sent to us. This means that throughput and other counts may not reflect the unsampled data. You can configure sampling application-side using the OpenTelemetry SDKs. If you enable our Infinite Tracing feature, you have the option to send us all of your trace data and use New Relic sampling. Note that New Relic may rate-limit trace data. Metrics When viewing your OpenTelemetry data, on the left side of the UI you can select Metrics explorer to visualize metrics and create dashboards. If you don’t see metrics, you may need to add OpenTelemetry API calls to add the metrics that you care about. Caution The OpenTelemetry standard for metric naming is still being defined and to date most instrumentation includes trace spans but not metrics. We expect that more instrumentation will include metrics in the coming months. Errors When viewing your OpenTelemetry data, on the left side of the UI you can select Errors to understand what errors your service is generating. The count of errors is based on how many traces have a span with the attribute error.stack. If you don’t see the errors listed that you expect to see, check that your instrumentation sets the following three attributes: error.stack error.msg error.type In order for the error to be listed, a span must have an attribute that starts with error. Caution The OpenTelemetry standard for representing errors in traces and metrics is still being defined so these attribute names are subject to change. We expect the specification to be frozen very soon.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.28842,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Troubleshoot the <em>OpenTelemetry</em> exporter",
        "sections": "Troubleshoot the <em>OpenTelemetry</em> exporter",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is an emerging standard for service-level instrumentation. Because it has not yet reached 1.0 for any language, the <em>OpenTelemetry</em> SDK, APIs, and instrumentation are subject to change. If you&#x27;re using New Relic&#x27;s <em>OpenTelemetry</em> exporter, but you&#x27;re not seeing the data you expect"
      },
      "id": "603e897de7b9d2ca9b2a07d6"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic": [
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 258.46008,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Tip",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Collector (recommended)",
        "OTLP (pre-release)",
        "What's next?"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-05-22T09:01:08Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can send OpenTelemetry data to New Relic and analyze the results in our observability platform. This introduction can help you decide if you want to get started with OpenTelemetry. If you're already familiar with OpenTelemetry and want to jump into the setup, go to our OpenTelemetry quick start. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can send data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Tip For more, see our blog posts on OpenTelemetry. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start sending data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic APM agents In general, New Relic APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic Here are two basic ways you could set up OpenTelemetry with New Relic: use the OpenTelemetry collector (recommended), or use the native OTLP endpoint (pre-release). Each approach assumes you set up instrumentation in your code, but they differ in how the data is gathered and sent to New Relic. If you are interested in tracing, we recommend that you instrument as many services as you can so that you get the most benefit from distributed tracing. There are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Tip We discuss some variations on these basic approaches in our OpenTelemetry architecture recipes. For a detailed discussion about the architecture of OpenTelemetry itself, see our blog. Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. Collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. In this workflow, note the alternate path if you set up New Relic Infinite Tracing: Here are some advantages of this approach: The OpenTelemetry collector can perform processing operations on telemetry data before it is sent to the backend, including filtering attributes, excluding traces, and sampling. The OpenTelemetry collector can receive telemetry data in many formats and forward it on to New Relic’s backend, including Prometheus, Zipkin, Jaeger, AWS X-Ray, OpenCensus, and much more. The OpenTelemetry collector can collect system-level metrics from the host where it is running. OTLP (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector, or send us data directly from your service. If you are interested, let us know by completing this form. Here are some advantages to operating without an OpenTelemetry collector: There is no need to deploy a collector alongside your service. Telemetry data is sent directly from the instrumented service to New Relic. If something goes wrong, and telemetry data does not arrive, there is one less component to debug. What's next? If you want to set up OpenTelemetry with New Relic, the next step is to check out our OpenTelemetry quick start. If you prefer to use a New Relic APM agent, go to Introduction to APM.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.51141,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": ", and as more vendors move to support this <em>open</em> standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "sections": [
        "Troubleshoot the OpenTelemetry exporter",
        "Install",
        "Traces",
        "Trace sampling",
        "Metrics",
        "Caution",
        "Errors"
      ],
      "title": "Troubleshoot the OpenTelemetry exporter",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "524e7f170806e42fddc8dee79c8da64b1640b8fb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/troubleshooting/troubleshoot-opentelemetry-exporter/",
      "published_at": "2021-05-22T16:00:43Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is an emerging standard for service-level instrumentation. Because it has not yet reached 1.0 for any language, the OpenTelemetry SDK, APIs, and instrumentation are subject to change. If you're using New Relic's OpenTelemetry exporter, but you're not seeing the data you expect, this guide should help. OpenTelemetry currently supports two telemetry types: distributed traces and metrics. Through the New Relic UI, you can see your traces and metrics, as well as a view that focuses on errors. Learn more about how to find your data. Install To send your data through the OpenTelemetry collector, configure your OpenTelemetry SDK service.name as a resource and configure the SDK to send data to the collector. Then configure the collector with the New Relic exporter with your Insert API key. Consult the collector documentation if you're having trouble sending data through the collector. Once OpenTelemetry is configured with the API key and service name, you should be able to find your service in the explorer. Select your service to view its summary page. Traces When viewing your OpenTelemetry data, on the left side of the UI you can select Distributed tracing to see distributed traces. If you don’t see traces: Ensure your service was reporting during the time window. Ensure you've added OpenTelemetry API calls to create trace spans. If you're using OpenTelemetry auto-instrumentation, ensure the instrumentation supports the libraries and frameworks you're using. Traces are the most mature telemetry type in OpenTelemetry, so you'll find the richest performance information from traces. Trace sampling Our trace UI will only reflect the data sent to us. This means that throughput and other counts may not reflect the unsampled data. You can configure sampling application-side using the OpenTelemetry SDKs. If you enable our Infinite Tracing feature, you have the option to send us all of your trace data and use New Relic sampling. Note that New Relic may rate-limit trace data. Metrics When viewing your OpenTelemetry data, on the left side of the UI you can select Metrics explorer to visualize metrics and create dashboards. If you don’t see metrics, you may need to add OpenTelemetry API calls to add the metrics that you care about. Caution The OpenTelemetry standard for metric naming is still being defined and to date most instrumentation includes trace spans but not metrics. We expect that more instrumentation will include metrics in the coming months. Errors When viewing your OpenTelemetry data, on the left side of the UI you can select Errors to understand what errors your service is generating. The count of errors is based on how many traces have a span with the attribute error.stack. If you don’t see the errors listed that you expect to see, check that your instrumentation sets the following three attributes: error.stack error.msg error.type In order for the error to be listed, a span must have an attribute that starts with error. Caution The OpenTelemetry standard for representing errors in traces and metrics is still being defined so these attribute names are subject to change. We expect the specification to be frozen very soon.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.28842,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Troubleshoot the <em>OpenTelemetry</em> exporter",
        "sections": "Troubleshoot the <em>OpenTelemetry</em> exporter",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is an emerging standard for service-level instrumentation. Because it has not yet reached 1.0 for any language, the <em>OpenTelemetry</em> SDK, APIs, and instrumentation are subject to change. If you&#x27;re using New Relic&#x27;s <em>OpenTelemetry</em> exporter, but you&#x27;re not seeing the data you expect"
      },
      "id": "603e897de7b9d2ca9b2a07d6"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/roku/roku-open-source-video-agent": [
    {
      "sections": [
        "Elixir open-source agent",
        "Tip",
        "Get started",
        "For more help"
      ],
      "title": "Elixir open-source agent",
      "type": "docs",
      "tags": [
        "Agents",
        "Open-source licensed agents",
        "Open-source licensed agents"
      ],
      "external_id": "aa03e1693b6ecdd06fa2940ddb99187247743772",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/elixir/elixir-open-source-agent/",
      "published_at": "2021-05-22T08:57:23Z",
      "updated_at": "2021-04-27T11:09:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Monitor Elixir behavior with New Relic using the Elixir open-source agent. The agent: Helps you track transactions, distributed traces, and other parts of your application’s behavior Provides an overview of underlying BEAM activity Tip This agent is released as open source on GitHub. A change log is also available there for the latest updates. Get started For requirements, installation, and configuration information, see the Open Source Elixir Agent README on GitHub. Visit New Relic’s Elixir repository on GitHub for questions about installation, usage, or other topics. Report issues or bugs as an issue in the GitHub repository. For more help Recommendations for learning more: Browse New Relic's Explorers Hub for community discussions about the open-source Elixir agent. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 423.54468,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elixir <em>open</em>-<em>source</em> <em>agent</em>",
        "sections": "Elixir <em>open</em>-<em>source</em> <em>agent</em>",
        "tags": "<em>Open</em>-<em>source</em> <em>licensed</em> <em>agents</em>",
        "body": "Monitor Elixir behavior with New Relic using the Elixir <em>open</em>-<em>source</em> <em>agent</em>. The <em>agent</em>: Helps you track transactions, distributed traces, and other parts of your application’s behavior Provides an overview of underlying BEAM activity Tip This <em>agent</em> is released as <em>open</em> <em>source</em> on GitHub. A change log"
      },
      "id": "6087f0ff28ccbceab351c13f"
    },
    {
      "image": "https://docs.newrelic.com/static/a0af3af7014ff88dc8c265d4d162da52/ae694/whats-new-rss.png",
      "url": "https://docs.newrelic.com/whats-new/2021/05/rss/",
      "sections": [
        "Keep track of What's New, security bulletins, and more with RSS!",
        "Get a link to the feeds"
      ],
      "published_at": "2021-05-26T09:45:59Z",
      "title": "Keep track of What's New, security bulletins, and more with RSS!",
      "updated_at": "2021-05-26T09:45:59Z",
      "type": "docs",
      "external_id": "6fd1f849e6612473675cfafb7f193fc6dd2a3281",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "Have you heard? RSS is back! And we're on board. We've long had release note feeds for agents and downloadable software from the release notes page. Lately, we've seen lots of requests to add RSS for What's New and our security bulletins, so we've done just that! Now you can subscribe to all the newness New Relic has to offer. You can plug those feeds into your feed reader, of course, but you can also automatically post to a channel with Slack's built in RSS app or Microsoft Team's RSS app. Or go crazy with IFTTT or Zapier for all manner of RSS-based automation. (Want to create a Zapier workflow that turns your smart lightbulbs green every time we release a new Node.js agent? Go for it!) We also release weekly Nerdlog episodes on YouTube (in case you miss our interactive livestream every Thursday at 12 p.m. PT on Twitch) to ensure you know about the latest products and features directly from the product managers and engineers at New Relic who built them, and Nerd Bytes episodes on YouTube for quick tips, tricks, and ideas about New Relic and observability. We've got RSS for those too! Get a link to the feeds What's New: https://docs.newrelic.com/whats-new/feed.xml Or find the feed from the RSS icon on What's New. Security bulletins: https://docs.newrelic.com/docs/security/new-relic-security/security-bulletins/feed.xml Or find the feed from the RSS link on the security bulletins page. Nerdlog: https://www.youtube.com/feeds/videos.xml?playlist_id=PLmhYj7Jl81JGOEHV7zUVfa_iGNyOfMGNh Nerd Bytes: https://www.youtube.com/feeds/videos.xml?playlist_id=PLmhYj7Jl81JEV-llIDkCVC05tD7fbOv_b Agents: We've got too many agents to list all the links! You can find a link by picking your agent from the release notes page and then clicking the RSS icon.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 32.84854,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": "=PLmhYj7Jl81JGOEHV7zUVfa_iGNyOfMGNh Nerd Bytes: https:&#x2F;&#x2F;www.youtube.com&#x2F;feeds&#x2F;videos.xml?playlist_id=PLmhYj7Jl81JEV-llIDkCVC05tD7fbOv_b <em>Agents</em>: We&#x27;ve got too many <em>agents</em> to list all the links! You can find a link by picking your <em>agent</em> from the release notes page and then clicking the RSS icon."
      },
      "id": "60ae18d7196a673bdee2f5f7"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/style-guide/processes-procedures/update-left-navigation-pane/",
      "sections": [
        "Update left-navigation pane",
        "The configuration file",
        "Tip",
        "Configuration file options",
        "Use nav YAML to generate auto index content",
        "Nav requirements for auto index pages",
        "Example"
      ],
      "published_at": "2021-05-21T16:38:01Z",
      "title": "Update left-navigation pane",
      "updated_at": "2021-05-21T16:38:01Z",
      "type": "docs",
      "external_id": "a05e38602a1eb152749923691d251ef2c99f05a9",
      "document_type": "page",
      "popularity": 1,
      "body": "Navigation for docs.newrelic.com is stored in YAML files located in the /src/nav/ directory. Each top-level navigation should have its own configuration file. You'll most likely edit a version of this file in your branch and merge it into the develop branch. The configuration file As an example, here's a snippet of the agents.yml navigation configuration. Note that the file has indentation that corresponds to the level of the navigation hierarchy. Be sure to imitate the same spacing when you make changes: title: Agents path: /docs/agents pages: - title: Manage APM agents path: /docs/agents/manage-apm-agents pages: - title: Agent data path: /docs/agents/manage-apm-agents/agent-data pages: - title: Real time streaming path: /docs/agents/manage-apm-agents/agent-data/real-time-streaming - title: Custom instrumentation path: /docs/agents/manage-apm-agents/agent-data/custom-instrumentation - title: Agent attributes path: /docs/agents/manage-apm-agents/agent-data/agent-attributes - title: Custom events path: /docs/agents/manage-apm-agents/agent-data/collect-custom-events - title: Custom metrics path: /docs/agents/manage-apm-agents/agent-data/collect-custom-metrics - title: Manage errors path: /docs/agents/manage-apm-agents/agent-data/manage-errors-apm-collect-ignore-or-mark-expected - title: Link Kubernetes path: /docs/agents/manage-apm-agents/agent-data/link-your-applications-kubernetes - title: App naming pages: - title: Name your application path: /docs/agents/manage-apm-agents/app-naming/name-your-application - title: Use multiple names for an app path: /docs/agents/manage-apm-agents/app-naming/use-multiple-names-app Copy Tip Navigation nesting is currently limited to a maximum of six levels deep. Please reach out to the engineering team if that is not enough. Configuration file options When you're changing the navigation, keep in mind that each node in the .yml configuration file can have the following properties: Key Required? Description title yes The text that is shown in the navigation. path * no The URL path to the page. Do not use trailing slashes. * Adding a path is required to generate auto index page content. See path requirements below. children no Any sub-navigation nodes. When the user goes to a page, we determine which section of the site they are on and load the appropriate .yml file to populate the sidebar navigation. The navigation for the homepage is an aggregate of all the top-level pages. Tip Each category has its own index.md page (list of pages for that category). When updating the navigation, you may also want to update these pages to reflect the new information architecture more correctly. Use nav YAML to generate auto index content On build we automatically generate an index page for each sub-directory in src/content that does not already have an index.mdx file and that has at least one .mdx or .md file in it. When creating the page we look through each nav YAML file for a reference to the path to know which nav data to use for the left nav and the auto index page content. If an index page path is in more than one nav YAML file, we use the nav that matches its top level path with the beginning of index page path. Nav requirements for auto index pages In order to populate the auto index page with nav YAML data, you must add the index page path somewhere in the most relevant nav YAML file. This helps the auto index page plugin determine which nav to attach to the index page, but also helps the <IndexContents /> component know which section to display on the index page itself. Each nav item must reference a directory path in src/content that contains at least one .md or .mdx file (otherwise the auto index plugin does not create a page for it). Example If this auto index page is used in full-stack-observability.yml and integrations.yml, we will attach the integrations.yml nav to it because: the top level path in the integrations.yml matches the beginning of the auto index page path: title: Integrations path: /docs/integrations Copy matches this auto index page path: /docs/integrations /amazon-integrations/aws-integrations-list Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 31.880127,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": "&#x2F;manage-apm-<em>agents</em> pages: - title: <em>Agent</em> data path: &#x2F;docs&#x2F;<em>agents</em>&#x2F;manage-apm-<em>agents</em>&#x2F;<em>agent</em>-data pages: - title: Real time streaming path: &#x2F;docs&#x2F;<em>agents</em>&#x2F;manage-apm-<em>agents</em>&#x2F;<em>agent</em>-data&#x2F;real-time-streaming - title: Custom instrumentation path: &#x2F;docs&#x2F;<em>agents</em>&#x2F;manage-apm-<em>agents</em>&#x2F;<em>agent</em>-data&#x2F;custom-instrumentation"
      },
      "id": "60422077196a67ad56a83dc8"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/troubleshooting/troubleshoot-opentelemetry-exporter": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed Tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Metrics Explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/623057d780c245a4a67ec46372991610/5f4af/explorer_filter_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-16T00:19:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Open one.newrelic.com, and go to explorer. Using the filter bar at the top, narrow your searches: Limit results to OpenTelemetry entities with the following: After the OpenTelemetry filter, you can add another filter for your service based on the service.name attributes you've used. For example, to find CartService: Click the service you want to know more about, and if you need help understanding the data, see the explanations below. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed Tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once you’re in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in the response time of your application. From the transaction summary page, you can get a list of transactions by selecting See transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals captures calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On this page, you can see total errors, as well as charts showing error count and error rate. Metrics Explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.99838,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-05-22T09:02:26Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 258.46005,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Tip",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Collector (recommended)",
        "OTLP (pre-release)",
        "What's next?"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-05-22T09:01:08Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can send OpenTelemetry data to New Relic and analyze the results in our observability platform. This introduction can help you decide if you want to get started with OpenTelemetry. If you're already familiar with OpenTelemetry and want to jump into the setup, go to our OpenTelemetry quick start. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can send data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Tip For more, see our blog posts on OpenTelemetry. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start sending data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic APM agents In general, New Relic APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic Here are two basic ways you could set up OpenTelemetry with New Relic: use the OpenTelemetry collector (recommended), or use the native OTLP endpoint (pre-release). Each approach assumes you set up instrumentation in your code, but they differ in how the data is gathered and sent to New Relic. If you are interested in tracing, we recommend that you instrument as many services as you can so that you get the most benefit from distributed tracing. There are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesn’t analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Tip We discuss some variations on these basic approaches in our OpenTelemetry architecture recipes. For a detailed discussion about the architecture of OpenTelemetry itself, see our blog. Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. Collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. In this workflow, note the alternate path if you set up New Relic Infinite Tracing: Here are some advantages of this approach: The OpenTelemetry collector can perform processing operations on telemetry data before it is sent to the backend, including filtering attributes, excluding traces, and sampling. The OpenTelemetry collector can receive telemetry data in many formats and forward it on to New Relic’s backend, including Prometheus, Zipkin, Jaeger, AWS X-Ray, OpenCensus, and much more. The OpenTelemetry collector can collect system-level metrics from the host where it is running. OTLP (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector, or send us data directly from your service. If you are interested, let us know by completing this form. Here are some advantages to operating without an OpenTelemetry collector: There is no need to deploy a collector alongside your service. Telemetry data is sent directly from the instrumented service to New Relic. If something goes wrong, and telemetry data does not arrive, there is one less component to debug. What's next? If you want to set up OpenTelemetry with New Relic, the next step is to check out our OpenTelemetry quick start. If you prefer to use a New Relic APM agent, go to Introduction to APM.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.51138,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": ", and as more vendors move to support this <em>open</em> standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end"
      },
      "id": "603e81ba196a67304da83dab"
    }
  ],
  "/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic": [
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-05-22T12:29:27Z",
      "updated_at": "2021-05-22T12:29:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.19 Kubernetes cluster GKE Currently tested with versions 1.10 and 1.17 Kubernetes cluster EKS (EC2 nodes) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.5 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 109.1573,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes <em>integration</em>: compatibility and requirements",
        "sections": "Kubernetes <em>integration</em>: compatibility and requirements",
        "tags": "<em>Get</em> <em>started</em>"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Introduction to New Relic NerdGraph, our GraphQL API",
        "Tip",
        "What is NerdGraph?",
        "Important",
        "Use the GraphiQL explorer",
        "Requirements and endpoints",
        "What can you do with NerdGraph?",
        "NerdGraph terminology",
        "Tips on using the GraphiQL explorer",
        "Query accounts a New Relic user can access",
        "Query user, account, and NRQL in one request"
      ],
      "title": "Introduction to New Relic NerdGraph, our GraphQL API",
      "type": "docs",
      "tags": [
        "APIs",
        "NerdGraph",
        "Get started"
      ],
      "external_id": "e8e96c16cd75f494ebfacb3bc53b4ee9ccf1c727",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/nerdgraph/get-started/introduction-new-relic-nerdgraph/",
      "published_at": "2021-05-21T14:11:07Z",
      "updated_at": "2021-05-21T14:11:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NerdGraph is our GraphQL-format API that lets you query New Relic data and configure some New Relic features. Tip To use our APIs, or the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is NerdGraph? New Relic has several APIs. NerdGraph is our preferred API for querying New Relic data, and for performing some specific configurations (learn more about features). NerdGraph provides a single API interface for returning data from New Relic’s various APIs and microservices. Over time, other configuration capabilities will be added to NerdGraph. Important NerdGraph isn’t used for data ingest. For that, you'd use our data ingest APIs. NerdGraph is built using GraphQL, which is an open source API format that allows you to request exactly the data needed, with no over-fetching or under-fetching. For a lesson in how to use NerdGraph, watch this 7-minute video: Want to watch more video tutorials? Go to the New Relic University’s Intro to NerdGraph. Or see the online course on New Relic APIs. Use the GraphiQL explorer To get started using GraphQL, we recommend playing around with our GraphiQL explorer (GraphiQL is an open source graphical interface for using GraphQL). You can use it to explore our data schema, to read built-in object definitions, and to build and execute queries. To use GraphQL, you’ll need a user-specific New Relic API key called a user key. You can generate one or find an existing one from the GraphiQL explorer’s API key dropdown. To find the GraphiQL explorer: If your New Relic account uses an EU data center, go to api.eu.newrelic.com/graphiql. Otherwise use api.newrelic.com/graphiql. For tips on how to build queries, see Build queries. Requirements and endpoints To use NerdGraph, you need a New Relic user key, which can be generated and accessed from the GraphiQL explorer. The endpoints are: Main endpoint: https://api.newrelic.com/graphql Endpoint for accounts using EU data center: https://api.eu.newrelic.com/graphql To access the endpoint, use the following cURL command: curl -X POST https://api.newrelic.com/graphql \\ -H 'Content-Type: application/json' \\ -H 'API-Key: YOUR_NEW_RELIC_USER_KEY' \\ -d '{ \"query\": \"{ requestContext { userId apiKey } }\" } ' Copy What can you do with NerdGraph? NerdGraph functionality can be broken down into two main categories: Querying New Relic data. You can fetch data for a variety of purposes, including using it in a programmatic workflow, or building a New Relic One app for custom data visualizations. Configuring New Relic features. There are a variety of configurations available and more will be added over time. You can do things like add tags, configure workloads, or customize \"golden metrics.\" You can use NerdGraph to return a wide range of New Relic data but we’ve created some tutorials for common use cases: Topic Tutorials Your monitored entities Get data about entities Understand entity relationships and dependencies (used to build service maps) Query and configure \"golden metrics\" (important entity metrics) Querying data Query using NRQL (our query language) Tags Add and manage tags Dashboards Create dashboards Export dashboards to other accounts Export dashboards as files Migrate from Insights Dashboard API to NerdGraph Alerts See all alert-related tutorials Applied Intelligence View and configure topology Workloads View and configure workloads Manage keys Create and manage keys (license keys used for data ingest, and user keys) Manage data Convert event data to metric data Drop data Distributed tracing Query distributed tracing data Configure Infinite Tracing New Relic One apps Build a New Relic One app Cloud integrations (AWS, Azure, GCP) Configure cloud integrations Partners and resellers Manage subscriptions (only for partners using original pricing plan) NerdGraph terminology The following are terms that originate with GraphQL (the API format NerdGraph uses). Term Definition Queries and mutations There are two classes of GraphQL operations: Queries are basic requests used only to fetch data. These queries are not static, meaning that you can ask for more data or less data, depending on your needs. For each query, you can specify exactly what data you want to retrieve, as long as it is supported by the schema. Mutations are requests that perform an action, such as creating a resource or changing configuration. Mutations require the keyword mutation, as well as the name of the mutation. Type Data in GraphQL is organized into types. Types can be scalars (like strings, numbers, or booleans) or object types. An object type is a custom type made up of a collection of fields. For example, an object type called User may represent a user in a system. Field A field represents a piece of information on an object type that can be queried. Fields can be scalars, lists, or objects. For example, a User object type could have a string field called name. Interface An interface is an abstract type that represents a collection of common fields that other object types can implement. Tips on using the GraphiQL explorer You can make queries with the NerdGraph GraphiQL explorer. The explorer provides built-in schema definitions and features, including auto-complete and query validation. Query accounts a New Relic user can access You can query for the name of an account that an actor (a New Relic authorized user) has access to: query { actor { account(id: YOUR_ACCOUNT_ID) { name } } } Copy The response will mirror the query structure you defined in the request, making it easy to ask for the specific data that you want. { \"data\": { \"actor\": { \"account\": { \"name\": \"Data Nerd\" } } } } Copy Query user, account, and NRQL in one request The graph structure shows its capabilities when queries become more complex. For example, you can query for user information, account information, and make a NRQL query with one request. With REST API, this would take three different requests to three different endpoints. query { actor { account(id: YOUR_ACCOUNT_ID) { name nrql(query: \"SELECT * FROM Transaction\") { results } } user { name id } } } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 103.7329,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Get</em> <em>started</em>",
        "body": ", with no over-fetching or under-fetching. For a lesson in how to use NerdGraph, watch this 7-minute video: Want to watch more video tutorials? Go to the New Relic University’s Intro to NerdGraph. Or see the online course on New Relic APIs. Use the GraphiQL explorer To <em>get</em> <em>started</em> using GraphQL, we"
      },
      "id": "6043ff97196a67d0a0960f55"
    },
    {
      "sections": [
        "Introduction to New Relic integrations",
        "Tip",
        "Choose what's right for you",
        "Create your own solutions"
      ],
      "title": "Introduction to New Relic integrations",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Instrument everything",
        "Get started"
      ],
      "external_id": "03217983a29af22737c1163da9ef0811b29c2bcd",
      "image": "",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/instrument-everything/get-started-new-relic-instrumentation/introduction-new-relic-integrations/",
      "published_at": "2021-05-21T16:02:11Z",
      "updated_at": "2021-03-16T07:30:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We provide hundreds of solutions to get your data into New Relic so you can analyze the data in one place. They give you a steady flow of useful data to fix problems quickly, maintain complex systems, improve your code, and accelerate your digital transformation. You can bring in data from hundreds of applications, frameworks, services, operating systems, and other technologies. Our integrations gather the data, and the agents send it to New Relic. The solution you need may require you to install both an integration and an agent. In some cases, you can just install our agents that contain integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to get you started. Tip To use integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Choose what's right for you We offer a wide range of solutions so you can easily collect data across your environment. You may only need one of our solutions to get the data you need, or you can choose a variety of options to capture a broader range of data types. Go to New Relic Integrations to find solutions that fit your environment. Here is a sample of what you’ll find there: Application performance monitoring (APM): C, Go, Java, Node, .NET, PHP, Python, and Ruby Mobile apps: Android and iOS Browser monitoring: Google Chrome, Mozilla Firefox, Microsoft Internet Explorer, and Apple Safari Host monitoring: Linux and Microsoft Windows Cloud platform monitoring: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry integrations: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you create your own: Use New Relic Flex to create lightweight monitoring solutions using infrastructure monitoring. Use New Relic Telemetry SDKs to build custom solutions for sending metrics, traces, and more. Build your own New Relic One applications that you can share with your colleagues, or edit open source applications in our catalog.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 93.2557,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic <em>integrations</em>",
        "sections": "Introduction to New Relic <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " <em>integrations</em>, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to <em>get</em> you <em>started</em>. Tip To use <em>integrations</em> and infrastructure monitoring, as well as the rest of our"
      },
      "id": "603e817f28ccbc4857eba798"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Tip",
        "Set up the integration",
        "Mapping of Prometheus metric types",
        "Override metric type mappings",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-05-22T16:24:18Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Tip To use Prometheus integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Mapping of Prometheus metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts or sub-accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.73138,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add the <em>remote</em> <em>write</em> URL to your Helm values.yaml file. Replace <em>remoteWrite</em>: [] with two lines similar to the following"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "0d190be5dc4fd91ce6bbcef7343d01f75670ca51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2021-05-22T16:11:56Z",
      "updated_at": "2021-03-13T03:46:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query for NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NrIntegrationError query on the error message using NRQL, then look at the Message field in the UI to see a description of what went wrong. For example: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' Copy Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If you’ve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.81514,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em>"
      },
      "id": "6044e65d196a67914a960f6b"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-05-22T16:08:58Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 83.34212,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-05-22T16:08:58Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.34032,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-05-22T16:10:49Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.08224,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-05-22T16:06:41Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.06064,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-rename-or-copy-prometheus-attributes": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-05-22T16:08:58Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.34032,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-05-22T16:10:49Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.08224,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-05-22T16:06:41Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.06064,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-05-22T16:08:58Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.34027,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-05-22T16:10:49Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.08224,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-05-22T16:02:36Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.28242,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations": [
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-05-22T16:10:49Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.08224,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-05-22T16:06:41Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.06062,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-05-22T16:02:36Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.28242,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/ignore-or-include-prometheus-metrics": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-05-22T16:08:58Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.34027,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-05-22T16:10:49Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.08224,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-05-22T16:06:41Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.06062,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-05-22T16:08:58Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.34024,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-05-22T16:06:41Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.06062,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-05-22T16:02:36Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.28242,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Tip",
        "Set up the integration",
        "Mapping of Prometheus metric types",
        "Override metric type mappings",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-05-22T16:24:18Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Tip To use Prometheus integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Mapping of Prometheus metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts or sub-accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.73137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add the <em>remote</em> <em>write</em> URL to your Helm values.yaml file. Replace <em>remoteWrite</em>: [] with two lines similar to the following"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-05-22T16:03:31Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.8024,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "6044e621196a67b846960f6b"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-05-22T16:08:58Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 83.34207,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration": [
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "0d190be5dc4fd91ce6bbcef7343d01f75670ca51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2021-05-22T16:11:56Z",
      "updated_at": "2021-03-13T03:46:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query for NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NrIntegrationError query on the error message using NRQL, then look at the Message field in the UI to see a description of what went wrong. For example: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' Copy Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If you’ve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.81512,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em>"
      },
      "id": "6044e65d196a67914a960f6b"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-05-22T16:03:31Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.80238,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "6044e621196a67b846960f6b"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-05-22T16:08:58Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 83.342064,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/debug-issues-data-sent-metric-api-prometheus-integration": [
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-05-22T08:42:17Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 108.64636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-05-22T08:41:23Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.608215,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    },
    {
      "sections": [
        "Sparse data, missing metrics, and data gaps",
        "Problem",
        "Solution"
      ],
      "title": "Sparse data, missing metrics, and data gaps",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "0389b736613331d774daf3c3643ca8362fd4babb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps/",
      "published_at": "2021-05-22T08:43:25Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic's UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled, and if the memory allocated to the container is enough. In case the container doesn't have enough resources available, it may send data with a longer interval between samples. Low memory limit may cause the integration to be killed and restarted. The correct limits and requests of resources can vary according to the number of targets monitored, and the number of metrics exposed by each target.For more information, see the guidelines in the large environment configuration documentation. Check for the following error message in the logs of the integration: {\"err\":\"unexpected post response code: 413: Request Entity Too Large\"} Copy This issue causes some payloads to be dropped, and it is currently fixed in new releases. If present, update the image to the most recent version. If some /metrics endpoints that are monitored by the integration time out or take several seconds to answer, they may slow down the scraping of data. The integration's performance can degrade if multiple endpoints take a huge amount of time to answer. This leads to intermittent and missing data. To detect those endpoints, run: SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy This query retrieves the data exposed by the Prometheus OpenMetrics integration, and it shows the time required to fetch each endpoint. Fix the endpoints with a latency above 1s, or exclude them from the monitoring removing the Prometheus scraping label. If it's not feasible to remove these endpoints, and the latency in the answer can't be avoided, configure the integration to run more workers in parallel. This allows the integration to fetch more endpoints at the same time. To do so, update the integration to the most recent version, and apply the new configuration option worker_threads. We advise to do it gradually, from 4, 6, 8 and up to 16. This workaround only minimizes the issue, and if multiple endpoints are misbehaving, performances will still be degraded. Moreover, memory and CPU consumption will increase with the number of workers, so memory and CPU must be increased accordingly. In case all the endpoints monitored have a low latency, and the container is not being throttled, run the following query. This helps you verify how much time the integration is taking to scrape all the targets, and to send the data if it's exceeding the configured scrape_duration. SELECT latest(nr_stats_integration_process_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO TIMESERIES Copy First, update the integration to the latest image published. Then, to reduce the time needed to scrape all the targets, increase the number of workers with the configuration option worker_threads. We advise is to do it gradually, from 4 to 6, 8, and up to 16 until r_stats_integration_process_duration_seconds approaches the defined scrape_duration. Note that memory consumption and CPU usage will increase.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.597725,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic&#x27;s UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled"
      },
      "id": "603e9b8228ccbc95f4eba75b"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption": [
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-05-22T08:42:17Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 108.64636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Sparse data, missing metrics, and data gaps",
        "Problem",
        "Solution"
      ],
      "title": "Sparse data, missing metrics, and data gaps",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "0389b736613331d774daf3c3643ca8362fd4babb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps/",
      "published_at": "2021-05-22T08:43:25Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic's UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled, and if the memory allocated to the container is enough. In case the container doesn't have enough resources available, it may send data with a longer interval between samples. Low memory limit may cause the integration to be killed and restarted. The correct limits and requests of resources can vary according to the number of targets monitored, and the number of metrics exposed by each target.For more information, see the guidelines in the large environment configuration documentation. Check for the following error message in the logs of the integration: {\"err\":\"unexpected post response code: 413: Request Entity Too Large\"} Copy This issue causes some payloads to be dropped, and it is currently fixed in new releases. If present, update the image to the most recent version. If some /metrics endpoints that are monitored by the integration time out or take several seconds to answer, they may slow down the scraping of data. The integration's performance can degrade if multiple endpoints take a huge amount of time to answer. This leads to intermittent and missing data. To detect those endpoints, run: SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy This query retrieves the data exposed by the Prometheus OpenMetrics integration, and it shows the time required to fetch each endpoint. Fix the endpoints with a latency above 1s, or exclude them from the monitoring removing the Prometheus scraping label. If it's not feasible to remove these endpoints, and the latency in the answer can't be avoided, configure the integration to run more workers in parallel. This allows the integration to fetch more endpoints at the same time. To do so, update the integration to the most recent version, and apply the new configuration option worker_threads. We advise to do it gradually, from 4, 6, 8 and up to 16. This workaround only minimizes the issue, and if multiple endpoints are misbehaving, performances will still be degraded. Moreover, memory and CPU consumption will increase with the number of workers, so memory and CPU must be increased accordingly. In case all the endpoints monitored have a low latency, and the container is not being throttled, run the following query. This helps you verify how much time the integration is taking to scrape all the targets, and to send the data if it's exceeding the configured scrape_duration. SELECT latest(nr_stats_integration_process_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO TIMESERIES Copy First, update the integration to the latest image published. Then, to reduce the time needed to scrape all the targets, increase the number of workers with the configuration option worker_threads. We advise is to do it gradually, from 4 to 6, 8, and up to 16 until r_stats_integration_process_duration_seconds approaches the defined scrape_duration. Note that memory consumption and CPU usage will increase.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.597725,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic&#x27;s UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled"
      },
      "id": "603e9b8228ccbc95f4eba75b"
    },
    {
      "sections": [
        "Restarts and gaps in data (Kubernetes)",
        "Problem",
        "Solution"
      ],
      "title": "Restarts and gaps in data (Kubernetes)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "215253af52dea03f816c3864e0f10911ae960897",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/restarts-gaps-data-kubernetes/",
      "published_at": "2021-05-22T08:43:25Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When running the Prometheus OpenMetrics integration for Kubernetes, you notice restarts and gaps in data sent to New Relic. Solution When running the Prometheus OpenMetrics integration for Kubernetes with 500K data points per minute, be sure to set these limits: CPU limit: 1 core Memory limit: 1Gb Recommendation: Always run the Kubernetes scraper with one replica. Adding more replicas will result in duplicated data. If the CPU and memory limits are not sufficient, this can result in restarts and gaps in the data. To check the status and restart events for the scraper: kubectl describe pod -l \"app=nri-prometheus\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.597725,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem When running the <em>Prometheus</em> OpenMetrics integration for Kubernetes, you notice restarts and gaps in data sent to New Relic. Solution When running the <em>Prometheus</em> OpenMetrics integration for Kubernetes with 500K data points per minute, be sure to set these limits: CPU limit: 1 core Memory"
      },
      "id": "603ea39964441f1da84e884f"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/get-logs-prometheus-integration": [
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-05-22T08:42:17Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 108.64636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-05-22T08:41:23Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.60821,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    },
    {
      "sections": [
        "Sparse data, missing metrics, and data gaps",
        "Problem",
        "Solution"
      ],
      "title": "Sparse data, missing metrics, and data gaps",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "0389b736613331d774daf3c3643ca8362fd4babb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps/",
      "published_at": "2021-05-22T08:43:25Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic's UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled, and if the memory allocated to the container is enough. In case the container doesn't have enough resources available, it may send data with a longer interval between samples. Low memory limit may cause the integration to be killed and restarted. The correct limits and requests of resources can vary according to the number of targets monitored, and the number of metrics exposed by each target.For more information, see the guidelines in the large environment configuration documentation. Check for the following error message in the logs of the integration: {\"err\":\"unexpected post response code: 413: Request Entity Too Large\"} Copy This issue causes some payloads to be dropped, and it is currently fixed in new releases. If present, update the image to the most recent version. If some /metrics endpoints that are monitored by the integration time out or take several seconds to answer, they may slow down the scraping of data. The integration's performance can degrade if multiple endpoints take a huge amount of time to answer. This leads to intermittent and missing data. To detect those endpoints, run: SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy This query retrieves the data exposed by the Prometheus OpenMetrics integration, and it shows the time required to fetch each endpoint. Fix the endpoints with a latency above 1s, or exclude them from the monitoring removing the Prometheus scraping label. If it's not feasible to remove these endpoints, and the latency in the answer can't be avoided, configure the integration to run more workers in parallel. This allows the integration to fetch more endpoints at the same time. To do so, update the integration to the most recent version, and apply the new configuration option worker_threads. We advise to do it gradually, from 4, 6, 8 and up to 16. This workaround only minimizes the issue, and if multiple endpoints are misbehaving, performances will still be degraded. Moreover, memory and CPU consumption will increase with the number of workers, so memory and CPU must be increased accordingly. In case all the endpoints monitored have a low latency, and the container is not being throttled, run the following query. This helps you verify how much time the integration is taking to scrape all the targets, and to send the data if it's exceeding the configured scrape_duration. SELECT latest(nr_stats_integration_process_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO TIMESERIES Copy First, update the integration to the latest image published. Then, to reduce the time needed to scrape all the targets, increase the number of workers with the configuration option worker_threads. We advise is to do it gradually, from 4 to 6, 8, and up to 16 until r_stats_integration_process_duration_seconds approaches the defined scrape_duration. Note that memory consumption and CPU usage will increase.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.597725,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic&#x27;s UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled"
      },
      "id": "603e9b8228ccbc95f4eba75b"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/get-scraper-metrics-prometheus-integration": [
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-05-22T08:42:17Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 108.64636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-05-22T08:41:23Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.60821,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    },
    {
      "sections": [
        "Sparse data, missing metrics, and data gaps",
        "Problem",
        "Solution"
      ],
      "title": "Sparse data, missing metrics, and data gaps",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "0389b736613331d774daf3c3643ca8362fd4babb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps/",
      "published_at": "2021-05-22T08:43:25Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic's UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled, and if the memory allocated to the container is enough. In case the container doesn't have enough resources available, it may send data with a longer interval between samples. Low memory limit may cause the integration to be killed and restarted. The correct limits and requests of resources can vary according to the number of targets monitored, and the number of metrics exposed by each target.For more information, see the guidelines in the large environment configuration documentation. Check for the following error message in the logs of the integration: {\"err\":\"unexpected post response code: 413: Request Entity Too Large\"} Copy This issue causes some payloads to be dropped, and it is currently fixed in new releases. If present, update the image to the most recent version. If some /metrics endpoints that are monitored by the integration time out or take several seconds to answer, they may slow down the scraping of data. The integration's performance can degrade if multiple endpoints take a huge amount of time to answer. This leads to intermittent and missing data. To detect those endpoints, run: SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy This query retrieves the data exposed by the Prometheus OpenMetrics integration, and it shows the time required to fetch each endpoint. Fix the endpoints with a latency above 1s, or exclude them from the monitoring removing the Prometheus scraping label. If it's not feasible to remove these endpoints, and the latency in the answer can't be avoided, configure the integration to run more workers in parallel. This allows the integration to fetch more endpoints at the same time. To do so, update the integration to the most recent version, and apply the new configuration option worker_threads. We advise to do it gradually, from 4, 6, 8 and up to 16. This workaround only minimizes the issue, and if multiple endpoints are misbehaving, performances will still be degraded. Moreover, memory and CPU consumption will increase with the number of workers, so memory and CPU must be increased accordingly. In case all the endpoints monitored have a low latency, and the container is not being throttled, run the following query. This helps you verify how much time the integration is taking to scrape all the targets, and to send the data if it's exceeding the configured scrape_duration. SELECT latest(nr_stats_integration_process_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO TIMESERIES Copy First, update the integration to the latest image published. Then, to reduce the time needed to scrape all the targets, increase the number of workers with the configuration option worker_threads. We advise is to do it gradually, from 4 to 6, 8, and up to 16 until r_stats_integration_process_duration_seconds approaches the defined scrape_duration. Note that memory consumption and CPU usage will increase.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.59772,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic&#x27;s UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled"
      },
      "id": "603e9b8228ccbc95f4eba75b"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration": [
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-05-22T08:41:23Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.60821,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    },
    {
      "sections": [
        "Sparse data, missing metrics, and data gaps",
        "Problem",
        "Solution"
      ],
      "title": "Sparse data, missing metrics, and data gaps",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "0389b736613331d774daf3c3643ca8362fd4babb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps/",
      "published_at": "2021-05-22T08:43:25Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic's UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled, and if the memory allocated to the container is enough. In case the container doesn't have enough resources available, it may send data with a longer interval between samples. Low memory limit may cause the integration to be killed and restarted. The correct limits and requests of resources can vary according to the number of targets monitored, and the number of metrics exposed by each target.For more information, see the guidelines in the large environment configuration documentation. Check for the following error message in the logs of the integration: {\"err\":\"unexpected post response code: 413: Request Entity Too Large\"} Copy This issue causes some payloads to be dropped, and it is currently fixed in new releases. If present, update the image to the most recent version. If some /metrics endpoints that are monitored by the integration time out or take several seconds to answer, they may slow down the scraping of data. The integration's performance can degrade if multiple endpoints take a huge amount of time to answer. This leads to intermittent and missing data. To detect those endpoints, run: SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy This query retrieves the data exposed by the Prometheus OpenMetrics integration, and it shows the time required to fetch each endpoint. Fix the endpoints with a latency above 1s, or exclude them from the monitoring removing the Prometheus scraping label. If it's not feasible to remove these endpoints, and the latency in the answer can't be avoided, configure the integration to run more workers in parallel. This allows the integration to fetch more endpoints at the same time. To do so, update the integration to the most recent version, and apply the new configuration option worker_threads. We advise to do it gradually, from 4, 6, 8 and up to 16. This workaround only minimizes the issue, and if multiple endpoints are misbehaving, performances will still be degraded. Moreover, memory and CPU consumption will increase with the number of workers, so memory and CPU must be increased accordingly. In case all the endpoints monitored have a low latency, and the container is not being throttled, run the following query. This helps you verify how much time the integration is taking to scrape all the targets, and to send the data if it's exceeding the configured scrape_duration. SELECT latest(nr_stats_integration_process_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO TIMESERIES Copy First, update the integration to the latest image published. Then, to reduce the time needed to scrape all the targets, increase the number of workers with the configuration option worker_threads. We advise is to do it gradually, from 4 to 6, 8, and up to 16 until r_stats_integration_process_duration_seconds approaches the defined scrape_duration. Note that memory consumption and CPU usage will increase.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.59772,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic&#x27;s UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled"
      },
      "id": "603e9b8228ccbc95f4eba75b"
    },
    {
      "sections": [
        "Restarts and gaps in data (Kubernetes)",
        "Problem",
        "Solution"
      ],
      "title": "Restarts and gaps in data (Kubernetes)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "215253af52dea03f816c3864e0f10911ae960897",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/restarts-gaps-data-kubernetes/",
      "published_at": "2021-05-22T08:43:25Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When running the Prometheus OpenMetrics integration for Kubernetes, you notice restarts and gaps in data sent to New Relic. Solution When running the Prometheus OpenMetrics integration for Kubernetes with 500K data points per minute, be sure to set these limits: CPU limit: 1 core Memory limit: 1Gb Recommendation: Always run the Kubernetes scraper with one replica. Adding more replicas will result in duplicated data. If the CPU and memory limits are not sufficient, this can result in restarts and gaps in the data. To check the status and restart events for the scraper: kubectl describe pod -l \"app=nri-prometheus\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.59772,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem When running the <em>Prometheus</em> OpenMetrics integration for Kubernetes, you notice restarts and gaps in data sent to New Relic. Solution When running the <em>Prometheus</em> OpenMetrics integration for Kubernetes with 500K data points per minute, be sure to set these limits: CPU limit: 1 core Memory"
      },
      "id": "603ea39964441f1da84e884f"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration": [
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-05-22T08:42:17Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 108.64636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-05-22T08:41:23Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.60821,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    },
    {
      "sections": [
        "Sparse data, missing metrics, and data gaps",
        "Problem",
        "Solution"
      ],
      "title": "Sparse data, missing metrics, and data gaps",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "0389b736613331d774daf3c3643ca8362fd4babb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps/",
      "published_at": "2021-05-22T08:43:25Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic's UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled, and if the memory allocated to the container is enough. In case the container doesn't have enough resources available, it may send data with a longer interval between samples. Low memory limit may cause the integration to be killed and restarted. The correct limits and requests of resources can vary according to the number of targets monitored, and the number of metrics exposed by each target.For more information, see the guidelines in the large environment configuration documentation. Check for the following error message in the logs of the integration: {\"err\":\"unexpected post response code: 413: Request Entity Too Large\"} Copy This issue causes some payloads to be dropped, and it is currently fixed in new releases. If present, update the image to the most recent version. If some /metrics endpoints that are monitored by the integration time out or take several seconds to answer, they may slow down the scraping of data. The integration's performance can degrade if multiple endpoints take a huge amount of time to answer. This leads to intermittent and missing data. To detect those endpoints, run: SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy This query retrieves the data exposed by the Prometheus OpenMetrics integration, and it shows the time required to fetch each endpoint. Fix the endpoints with a latency above 1s, or exclude them from the monitoring removing the Prometheus scraping label. If it's not feasible to remove these endpoints, and the latency in the answer can't be avoided, configure the integration to run more workers in parallel. This allows the integration to fetch more endpoints at the same time. To do so, update the integration to the most recent version, and apply the new configuration option worker_threads. We advise to do it gradually, from 4, 6, 8 and up to 16. This workaround only minimizes the issue, and if multiple endpoints are misbehaving, performances will still be degraded. Moreover, memory and CPU consumption will increase with the number of workers, so memory and CPU must be increased accordingly. In case all the endpoints monitored have a low latency, and the container is not being throttled, run the following query. This helps you verify how much time the integration is taking to scrape all the targets, and to send the data if it's exceeding the configured scrape_duration. SELECT latest(nr_stats_integration_process_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO TIMESERIES Copy First, update the integration to the latest image published. Then, to reduce the time needed to scrape all the targets, increase the number of workers with the configuration option worker_threads. We advise is to do it gradually, from 4 to 6, 8, and up to 16 until r_stats_integration_process_duration_seconds approaches the defined scrape_duration. Note that memory consumption and CPU usage will increase.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.59772,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic&#x27;s UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled"
      },
      "id": "603e9b8228ccbc95f4eba75b"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/restarts-gaps-data-kubernetes": [
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-05-22T08:42:17Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 108.64636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-05-22T08:41:23Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.60821,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    },
    {
      "sections": [
        "Sparse data, missing metrics, and data gaps",
        "Problem",
        "Solution"
      ],
      "title": "Sparse data, missing metrics, and data gaps",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "0389b736613331d774daf3c3643ca8362fd4babb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps/",
      "published_at": "2021-05-22T08:43:25Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic's UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled, and if the memory allocated to the container is enough. In case the container doesn't have enough resources available, it may send data with a longer interval between samples. Low memory limit may cause the integration to be killed and restarted. The correct limits and requests of resources can vary according to the number of targets monitored, and the number of metrics exposed by each target.For more information, see the guidelines in the large environment configuration documentation. Check for the following error message in the logs of the integration: {\"err\":\"unexpected post response code: 413: Request Entity Too Large\"} Copy This issue causes some payloads to be dropped, and it is currently fixed in new releases. If present, update the image to the most recent version. If some /metrics endpoints that are monitored by the integration time out or take several seconds to answer, they may slow down the scraping of data. The integration's performance can degrade if multiple endpoints take a huge amount of time to answer. This leads to intermittent and missing data. To detect those endpoints, run: SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy This query retrieves the data exposed by the Prometheus OpenMetrics integration, and it shows the time required to fetch each endpoint. Fix the endpoints with a latency above 1s, or exclude them from the monitoring removing the Prometheus scraping label. If it's not feasible to remove these endpoints, and the latency in the answer can't be avoided, configure the integration to run more workers in parallel. This allows the integration to fetch more endpoints at the same time. To do so, update the integration to the most recent version, and apply the new configuration option worker_threads. We advise to do it gradually, from 4, 6, 8 and up to 16. This workaround only minimizes the issue, and if multiple endpoints are misbehaving, performances will still be degraded. Moreover, memory and CPU consumption will increase with the number of workers, so memory and CPU must be increased accordingly. In case all the endpoints monitored have a low latency, and the container is not being throttled, run the following query. This helps you verify how much time the integration is taking to scrape all the targets, and to send the data if it's exceeding the configured scrape_duration. SELECT latest(nr_stats_integration_process_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO TIMESERIES Copy First, update the integration to the latest image published. Then, to reduce the time needed to scrape all the targets, increase the number of workers with the configuration option worker_threads. We advise is to do it gradually, from 4 to 6, 8, and up to 16 until r_stats_integration_process_duration_seconds approaches the defined scrape_duration. Note that memory consumption and CPU usage will increase.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.59772,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic&#x27;s UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled"
      },
      "id": "603e9b8228ccbc95f4eba75b"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps": [
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-05-22T08:42:17Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 108.64636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-05-22T08:41:23Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.60821,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    },
    {
      "sections": [
        "Restarts and gaps in data (Kubernetes)",
        "Problem",
        "Solution"
      ],
      "title": "Restarts and gaps in data (Kubernetes)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "215253af52dea03f816c3864e0f10911ae960897",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/restarts-gaps-data-kubernetes/",
      "published_at": "2021-05-22T08:43:25Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When running the Prometheus OpenMetrics integration for Kubernetes, you notice restarts and gaps in data sent to New Relic. Solution When running the Prometheus OpenMetrics integration for Kubernetes with 500K data points per minute, be sure to set these limits: CPU limit: 1 core Memory limit: 1Gb Recommendation: Always run the Kubernetes scraper with one replica. Adding more replicas will result in duplicated data. If the CPU and memory limits are not sufficient, this can result in restarts and gaps in the data. To check the status and restart events for the scraper: kubectl describe pod -l \"app=nri-prometheus\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.59772,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem When running the <em>Prometheus</em> OpenMetrics integration for Kubernetes, you notice restarts and gaps in data sent to New Relic. Solution When running the <em>Prometheus</em> OpenMetrics integration for Kubernetes with 500K data points per minute, be sure to set these limits: CPU limit: 1 core Memory"
      },
      "id": "603ea39964441f1da84e884f"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features": [
    {
      "sections": [
        "View and query your Prometheus data",
        "Default attributes for the OpenMetrics integration",
        "Default attributes for the remote write integration",
        "NRQL query examples",
        "Get metric names",
        "Get the attributes for a metric",
        "Get the values for an attribute in OpenMetrics",
        "Build the query",
        "Get metric values",
        "Get a chart of the metric",
        "Query counter metrics (deltas)",
        "View connected Redis clients per pod with OpenMetrics",
        "Docker: View average memory free for scraped endpoints",
        "Kubernetes: View average memory usage for pods in a deployment",
        "View data in New Relic",
        "Create histograms and summaries"
      ],
      "title": "View and query your Prometheus data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "91a0388492ae73a9ddb8a1701b639a6b6a71822a",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data/",
      "published_at": "2021-05-22T08:44:29Z",
      "updated_at": "2021-03-16T04:13:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To query and visualize the metrics collected for your Prometheus OpenMetrics or remote write integration with New Relic, you can use NRQL. You can also translate your PromQL-style queries to NRQL using either Grafana or the query builder. All metrics for Docker and Kubernetes are stored in the Metric type. Default attributes for the OpenMetrics integration By default, the following attributes will be added to all metrics for Docker and Kubernetes integrations: Default attributes (all integrations) Description clusterName The name of the cluster provided in the scraper configuration. integrationName The name of this integration (nri-prometheus). integrationVersion The version of the integration; for example, 0.2.0. metricName The name of the metric itself. nrMetricType The type of the New Relic Metric type; for example, Gauges. promMetricType The metric type of the Prometheus metric scrapedEndpoint The URL of the endpoint is being scraped. Kubernetes: If the scraper is running in Kubernetes, New Relic also adds the following attributes to all the metrics: Additional Kubernetes attributes Description deploymentName Name of the deployment, if scraping a pod. label The Kubernetes labels of the object being scraped, prefixed by \"label\". namespaceName Name of the namespace. nodeName Name of the node where the pod being scraped is running, if applicable. podName Name of the pod being scraped, if applicable. serviceName Name of the service being scraped, if applicable Default attributes for the remote write integration By default, the following attributes will be added to Prometheus remote write metrics: Default attributes (all integrations) Description prometheus_server A user supplied label specified as a Prometheus remote write URL parameter. The value supplied should be unique as it is intended to differentiate between source Prometheus servers at query time. Unspecified by default. newrelic.source The name of the New Relic ingest point (prometheusAPI). instrumentation.provider prometheus instrumentation.name remote-write instrumentation.source A user supplied identifier for the source of the Prometheus data that matches the value of prometheus_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL query examples When you build queries, be aware that there is no linking between the metrics, entities, and attributes. Use the following NRQL queries to find out which metrics are available and which attributes are present on these metrics: Get metric names To get all metric names for OpenMetrics: FROM Metric SELECT uniques(metricName) Copy To get metric names for a remote write integration: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' Copy To get metric names for a remote write integration from a single Prometheus source: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' AND instrumentation.source='<ds>' Copy To get metric names for a specific OpenMetrics endpoint: FROM Metric SELECT uniques(metricName) WHERE scrapedEndpoint='<ep>' Copy To get metric names for a specific OpenMetrics cluster, namespace, or pod: FROM Metric SELECT uniques(metricName) WHERE clusterName='<cn>' Copy FROM Metric SELECT uniques(metricName) WHERE namespaceName='<ns>' Copy FROM Metric SELECT uniques(metricName) WHERE podName='<pod>' Copy Get the attributes for a metric To get all attributes for the selected metric: FROM Metric SELECT keyset() WHERE metricName='<mn>' Copy Get the values for an attribute in OpenMetrics The autocomplete will show all values of the attribute, regardless of the pod. To determine the attribute values for a specific pod: FROM Metric SELECT uniques(<attribute>) WHERE metricName='<mn>' AND podName='<pod>' Copy Build the query Using metric name and attributes, you can query your data. For more information about facets, time series, and time selection, see the NRQL documentation. To build PromQL-style queries, see our docs. Get metric values To get raw metric values: FROM Metric SELECT <metricName> WHERE <attribute>='<value>' Copy Get a chart of the metric To get a chart of the metric with an aggregator of average, min, max, or sum: FROM Metric SELECT <aggregator>(<metricname>) WHERE <attribute>='<value>' TIMESERIES Copy Query counter metrics (deltas) Currently the integration calculates the deltas for counter metrics. This is why queries on counter metrics will show the deltas of the counter instead of the absolute value of the counter. View connected Redis clients per pod with OpenMetrics Docker: This example assumes you are scraping Redis exporters. To view the number of connected Redis clients per endpoint in a cluster: FROM Metric SELECT latest(redis_connected_clients) WHERE clusterName='my-cluster' FACET scrapedEndpoint TIMESERIES Copy Kubernetes: This example assumes that you have Redis pods with the Redis exporter installed. To view the number of connected Redis clients per pod in the default namespace: FROM Metric SELECT latest(redis_connected_clients) WHERE namespaceName='default' FACET podName TIMESERIES Copy Docker: View average memory free for scraped endpoints This example assumes you are scraping node exporters for Docker and want to use OpenMetrics. To view average memory free for all scraped endpoints in a cluster: FROM Metric SELECT average(node_memory_MemFree_bytes) WHERE clusterName='my-cluster' Copy Kubernetes: View average memory usage for pods in a deployment To view average memory usage for all pods in a Kubernetes deployment using OpenMetrics: FROM Metric SELECT average(container_memory_usage_bytes) WHERE deploymentName='my-app-deployment' AND namespaceName='default' Copy View data in New Relic When you query the data, you can view the results in the New Relic UI. You can also visualize the data as charts, histograms, etc. To view the NRQL query results for your Prometheus integration's data: Go to one.newrelic.com > Query your data. For more information, see New Relic's query builder documentation. Create histograms and summaries With remote write or version 1.2.0 or higher of the Prometheus OpenMetrics integration, you can create histograms and percentiles (summaries) of your data. The OpenMetrics data is based on New Relic's guidelines in GitHub for higher level metric abstractions, while the remote write data closely matches the schema of the original Prometheus data. Data presentation Comments Histograms A bucket <basename>_bucket{le=\"42\"} will be sent as this: For OpenMetrics: <basename>_buckets For remote write: <basename>_bucket The dimension will be this: For OpenMetrics: {histogram.bucket.upperBound=\"42\"} For remote write: {histogram.bucket.le=\"42\"} Percentiles Quantiles (summaries) are transformed into percentiles. A metric <basename>{quantile=\"0.3\"} will be sent to New Relic as <basename>.percentiles. The dimension will be this: {percentile=\"30\"} NRQL has two functions that work on remote write ingested PromQL: bucketPercentile() and histogram(). The links include query examples.These two functions don't work on OpenMetrics ingested buckets. To better support visualization of histograms, percentiles are calculated based on the histogram metrics and sent to New Relic. To configure the calculated percentiles for OpenMetrics, use the percentiles configuration option.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.20267,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "sections": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " identifier for the source of the <em>Prometheus</em> <em>data</em> that matches the value of <em>prometheus</em>_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL <em>query</em> examples When you build queries, be aware that there is no linking between the metrics, entities"
      },
      "id": "603eb04be7b9d20f9d2a07eb"
    },
    {
      "sections": [
        "Translate PromQL queries to NRQL",
        "Tip",
        "Prometheus and New Relic metric types",
        "Mapping between NRQL and our PromQL-style queries",
        "PromQL-style query example",
        "NRQL query example",
        "Filter examples",
        "PromQL-style to NRQL query examples",
        "For more help"
      ],
      "title": "Translate PromQL queries to NRQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "fcf45fb8fb49f9d22f74574c2e7032533377e584",
      "image": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/images/PROMQL-query-2.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/",
      "published_at": "2021-05-22T09:14:15Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Do you have a PromQL query you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style query language to explore your Prometheus OpenMetrics integration data along with other data sent to New Relic. Tip To run PromQL-style queries in New Relic One, go to the query builder advanced PromQL-style mode. Prometheus and New Relic metric types The different metric types supported by Prometheus and New Relic are related to each other: New Relic Prometheus Description Count Counter The Prometheus counter is a cumulative sum while the New Relic count is a delta sum. For example, if you see 2 requests in the first reporting period and 3 requests in the second reporting period. The Prometheus counter will report 2 and then 5, while the New Relic count will report 2 and then 3. Gauge Gauge A Prometheus gauge is similar to a New Relic gauge. Multiple counts Histogram Prometheus automatically maps a histogram to a set of counters. In New Relic, these counters should be changed to deltas and reported as counts. Gauges and counts Summary Prometheus represents a Summary with a given basename as the following time series: a basename_sum a basename_count and 0 or more of basename{quantile=\".xx\"...} metrics New Relic maps the _sum as a Summary, the _count as a Counter, and each quantile metric as a Gauge. Summary (No equivalent in Prometheus) New Relic has a distinct metric type called a summary that is different than the Prometheus summary. It is designed for reporting aggregated discrete events so that you can query the count, sum, min, max, and average values. Mapping between NRQL and our PromQL-style queries Tip To see how New Relic translates PromQL-style queries to NRQL, write a query in the query builder PromQL-style tab, then switch to the NRQL tab. This table shows the mapping between NRQL and our PromQL-style queries when exploring data. For more contextual information, see the examples. Description Mapping between NRQL and PromQL-style queries Search for attributes: Explore the attributes on the container_memory_usage_bytes metric. PromQL: container_memory_usage_bytes Copy NRQL: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy Find attribute's value: Explore the current value of the container_memory_usage_bytes metric for unique id attributes. PromQL: sum(container_memory_usage_bytes) by (id) Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy Visualize the attribute's value: Chart the value of the container_memory_usage_bytes metric with the given id attribute value. PromQL: container_memory_usage_bytes{id=\"/\"} Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = '/' TIMESERIES Copy PromQL-style query example 1. Start your query. When exploring your data for a particular metric in PromQL, such as memory by container usage in bytes, you can start with a query such as: container_memory_usage_bytes Copy This will chart all the unique metric timeseries for the input metric. 2. Filter the query results. Looking at the data, you can add more query parameters to filter down the number of metric timeseries. For example, if you only want timeseries where the id is /, the PromQL-style query will be: container_memory_usage_bytes{id=\"/\"} Copy PromQL-style example: To filter the data, run this PromQL-style query: container_memory_usage_bytes { id=\"/\"}. NRQL query example 1. Query available metrics. To explore your data, start by looking at all the available metrics. Use the following NRQL query: FROM Metric SELECT uniques(metricName) Copy 2. Find unique attributes. Once you have found the metric you want to review, such as container_memory_usage_bytes, you can find the unique attributes with the following query: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy The results will show each available attribute key and the value type (string, boolean, or number). 3. Aggregate and chart the metrics. To chart metrics using NRQL, you first need an aggregation function. For example, you can use latest for gauges, sum for counts, and average for summaries. As the following chart shows, all the unique timeseries are aggregated into one unique timeseries by default: one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes'. 4. View metrics by ID. To view the unique metric timeseries with various id values, run the following query: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT latest(container_memory_usage_bytes) FACET id. 5. Add the selected ID to the query. Next you can select an id value and put it in the NRQL where clause. FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = \"/\" timeseries Copy one.newrelic.com > Query your data: This example shows the data displayed after running From Metric select latest(container_memory_usage_bytes) where id = \"/\" timeseries. Filter examples Both our PromQL-style query language and NRQL provide syntax to filter down the number of unique metric timeseries. PromQL-style uses brackets to filter. NRQL uses a WHERE clause. Here are some example queries: Description PromQL-style and NRQL queries Select data with specific values. PromQL: go_memstats_heap_alloc_bytes{job=\"apiserver\", instance=\"1234\"}) Copy NRQL: To only select data with specific values in NRQL, use the WHERE clause with =. In this example, all data must have the selected value for job and handler. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE job = 'apiserver' AND instance = '1234' TIMESERIES Copy Select data with multiple values. PromQL: go_memstats_heap_alloc_bytes{environment=~\"staging|testing|development\",method!=\"GET\"} Copy NRQL: In NRQL use the in clause to select multiple values for an attribute and the != sign to select all values but the one listed. In this example, the environment can be staging, testing, or development, and the method cannot be GET. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE environment IN ('staging', 'testing', 'development') AND method != 'GET' TIMESERIES Copy Select data using partial string values. PromQL: go_memstats_heap_alloc_bytes{job=~\"api.*\"} Copy NRQL: In NRQL use the LIKE clause to match part of a string value. In this example, all data will be returned where the job attributes start with api. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHEREe job LIKE 'api%' TIMESERIES Copy PromQL-style to NRQL query examples You can simulate the following PromQL-style queries with NRQL queries: Description PromQL-style and NRQL queries Measure the per second rate over the last minute of the http_request_total metric. PromQL: sum(rate(http_requests_total[1m])) Copy NRQL: FROM Metric SELECT rate(sum(http_request_total), 1 second) TIMESERIES 1 minute Copy Chart the difference of the two metrics, then divide by 1024. PromQL: (instance_memory_limit_bytes - instance_memory_usage_bytes) / 1024 Copy NRQL: FROM Metric SELECT (latest(instance_memory_limit_bytes) - latest(instance_memory_usage_bytes)) / 1024 TIMESERIES Copy Provide the summed rate per 30-second interval by each handler. PromQL: sum(rate(http_requests_total[30s])) by (handler) Copy NRQL: FROM Metric SELECT rate(sum(http_requests_total), 30 seconds) FACET handler TIMESERIES Copy Chart the difference in the two metrics where the instance is named foo and the fstype is either ext4 or xfs. PromQL: (node_filesystem_free_bytes{instance='foo',fstype=~\"ext4|xfs\"} / node_filesystem_size_bytes{instance='foo',fstype=~\"ext4|xfs\"}) Copy NRQL: FROM Metric SELECT latest(node_filesystem_free_bytes) / latest(node_filesystem_size_bytes) WHERE instance = 'foo' AND fstype IN ('ext4', 'xfs') Copy For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.61276,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Translate PromQL <em>queries</em> to NRQL",
        "sections": "<em>Prometheus</em> <em>and</em> New Relic metric types",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": "Do you have a PromQL <em>query</em> you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style <em>query</em> language to explore your <em>Prometheus</em> OpenMetrics integration <em>data</em> along with other <em>data</em> sent to New"
      },
      "id": "603ead4528ccbcbecfeba77b"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-05-22T16:08:58Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 83.34201,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "sections": "Configure <em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": " PromQL to NRQL. # The name of your cluster. It&#x27;s important to match other New Relic products to relate the <em>data</em>. cluster_name: &quot;&lt;YOUR_CLUSTER_NAME&gt;&quot; # When standalone is set to false nri-<em>prometheus</em> requires an infrastructure agent to work and send <em>data</em>. Defaults to true # standalone: true # How"
      },
      "id": "603e830964441f85a04e8877"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql": [
    {
      "sections": [
        "View and query your Prometheus data",
        "Default attributes for the OpenMetrics integration",
        "Default attributes for the remote write integration",
        "NRQL query examples",
        "Get metric names",
        "Get the attributes for a metric",
        "Get the values for an attribute in OpenMetrics",
        "Build the query",
        "Get metric values",
        "Get a chart of the metric",
        "Query counter metrics (deltas)",
        "View connected Redis clients per pod with OpenMetrics",
        "Docker: View average memory free for scraped endpoints",
        "Kubernetes: View average memory usage for pods in a deployment",
        "View data in New Relic",
        "Create histograms and summaries"
      ],
      "title": "View and query your Prometheus data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "91a0388492ae73a9ddb8a1701b639a6b6a71822a",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data/",
      "published_at": "2021-05-22T08:44:29Z",
      "updated_at": "2021-03-16T04:13:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To query and visualize the metrics collected for your Prometheus OpenMetrics or remote write integration with New Relic, you can use NRQL. You can also translate your PromQL-style queries to NRQL using either Grafana or the query builder. All metrics for Docker and Kubernetes are stored in the Metric type. Default attributes for the OpenMetrics integration By default, the following attributes will be added to all metrics for Docker and Kubernetes integrations: Default attributes (all integrations) Description clusterName The name of the cluster provided in the scraper configuration. integrationName The name of this integration (nri-prometheus). integrationVersion The version of the integration; for example, 0.2.0. metricName The name of the metric itself. nrMetricType The type of the New Relic Metric type; for example, Gauges. promMetricType The metric type of the Prometheus metric scrapedEndpoint The URL of the endpoint is being scraped. Kubernetes: If the scraper is running in Kubernetes, New Relic also adds the following attributes to all the metrics: Additional Kubernetes attributes Description deploymentName Name of the deployment, if scraping a pod. label The Kubernetes labels of the object being scraped, prefixed by \"label\". namespaceName Name of the namespace. nodeName Name of the node where the pod being scraped is running, if applicable. podName Name of the pod being scraped, if applicable. serviceName Name of the service being scraped, if applicable Default attributes for the remote write integration By default, the following attributes will be added to Prometheus remote write metrics: Default attributes (all integrations) Description prometheus_server A user supplied label specified as a Prometheus remote write URL parameter. The value supplied should be unique as it is intended to differentiate between source Prometheus servers at query time. Unspecified by default. newrelic.source The name of the New Relic ingest point (prometheusAPI). instrumentation.provider prometheus instrumentation.name remote-write instrumentation.source A user supplied identifier for the source of the Prometheus data that matches the value of prometheus_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL query examples When you build queries, be aware that there is no linking between the metrics, entities, and attributes. Use the following NRQL queries to find out which metrics are available and which attributes are present on these metrics: Get metric names To get all metric names for OpenMetrics: FROM Metric SELECT uniques(metricName) Copy To get metric names for a remote write integration: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' Copy To get metric names for a remote write integration from a single Prometheus source: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' AND instrumentation.source='<ds>' Copy To get metric names for a specific OpenMetrics endpoint: FROM Metric SELECT uniques(metricName) WHERE scrapedEndpoint='<ep>' Copy To get metric names for a specific OpenMetrics cluster, namespace, or pod: FROM Metric SELECT uniques(metricName) WHERE clusterName='<cn>' Copy FROM Metric SELECT uniques(metricName) WHERE namespaceName='<ns>' Copy FROM Metric SELECT uniques(metricName) WHERE podName='<pod>' Copy Get the attributes for a metric To get all attributes for the selected metric: FROM Metric SELECT keyset() WHERE metricName='<mn>' Copy Get the values for an attribute in OpenMetrics The autocomplete will show all values of the attribute, regardless of the pod. To determine the attribute values for a specific pod: FROM Metric SELECT uniques(<attribute>) WHERE metricName='<mn>' AND podName='<pod>' Copy Build the query Using metric name and attributes, you can query your data. For more information about facets, time series, and time selection, see the NRQL documentation. To build PromQL-style queries, see our docs. Get metric values To get raw metric values: FROM Metric SELECT <metricName> WHERE <attribute>='<value>' Copy Get a chart of the metric To get a chart of the metric with an aggregator of average, min, max, or sum: FROM Metric SELECT <aggregator>(<metricname>) WHERE <attribute>='<value>' TIMESERIES Copy Query counter metrics (deltas) Currently the integration calculates the deltas for counter metrics. This is why queries on counter metrics will show the deltas of the counter instead of the absolute value of the counter. View connected Redis clients per pod with OpenMetrics Docker: This example assumes you are scraping Redis exporters. To view the number of connected Redis clients per endpoint in a cluster: FROM Metric SELECT latest(redis_connected_clients) WHERE clusterName='my-cluster' FACET scrapedEndpoint TIMESERIES Copy Kubernetes: This example assumes that you have Redis pods with the Redis exporter installed. To view the number of connected Redis clients per pod in the default namespace: FROM Metric SELECT latest(redis_connected_clients) WHERE namespaceName='default' FACET podName TIMESERIES Copy Docker: View average memory free for scraped endpoints This example assumes you are scraping node exporters for Docker and want to use OpenMetrics. To view average memory free for all scraped endpoints in a cluster: FROM Metric SELECT average(node_memory_MemFree_bytes) WHERE clusterName='my-cluster' Copy Kubernetes: View average memory usage for pods in a deployment To view average memory usage for all pods in a Kubernetes deployment using OpenMetrics: FROM Metric SELECT average(container_memory_usage_bytes) WHERE deploymentName='my-app-deployment' AND namespaceName='default' Copy View data in New Relic When you query the data, you can view the results in the New Relic UI. You can also visualize the data as charts, histograms, etc. To view the NRQL query results for your Prometheus integration's data: Go to one.newrelic.com > Query your data. For more information, see New Relic's query builder documentation. Create histograms and summaries With remote write or version 1.2.0 or higher of the Prometheus OpenMetrics integration, you can create histograms and percentiles (summaries) of your data. The OpenMetrics data is based on New Relic's guidelines in GitHub for higher level metric abstractions, while the remote write data closely matches the schema of the original Prometheus data. Data presentation Comments Histograms A bucket <basename>_bucket{le=\"42\"} will be sent as this: For OpenMetrics: <basename>_buckets For remote write: <basename>_bucket The dimension will be this: For OpenMetrics: {histogram.bucket.upperBound=\"42\"} For remote write: {histogram.bucket.le=\"42\"} Percentiles Quantiles (summaries) are transformed into percentiles. A metric <basename>{quantile=\"0.3\"} will be sent to New Relic as <basename>.percentiles. The dimension will be this: {percentile=\"30\"} NRQL has two functions that work on remote write ingested PromQL: bucketPercentile() and histogram(). The links include query examples.These two functions don't work on OpenMetrics ingested buckets. To better support visualization of histograms, percentiles are calculated based on the histogram metrics and sent to New Relic. To configure the calculated percentiles for OpenMetrics, use the percentiles configuration option.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.20267,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "sections": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " identifier for the source of the <em>Prometheus</em> <em>data</em> that matches the value of <em>prometheus</em>_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL <em>query</em> examples When you build queries, be aware that there is no linking between the metrics, entities"
      },
      "id": "603eb04be7b9d20f9d2a07eb"
    },
    {
      "sections": [
        "Supported PromQL Features",
        "Important",
        "Supported features",
        "Aggregation operators and functions",
        "Arithmetic binary operators",
        "Logical operators",
        "Date/time functions",
        "Mathematical functions",
        "Rate-like functions",
        "Predictive functions",
        "Time-series selectors",
        "PromQL troubleshooting",
        "Metric types",
        "Limits",
        "Range vector selectors (sliding windows and smoothing behavior)",
        "Query range and data scraping intervals"
      ],
      "title": "Supported PromQL Features",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "d4a93b9db3bfe5639ed01968b6e55a8e0aaa9389",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features/",
      "published_at": "2021-05-22T08:43:25Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic supports PromQL-style queries, and our query builder offers a PromQL-style query mode that translates PromQL syntax queries into the closest NRQL approximation. Although the method of approximation means that a handful of edge cases are not fully supported, it provides coverage for an overwhelming majority of queries, supporting over 99.5% of queries across the 7.8 million top Grafana dashboard downloads. Read on to learn about how we work with PromQL queries, as well as differences between standard PromQL and our PromQL-like query language we want you to be aware of. Important For general information about Prometheus queries and operators, see the Prometheus.io documentation. Supported features We support the following aggregation, arithmetic, mathematical, and rate-like functions. As we continue to expand support for Prometheus and PromQL, this list will be updated. Aggregation operators and functions Aggregation operators: avg() count() min() max() quantile() stddev() stdvar() sum() topk() Aggregation functions: histogram_quantile() <aggregation>_over_time() functions: avg_over_time count_over_time min_over_time max_over_time quantile_over_time stdev_over_time stvar_over_time sum_over_time Arithmetic binary operators + (addition) - (subtraction) * (multiplication) / (division) % (percent) ^ (power/exponents) Logical operators and or Date/time functions day_of_month() day_of_week() days_in_month() hour() minute() month() time() timestamp() year() Mathematical functions abs() ceil() clamp_max() clamp_min() exp() floor() ln() log10() log2() round() sqrt() Rate-like functions delta() deriv() idelta() increase() irate() rate() Predictive functions predict_linear Time-series selectors We offer support for PromQL time-series selectors including the following: instant vector selectors range series selectors offset modifier Important We only support offset queries if every vector in the query has the same offset value. PromQL troubleshooting This section describes differences in behavior between PromQL and our PromQL-style query behavior and how to work with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the query builder. Metric types Prometheus recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase() on counters, but queries in Prometheus still work most of the time even if they don’t follow those instructions. However, because NRDB converts PromQL-style accumulating counters to delta counters, our implementation is unforgiving when using these functions on the wrong data type and will produce different or incorrect answers. For this reason, it's best to follow all Prometheus recommendations when working with our PromQL-style queries, even if you don't follow these recommendations in Prometheus. Limits In order to ensure the stability and performance of our system for all users, we place some limits on what queries can be run. In all cases, we enforce a limit of 366 steps in range queries. We also default to only returning 100 timeseries from queries by default. If you want to see more (or fewer), you need to explicitly add a topk() to your query. (Note that the topk() implementation in our PromQL-style query is different from that of Prometheus.) We limit the total memory a query can use. This means that requests for large numbers of time steps or large numbers of time series may be rejected, particularly if they are combined with an aggregation like unique count or quantile which require significantly more memory to compute than simple arithmetic aggregations. Range vector selectors (sliding windows and smoothing behavior) We provide support for sliding window timeseries aggregations. For more information, see our NRQL syntax, clauses, and functions resource and our sliding windows deep dive. For information on translating between NRQL and our PromQL-style language, see Translate PromQL queries to NRQL. Query range and data scraping intervals The range of your query in PromQL must be larger than the duration of the step size of the query to avoid the error \"TIMESERIES bucket size is larger than the current time window\". We inspect data up to one minute old when servicing instant queries. If your scrape interval is greater than 1 minute, some queries may result in No data found. Avoid this by sending data at least once per minute. If the timeseries unit for your NRQL query is less than the scrape interval for your application, some periods will lack data, and the resulting graph may be jagged or contain peaks and valleys. In general, set the step size to your scrape interval, or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.61693,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Query</em> range <em>and</em> <em>data</em> scraping intervals",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the <em>query</em> builder. Metric types <em>Prometheus</em> recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase"
      },
      "id": "603e9523e7b9d292ec2a07ce"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-05-22T16:08:58Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 83.34201,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "sections": "Configure <em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": " PromQL to NRQL. # The name of your cluster. It&#x27;s important to match other New Relic products to relate the <em>data</em>. cluster_name: &quot;&lt;YOUR_CLUSTER_NAME&gt;&quot; # When standalone is set to false nri-<em>prometheus</em> requires an infrastructure agent to work and send <em>data</em>. Defaults to true # standalone: true # How"
      },
      "id": "603e830964441f85a04e8877"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data": [
    {
      "sections": [
        "Supported PromQL Features",
        "Important",
        "Supported features",
        "Aggregation operators and functions",
        "Arithmetic binary operators",
        "Logical operators",
        "Date/time functions",
        "Mathematical functions",
        "Rate-like functions",
        "Predictive functions",
        "Time-series selectors",
        "PromQL troubleshooting",
        "Metric types",
        "Limits",
        "Range vector selectors (sliding windows and smoothing behavior)",
        "Query range and data scraping intervals"
      ],
      "title": "Supported PromQL Features",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "d4a93b9db3bfe5639ed01968b6e55a8e0aaa9389",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features/",
      "published_at": "2021-05-22T08:43:25Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic supports PromQL-style queries, and our query builder offers a PromQL-style query mode that translates PromQL syntax queries into the closest NRQL approximation. Although the method of approximation means that a handful of edge cases are not fully supported, it provides coverage for an overwhelming majority of queries, supporting over 99.5% of queries across the 7.8 million top Grafana dashboard downloads. Read on to learn about how we work with PromQL queries, as well as differences between standard PromQL and our PromQL-like query language we want you to be aware of. Important For general information about Prometheus queries and operators, see the Prometheus.io documentation. Supported features We support the following aggregation, arithmetic, mathematical, and rate-like functions. As we continue to expand support for Prometheus and PromQL, this list will be updated. Aggregation operators and functions Aggregation operators: avg() count() min() max() quantile() stddev() stdvar() sum() topk() Aggregation functions: histogram_quantile() <aggregation>_over_time() functions: avg_over_time count_over_time min_over_time max_over_time quantile_over_time stdev_over_time stvar_over_time sum_over_time Arithmetic binary operators + (addition) - (subtraction) * (multiplication) / (division) % (percent) ^ (power/exponents) Logical operators and or Date/time functions day_of_month() day_of_week() days_in_month() hour() minute() month() time() timestamp() year() Mathematical functions abs() ceil() clamp_max() clamp_min() exp() floor() ln() log10() log2() round() sqrt() Rate-like functions delta() deriv() idelta() increase() irate() rate() Predictive functions predict_linear Time-series selectors We offer support for PromQL time-series selectors including the following: instant vector selectors range series selectors offset modifier Important We only support offset queries if every vector in the query has the same offset value. PromQL troubleshooting This section describes differences in behavior between PromQL and our PromQL-style query behavior and how to work with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the query builder. Metric types Prometheus recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase() on counters, but queries in Prometheus still work most of the time even if they don’t follow those instructions. However, because NRDB converts PromQL-style accumulating counters to delta counters, our implementation is unforgiving when using these functions on the wrong data type and will produce different or incorrect answers. For this reason, it's best to follow all Prometheus recommendations when working with our PromQL-style queries, even if you don't follow these recommendations in Prometheus. Limits In order to ensure the stability and performance of our system for all users, we place some limits on what queries can be run. In all cases, we enforce a limit of 366 steps in range queries. We also default to only returning 100 timeseries from queries by default. If you want to see more (or fewer), you need to explicitly add a topk() to your query. (Note that the topk() implementation in our PromQL-style query is different from that of Prometheus.) We limit the total memory a query can use. This means that requests for large numbers of time steps or large numbers of time series may be rejected, particularly if they are combined with an aggregation like unique count or quantile which require significantly more memory to compute than simple arithmetic aggregations. Range vector selectors (sliding windows and smoothing behavior) We provide support for sliding window timeseries aggregations. For more information, see our NRQL syntax, clauses, and functions resource and our sliding windows deep dive. For information on translating between NRQL and our PromQL-style language, see Translate PromQL queries to NRQL. Query range and data scraping intervals The range of your query in PromQL must be larger than the duration of the step size of the query to avoid the error \"TIMESERIES bucket size is larger than the current time window\". We inspect data up to one minute old when servicing instant queries. If your scrape interval is greater than 1 minute, some queries may result in No data found. Avoid this by sending data at least once per minute. If the timeseries unit for your NRQL query is less than the scrape interval for your application, some periods will lack data, and the resulting graph may be jagged or contain peaks and valleys. In general, set the step size to your scrape interval, or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.61693,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Query</em> range <em>and</em> <em>data</em> scraping intervals",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the <em>query</em> builder. Metric types <em>Prometheus</em> recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase"
      },
      "id": "603e9523e7b9d292ec2a07ce"
    },
    {
      "sections": [
        "Translate PromQL queries to NRQL",
        "Tip",
        "Prometheus and New Relic metric types",
        "Mapping between NRQL and our PromQL-style queries",
        "PromQL-style query example",
        "NRQL query example",
        "Filter examples",
        "PromQL-style to NRQL query examples",
        "For more help"
      ],
      "title": "Translate PromQL queries to NRQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "fcf45fb8fb49f9d22f74574c2e7032533377e584",
      "image": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/images/PROMQL-query-2.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/",
      "published_at": "2021-05-22T09:14:15Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Do you have a PromQL query you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style query language to explore your Prometheus OpenMetrics integration data along with other data sent to New Relic. Tip To run PromQL-style queries in New Relic One, go to the query builder advanced PromQL-style mode. Prometheus and New Relic metric types The different metric types supported by Prometheus and New Relic are related to each other: New Relic Prometheus Description Count Counter The Prometheus counter is a cumulative sum while the New Relic count is a delta sum. For example, if you see 2 requests in the first reporting period and 3 requests in the second reporting period. The Prometheus counter will report 2 and then 5, while the New Relic count will report 2 and then 3. Gauge Gauge A Prometheus gauge is similar to a New Relic gauge. Multiple counts Histogram Prometheus automatically maps a histogram to a set of counters. In New Relic, these counters should be changed to deltas and reported as counts. Gauges and counts Summary Prometheus represents a Summary with a given basename as the following time series: a basename_sum a basename_count and 0 or more of basename{quantile=\".xx\"...} metrics New Relic maps the _sum as a Summary, the _count as a Counter, and each quantile metric as a Gauge. Summary (No equivalent in Prometheus) New Relic has a distinct metric type called a summary that is different than the Prometheus summary. It is designed for reporting aggregated discrete events so that you can query the count, sum, min, max, and average values. Mapping between NRQL and our PromQL-style queries Tip To see how New Relic translates PromQL-style queries to NRQL, write a query in the query builder PromQL-style tab, then switch to the NRQL tab. This table shows the mapping between NRQL and our PromQL-style queries when exploring data. For more contextual information, see the examples. Description Mapping between NRQL and PromQL-style queries Search for attributes: Explore the attributes on the container_memory_usage_bytes metric. PromQL: container_memory_usage_bytes Copy NRQL: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy Find attribute's value: Explore the current value of the container_memory_usage_bytes metric for unique id attributes. PromQL: sum(container_memory_usage_bytes) by (id) Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy Visualize the attribute's value: Chart the value of the container_memory_usage_bytes metric with the given id attribute value. PromQL: container_memory_usage_bytes{id=\"/\"} Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = '/' TIMESERIES Copy PromQL-style query example 1. Start your query. When exploring your data for a particular metric in PromQL, such as memory by container usage in bytes, you can start with a query such as: container_memory_usage_bytes Copy This will chart all the unique metric timeseries for the input metric. 2. Filter the query results. Looking at the data, you can add more query parameters to filter down the number of metric timeseries. For example, if you only want timeseries where the id is /, the PromQL-style query will be: container_memory_usage_bytes{id=\"/\"} Copy PromQL-style example: To filter the data, run this PromQL-style query: container_memory_usage_bytes { id=\"/\"}. NRQL query example 1. Query available metrics. To explore your data, start by looking at all the available metrics. Use the following NRQL query: FROM Metric SELECT uniques(metricName) Copy 2. Find unique attributes. Once you have found the metric you want to review, such as container_memory_usage_bytes, you can find the unique attributes with the following query: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy The results will show each available attribute key and the value type (string, boolean, or number). 3. Aggregate and chart the metrics. To chart metrics using NRQL, you first need an aggregation function. For example, you can use latest for gauges, sum for counts, and average for summaries. As the following chart shows, all the unique timeseries are aggregated into one unique timeseries by default: one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes'. 4. View metrics by ID. To view the unique metric timeseries with various id values, run the following query: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT latest(container_memory_usage_bytes) FACET id. 5. Add the selected ID to the query. Next you can select an id value and put it in the NRQL where clause. FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = \"/\" timeseries Copy one.newrelic.com > Query your data: This example shows the data displayed after running From Metric select latest(container_memory_usage_bytes) where id = \"/\" timeseries. Filter examples Both our PromQL-style query language and NRQL provide syntax to filter down the number of unique metric timeseries. PromQL-style uses brackets to filter. NRQL uses a WHERE clause. Here are some example queries: Description PromQL-style and NRQL queries Select data with specific values. PromQL: go_memstats_heap_alloc_bytes{job=\"apiserver\", instance=\"1234\"}) Copy NRQL: To only select data with specific values in NRQL, use the WHERE clause with =. In this example, all data must have the selected value for job and handler. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE job = 'apiserver' AND instance = '1234' TIMESERIES Copy Select data with multiple values. PromQL: go_memstats_heap_alloc_bytes{environment=~\"staging|testing|development\",method!=\"GET\"} Copy NRQL: In NRQL use the in clause to select multiple values for an attribute and the != sign to select all values but the one listed. In this example, the environment can be staging, testing, or development, and the method cannot be GET. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE environment IN ('staging', 'testing', 'development') AND method != 'GET' TIMESERIES Copy Select data using partial string values. PromQL: go_memstats_heap_alloc_bytes{job=~\"api.*\"} Copy NRQL: In NRQL use the LIKE clause to match part of a string value. In this example, all data will be returned where the job attributes start with api. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHEREe job LIKE 'api%' TIMESERIES Copy PromQL-style to NRQL query examples You can simulate the following PromQL-style queries with NRQL queries: Description PromQL-style and NRQL queries Measure the per second rate over the last minute of the http_request_total metric. PromQL: sum(rate(http_requests_total[1m])) Copy NRQL: FROM Metric SELECT rate(sum(http_request_total), 1 second) TIMESERIES 1 minute Copy Chart the difference of the two metrics, then divide by 1024. PromQL: (instance_memory_limit_bytes - instance_memory_usage_bytes) / 1024 Copy NRQL: FROM Metric SELECT (latest(instance_memory_limit_bytes) - latest(instance_memory_usage_bytes)) / 1024 TIMESERIES Copy Provide the summed rate per 30-second interval by each handler. PromQL: sum(rate(http_requests_total[30s])) by (handler) Copy NRQL: FROM Metric SELECT rate(sum(http_requests_total), 30 seconds) FACET handler TIMESERIES Copy Chart the difference in the two metrics where the instance is named foo and the fstype is either ext4 or xfs. PromQL: (node_filesystem_free_bytes{instance='foo',fstype=~\"ext4|xfs\"} / node_filesystem_size_bytes{instance='foo',fstype=~\"ext4|xfs\"}) Copy NRQL: FROM Metric SELECT latest(node_filesystem_free_bytes) / latest(node_filesystem_size_bytes) WHERE instance = 'foo' AND fstype IN ('ext4', 'xfs') Copy For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.61275,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Translate PromQL <em>queries</em> to NRQL",
        "sections": "<em>Prometheus</em> <em>and</em> New Relic metric types",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": "Do you have a PromQL <em>query</em> you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style <em>query</em> language to explore your <em>Prometheus</em> OpenMetrics integration <em>data</em> along with other <em>data</em> sent to New"
      },
      "id": "603ead4528ccbcbecfeba77b"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-05-22T16:08:58Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 83.341995,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "sections": "Configure <em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": " PromQL to NRQL. # The name of your cluster. It&#x27;s important to match other New Relic products to relate the <em>data</em>. cluster_name: &quot;&lt;YOUR_CLUSTER_NAME&gt;&quot; # When standalone is set to false nri-<em>prometheus</em> requires an infrastructure agent to work and send <em>data</em>. Defaults to true # standalone: true # How"
      },
      "id": "603e830964441f85a04e8877"
    }
  ],
  "/docs/introduction-new-relic-agent-sdk": [
    {
      "image": "https://docs.newrelic.com/static/futurestack-registration-cf388ce323a82db74de9894bfbe3fb63.png",
      "url": "https://docs.newrelic.com/",
      "sections": [
        "Welcome to New Relic",
        "Create a free account",
        "Start collecting data",
        "Set up alerts",
        "Telemetry Data Platform",
        "Introduction to Telemetry Data Platform",
        "Data explorer",
        "Dashboards",
        "Log management",
        "APIs",
        "Manage data",
        "Build on New Relic One",
        "Full-Stack Observability",
        "Introduction to Full-Stack Observability",
        "APM",
        "Browser",
        "Distributed Tracing",
        "Infrastructure",
        "Logs in context",
        "Mobile",
        "Serverless",
        "Synthetics",
        "Workloads",
        "Alerts and Applied Intelligence (AI)",
        "Introduction to Alerts",
        "Introduction to Applied Intelligence",
        "Incident Intelligence",
        "Incident Workflows",
        "Proactive Detection",
        "New Relic integrations",
        "Back-end, front-end, and mobile applications",
        "Infrastructure and cloud platforms",
        "Open-source monitoring systems",
        "Security, privacy, and legal information",
        "Data privacy",
        "Security compliance",
        "Security bulletins",
        "Licenses"
      ],
      "published_at": "2021-05-26T01:37:37Z",
      "title": "New Relic Documentation",
      "updated_at": "2021-05-23T01:37:22Z",
      "type": "docs",
      "external_id": "807276d5fdd805ba0247a1ce1016af986fb31995",
      "popularity": 1,
      "body": "Welcome to New Relic If you're new, follow these three steps to create an account and get going. (It's free!) Scroll on for more about our Telemetry Data Platform, Full-Stack Observability, and Applied Intelligence. Or, to get a wider view of our platform's capabilities, read the Intro to New Relic, and use our solutions and best practices guides. Create a free account No credit card required. Start collecting data Our UI guides you through setup and install. Set up alerts Get notified quickly about changes in your system. Telemetry Data Platform Ingest, visualize, and alert on all your telemetry data in one place. Introduction to Telemetry Data Platform How to manage all your monitoring in one place. Data explorer Query and build charts with NRQL, our PromQL-style syntax, or our visual chart builder. Dashboards Combine data from anywhere in our platform into customized dashboards. Log management Get logs alongside your telemetry data with our fast, scalable log management. APIs Find APIs to send data in, get data out, or manage the New Relic platform. Manage data Monitor and control your data usage. Build on New Relic One Learn how to build custom apps on our platform at developer.newrelic.com Full-Stack Observability Analyze and troubleshoot problems easily across your entire software stack. Introduction to Full-Stack Observability Get deep insight into everything from infrastructure to server code to end-user apps. APM Get real-time and trending data about your app's performance and stability. Browser Measure website performance, track errors, and see how users interact with your site. Distributed Tracing Track requests through your distributed system to find trends and anomalies. Infrastructure Monitor your infrastructure—hosts, cloud providers, container services, backend services, orchestrators, and more. Logs in context Link your log data to APM agent data so you can pinpoint where things are going wrong. Mobile Understand user journeys in your Android and iOS apps and troubleshoot crashes. Serverless Monitor AWS Lambda, Azure Functions, and Google Cloud Functions. Synthetics Simulate user activity to detect outages and fix poor website performance. Workloads Group or monitor entities across your entire stack based on a team or a set of responsibilities. Alerts and Applied Intelligence (AI) Automatically detect anomalies, correlate issues, and reduce alert noise. Introduction to Alerts Get notified about important changes in your system based on any data you connect to New Relic. Introduction to Applied Intelligence Recognize issues sooner, resolve problems faster, and reduce noise for better incident management. Incident Intelligence Get an overview of all your incidents. See sources and related events, and find out how they all correlate. Incident Workflows Enrich your incidents with New Relic data before sending them to your notification platform. Proactive Detection Get notified by Slack or webhook of unusual app behavior. New Relic integrations Integrations connect the technologies in your stack to New Relic. Here are a few of our 370+ integrations: Back-end, front-end, and mobile applications Android Browser (JavaScript) C SDK Go iOS Java .NET Node.js PHP Python Ruby Synthetics Infrastructure and cloud platforms Apache AWS Azure Google Cloud Kafka Kubernetes Linux Microsoft SQL MongoDB MySQL NGINX PostgreSQL Redis Windows Open-source monitoring systems Dropwizard Istio JMX Kamon Micrometer OpenCensus OpenTelemetry Prometheus StatsD See all 360+ integrations Security, privacy, and legal information Find out how we ensure security, data privacy, and compliance, and find our terms of service. Data privacy New Relic takes your data privacy seriously. Our principles-based approach aims to go beyond the legal requirements for consent. Security compliance Whether your data is in transit to New Relic or at rest in our storage, strong encryption measures can help protect against unauthorized access or theft of valuable data. Security bulletins Security bulletins are a way for us to let you know about security vulnerabilities, remediation strategies, and applicable updates. Licenses All about New Relic's licenses, Usage Plan, and policies. Find out what open source software we use in our products.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 109.13074,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> Documentation",
        "sections": "Welcome <em>to</em> <em>New</em> <em>Relic</em>",
        "body": " alert noise. <em>Introduction</em> to Alerts Get notified about important changes in your system based on any data you connect to <em>New</em> <em>Relic</em>. <em>Introduction</em> to Applied Intelligence Recognize issues sooner, resolve problems faster, and reduce noise for better incident management. Incident Intelligence Get"
      },
      "id": "5be4e5d38e9c0f3585d72edf"
    },
    {
      "image": "https://developer.newrelic.com/static/c1fd6182602c7dbc74bf14b13dc1a4c0/0086b/dev-terms-and-conditions.png",
      "url": "https://developer.newrelic.com/build-apps/ab-test/install-nr1/",
      "sections": [
        "Install and configure the New Relic One CLI",
        "Course",
        "Install and configure the CLI",
        "Tip"
      ],
      "published_at": "2021-05-26T01:43:50Z",
      "title": "Install and configure the New Relic One CLI",
      "updated_at": "2021-05-21T01:46:56Z",
      "type": "developer",
      "external_id": "a09ffc1a2296796669ad9d026d6c16937b23a3d4",
      "document_type": "page",
      "popularity": 1,
      "info": "Install and configure the New Relic One CLI",
      "body": "Course This lesson is part of a course that teaches you how to build a New Relic One application from the ground up. If you haven't already, check out the course introduction. Each lesson in the course builds upon the last, so make sure you've completed the last lesson, Spin up your demo services, before starting this one. One of the primary elements of the New Relic One SDK is the command line interface (CLI). To create a Nerdpack , you'll need to install the SDK, configure the CLI to work with your New Relic account, and then utilize its create command. Install and configure the CLI Step 1 of 5 Go to the Build on New Relic quick start. Step 2 of 5 Get your API key: Once you install the CLI, you'll use this key to create a user profile that's associated with your account. The CLI uses this profile to manage entities within your account. Step 3 of 5 Read and accept the New Relic developer terms and conditions: Even if you install the CLI, you won't be able to use it without first accepting these terms and conditions. Step 4 of 5 Choose your operating system and click Download installer: Once you've installed the SDK, you'll have access to the nr1 CLI. Verify this by checking your SDK version: bash Copy $ nr1 --version If you already had the CLI, update it: bash Copy $ nr1 update Tip It’s important to distinguish between the newrelic CLI and the nr1 CLI. newrelic is for managing entities in your New Relic account. nr1 is for managing New Relic One applications. Step 5 of 5 Copy the command to save your credentials: This command has a profile name, your region, and your API key baked in. Run the command in your terminal: bash Copy $ nr1 profiles:add --name <profile name> --api-key <User key> --region <region> Profiles let you select which New Relic account you want to run commands against. If you have multiple accounts, you can view them with profiles:list: bash Copy $ nr1 profiles:list Notice that one profile is your default profile. This is the account your commands will run against, unless you specify another. To specify a profile for a particular command, use the --profile option: bash Copy $ nr1 create --profile <your profile> If this is your first time using the CLI, then the profile you just added is your default profile. If you have other profiles, you need to set your default to the one you'd like to use for this course: bash Copy $ nr1 profiles:default Tip If you forget these commands, you can look them up in the profiles help menu: bash Copy $ nr1 profiles --help Now, you can exit the Build on New Relic quick start. You’re ready to build an application with the New Relic One CLI! Course This lesson is part of a course that teaches you how to build a New Relic One application from the ground up. Continue on to the next lesson: Create a Nerdpack.",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 96.190414,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install and configure <em>the</em> <em>New</em> <em>Relic</em> One CLI",
        "sections": "Install and configure <em>the</em> <em>New</em> <em>Relic</em> One CLI",
        "info": "Install and configure <em>the</em> <em>New</em> <em>Relic</em> One CLI",
        "body": "Course This lesson is part of a course that teaches you how to build a <em>New</em> <em>Relic</em> One application from the ground up. If you haven&#x27;t already, check out the course <em>introduction</em>. Each lesson in the course builds upon the last, so make sure you&#x27;ve completed the last lesson, Spin up your demo services"
      },
      "id": "6091faf1196a6714b4d52a39"
    },
    {
      "sections": [
        "Introduction to New Relic APIs",
        "Tip",
        "NerdGraph (GraphQL)",
        "REST APIs by capability",
        "Alerts",
        "APM",
        "Browser monitoring",
        "Infrastructure monitoring",
        "Mobile monitoring",
        "Synthetic monitoring",
        "Telemetry APIs for core data types",
        "Account management, admin, and usage APIs",
        "Other APIs",
        "Insights",
        "Plugins",
        "See APIs in action"
      ],
      "title": "Introduction to New Relic APIs",
      "type": "docs",
      "tags": [
        "APIs",
        "Get started",
        "Intro to APIs"
      ],
      "external_id": "01e9799a214baad5de04de6146483f6dbbc198aa",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/intro-apis/introduction-new-relic-apis/",
      "published_at": "2021-05-25T01:49:04Z",
      "updated_at": "2021-05-15T10:43:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers a variety of APIs and SDKs you can use to: Retrieve data from New Relic. Send data to New Relic. Adjust settings. This document provides examples and reference information for our API endpoints. For developer-focused content on how to use and customize New Relic, see developer.newrelic.com. Tip To use APIs and SDKs, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. NerdGraph (GraphQL) NerdGraph is New Relic's GraphQL-format API, an efficient and flexible query language that lets you request exactly the data you need, without over-fetching or under-fetching. NerdGraph is the preferred API for querying New Relic data and making a range of feature configurations. To get started, see Introduction to NerdGraph. REST APIs by capability New Relic capabilities, like APM, infrastructure monitoring, or alerts, are often used together, and sometimes they overlap in functionality. This is why multiple APIs may be relevant to each area. Some API functionality will depend on your access to features and data. Tip To learn more about different API key types, see Understand New Relic API keys. Alerts Use the REST API for alerts and the API Explorer to: Create and manage policies, conditions, and notification channels. Create alert conditions based on NRQL queries. Create alert conditions based on data from other New Relic capabilities. APM API resources for application monitoring include: Resource Details REST API REST API features include: Retrieve APM data, including metrics, Apdex, error rates, and host data. Report deployments. Change the app name in the UI. Agent APIs Every APM language agent has an API that lets you customize the agent's default behavior, including reporting custom data. APM agent APIs include: C SDK API Go agent API Java agent API .NET agent API Node.js agent API PHP agent API Python agent API Ruby agent API Query API To query APM data, use the Query API. Account management APIs For APIs related to accounts and subscription usage, see the account-related APIs. Browser monitoring The Browser API resources include: Resource Details Browser agent API Use the Browser agent API for tasks such as: Report custom end user data to browser monitoring. Monitor asynchronous browser activity using SPA API calls. Insert custom data into New Relic dashboards . Manage source maps. REST API With the REST API you can: Retrieve page load timing data and throughput. Add or list apps monitored by browser monitoring. Manage alerts conditions for your browser data. Query API To retrieve browser monitoring data, use the Query API. Account management APIs For APIs related to accounts and subscription usage, see the account-related APIs. Infrastructure monitoring The Infrastructure API resources include: Resource Details Query API To retrieve infrastructure data, use the Query API. This API can also be used to retrieve subscription usage data. Infrastructure alert API To manage your infrastructure alerts, use the Infrastructure alert API. Integrations SDK To make your own custom integrations for reporting data to infrastructure monitoring, use the Integrations SDK. NerdGraph You can use NerdGraph (our GraphQL API) to query your cloud integration data and make changes to cloud integration settings. Mobile monitoring Mobile API resources include: Resource Details Mobile agent APIs Mobile APIs let you custom instrument your own code and send events to New Relic. See the platform-specific documentation: iOS Android Unity REST API Use the REST API for such tasks as: Retrieve a list of monitored apps. Get subscription usage data. Get metric names and data. Get crash count and crash rate data. Manage New Relic alerts conditions for your mobile apps. Query API To retrieve Mobile data from New Relic, use the Query API. Account management APIs For account-related APIs, see Account APIs. Synthetic monitoring Synthetics API resources include: Resource Details Synthetics REST API The Synthetics REST API functionality includes: Create and manage synthetics monitors. Manage synthetics alert notifications. Add labels to monitors, and retrieve monitors with specific labels. Query API To retrieve synthetics event data, use the Query API. Alerts API To create and manage alert conditions that target synthetics monitors, use the Alerts API. Telemetry APIs for core data types We offer several APIs that allow you to get our core data types (metrics, logs, traces, and events) into New Relic without the use of an installed agent. Data type Description Trace API Send distributed tracing data to New Relic. Event API Send event data to New Relic. Metric API Send metrics to New Relic from any source (including other telemetry monitoring services). Log API Send your log data to New Relic. Account management, admin, and usage APIs Like any other New Relic product or service, you want to be confident that your APIs protect you and your customers' data privacy. The following are API resources related to New Relic account administration and usage. For more information about API capabilities, see the specific New Relic API. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website. Resource Details REST API REST API features include: Find your API keys, account ID, and information needed to use the REST API. Return a list of account users (original user model only). Get SLA report data for browser and application monitoring. Subscription usage You can use the Query API to retrieve subscription usage data. This can be helpful to see how usage compares to your current subscription level, or for doing departmental chargebacks. Partner API To retrieve information about your New Relic partner account, sub-accounts, and users, use the Partner API. Other APIs Insights API resources for Insights include: Resource Details Insert events API To report custom data use the Event insertion API. Query API To query your Insights data using NRQL-formatted queries, use the Query API. This API can also be used to retrieve subscription usage data. Dashboard API To create, read, update, and delete dashboards, use the Dashboard API. Other New Relic product APIs You can also report custom data from other New Relic features. For more information, see the API sections for other products. NerdGraph You can use NerdGraph (our GraphQL API) to query data with NRQL. Plugins Use the REST API for New Relic plugins and the API Explorer to: Get a list of plugins, including their names, IDs, and GUIDs. List one or more plugin components, their output, and their metric timeslice data. Developers and New Relic partners can also use New Relic's Plugin API to write an agent in any language that can work directly with the API for plugins. This allows you to send your own metric data to our plugins and view data received from the API in New Relic. See APIs in action For more on how you as a developer can optimize your ability to solve problems using New Relic, go to developer.newrelic.com.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 77.51916,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Introduction</em> <em>to</em> <em>New</em> <em>Relic</em> APIs",
        "sections": "<em>Introduction</em> <em>to</em> <em>New</em> <em>Relic</em> APIs",
        "tags": "Intro <em>to</em> APIs",
        "body": ", an efficient and flexible query language that lets you request exactly the data you need, without over-fetching or under-fetching. NerdGraph is the preferred API for querying <em>New</em> <em>Relic</em> data and making a range of feature configurations. To get started, see <em>Introduction</em> to NerdGraph. REST APIs"
      },
      "id": "609fa5cf196a67066022b194"
    }
  ],
  "/docs/introduction-new-relic-mobile-unity": [
    {
      "image": "https://docs.newrelic.com/static/futurestack-registration-cf388ce323a82db74de9894bfbe3fb63.png",
      "url": "https://docs.newrelic.com/",
      "sections": [
        "Welcome to New Relic",
        "Create a free account",
        "Start collecting data",
        "Set up alerts",
        "Telemetry Data Platform",
        "Introduction to Telemetry Data Platform",
        "Data explorer",
        "Dashboards",
        "Log management",
        "APIs",
        "Manage data",
        "Build on New Relic One",
        "Full-Stack Observability",
        "Introduction to Full-Stack Observability",
        "APM",
        "Browser",
        "Distributed Tracing",
        "Infrastructure",
        "Logs in context",
        "Mobile",
        "Serverless",
        "Synthetics",
        "Workloads",
        "Alerts and Applied Intelligence (AI)",
        "Introduction to Alerts",
        "Introduction to Applied Intelligence",
        "Incident Intelligence",
        "Incident Workflows",
        "Proactive Detection",
        "New Relic integrations",
        "Back-end, front-end, and mobile applications",
        "Infrastructure and cloud platforms",
        "Open-source monitoring systems",
        "Security, privacy, and legal information",
        "Data privacy",
        "Security compliance",
        "Security bulletins",
        "Licenses"
      ],
      "published_at": "2021-05-26T01:37:37Z",
      "title": "New Relic Documentation",
      "updated_at": "2021-05-23T01:37:22Z",
      "type": "docs",
      "external_id": "807276d5fdd805ba0247a1ce1016af986fb31995",
      "popularity": 1,
      "body": "Welcome to New Relic If you're new, follow these three steps to create an account and get going. (It's free!) Scroll on for more about our Telemetry Data Platform, Full-Stack Observability, and Applied Intelligence. Or, to get a wider view of our platform's capabilities, read the Intro to New Relic, and use our solutions and best practices guides. Create a free account No credit card required. Start collecting data Our UI guides you through setup and install. Set up alerts Get notified quickly about changes in your system. Telemetry Data Platform Ingest, visualize, and alert on all your telemetry data in one place. Introduction to Telemetry Data Platform How to manage all your monitoring in one place. Data explorer Query and build charts with NRQL, our PromQL-style syntax, or our visual chart builder. Dashboards Combine data from anywhere in our platform into customized dashboards. Log management Get logs alongside your telemetry data with our fast, scalable log management. APIs Find APIs to send data in, get data out, or manage the New Relic platform. Manage data Monitor and control your data usage. Build on New Relic One Learn how to build custom apps on our platform at developer.newrelic.com Full-Stack Observability Analyze and troubleshoot problems easily across your entire software stack. Introduction to Full-Stack Observability Get deep insight into everything from infrastructure to server code to end-user apps. APM Get real-time and trending data about your app's performance and stability. Browser Measure website performance, track errors, and see how users interact with your site. Distributed Tracing Track requests through your distributed system to find trends and anomalies. Infrastructure Monitor your infrastructure—hosts, cloud providers, container services, backend services, orchestrators, and more. Logs in context Link your log data to APM agent data so you can pinpoint where things are going wrong. Mobile Understand user journeys in your Android and iOS apps and troubleshoot crashes. Serverless Monitor AWS Lambda, Azure Functions, and Google Cloud Functions. Synthetics Simulate user activity to detect outages and fix poor website performance. Workloads Group or monitor entities across your entire stack based on a team or a set of responsibilities. Alerts and Applied Intelligence (AI) Automatically detect anomalies, correlate issues, and reduce alert noise. Introduction to Alerts Get notified about important changes in your system based on any data you connect to New Relic. Introduction to Applied Intelligence Recognize issues sooner, resolve problems faster, and reduce noise for better incident management. Incident Intelligence Get an overview of all your incidents. See sources and related events, and find out how they all correlate. Incident Workflows Enrich your incidents with New Relic data before sending them to your notification platform. Proactive Detection Get notified by Slack or webhook of unusual app behavior. New Relic integrations Integrations connect the technologies in your stack to New Relic. Here are a few of our 370+ integrations: Back-end, front-end, and mobile applications Android Browser (JavaScript) C SDK Go iOS Java .NET Node.js PHP Python Ruby Synthetics Infrastructure and cloud platforms Apache AWS Azure Google Cloud Kafka Kubernetes Linux Microsoft SQL MongoDB MySQL NGINX PostgreSQL Redis Windows Open-source monitoring systems Dropwizard Istio JMX Kamon Micrometer OpenCensus OpenTelemetry Prometheus StatsD See all 360+ integrations Security, privacy, and legal information Find out how we ensure security, data privacy, and compliance, and find our terms of service. Data privacy New Relic takes your data privacy seriously. Our principles-based approach aims to go beyond the legal requirements for consent. Security compliance Whether your data is in transit to New Relic or at rest in our storage, strong encryption measures can help protect against unauthorized access or theft of valuable data. Security bulletins Security bulletins are a way for us to let you know about security vulnerabilities, remediation strategies, and applicable updates. Licenses All about New Relic's licenses, Usage Plan, and policies. Find out what open source software we use in our products.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 101.22524,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> Documentation",
        "sections": "Welcome <em>to</em> <em>New</em> <em>Relic</em>",
        "body": " alert noise. <em>Introduction</em> to Alerts Get notified about important changes in your system based on any data you connect to <em>New</em> <em>Relic</em>. <em>Introduction</em> to Applied Intelligence Recognize issues sooner, resolve problems faster, and reduce noise for better incident management. Incident Intelligence Get"
      },
      "id": "5be4e5d38e9c0f3585d72edf"
    },
    {
      "sections": [
        "Introduction to New Relic APIs",
        "Tip",
        "NerdGraph (GraphQL)",
        "REST APIs by capability",
        "Alerts",
        "APM",
        "Browser monitoring",
        "Infrastructure monitoring",
        "Mobile monitoring",
        "Synthetic monitoring",
        "Telemetry APIs for core data types",
        "Account management, admin, and usage APIs",
        "Other APIs",
        "Insights",
        "Plugins",
        "See APIs in action"
      ],
      "title": "Introduction to New Relic APIs",
      "type": "docs",
      "tags": [
        "APIs",
        "Get started",
        "Intro to APIs"
      ],
      "external_id": "01e9799a214baad5de04de6146483f6dbbc198aa",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/intro-apis/introduction-new-relic-apis/",
      "published_at": "2021-05-25T01:49:04Z",
      "updated_at": "2021-05-15T10:43:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers a variety of APIs and SDKs you can use to: Retrieve data from New Relic. Send data to New Relic. Adjust settings. This document provides examples and reference information for our API endpoints. For developer-focused content on how to use and customize New Relic, see developer.newrelic.com. Tip To use APIs and SDKs, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. NerdGraph (GraphQL) NerdGraph is New Relic's GraphQL-format API, an efficient and flexible query language that lets you request exactly the data you need, without over-fetching or under-fetching. NerdGraph is the preferred API for querying New Relic data and making a range of feature configurations. To get started, see Introduction to NerdGraph. REST APIs by capability New Relic capabilities, like APM, infrastructure monitoring, or alerts, are often used together, and sometimes they overlap in functionality. This is why multiple APIs may be relevant to each area. Some API functionality will depend on your access to features and data. Tip To learn more about different API key types, see Understand New Relic API keys. Alerts Use the REST API for alerts and the API Explorer to: Create and manage policies, conditions, and notification channels. Create alert conditions based on NRQL queries. Create alert conditions based on data from other New Relic capabilities. APM API resources for application monitoring include: Resource Details REST API REST API features include: Retrieve APM data, including metrics, Apdex, error rates, and host data. Report deployments. Change the app name in the UI. Agent APIs Every APM language agent has an API that lets you customize the agent's default behavior, including reporting custom data. APM agent APIs include: C SDK API Go agent API Java agent API .NET agent API Node.js agent API PHP agent API Python agent API Ruby agent API Query API To query APM data, use the Query API. Account management APIs For APIs related to accounts and subscription usage, see the account-related APIs. Browser monitoring The Browser API resources include: Resource Details Browser agent API Use the Browser agent API for tasks such as: Report custom end user data to browser monitoring. Monitor asynchronous browser activity using SPA API calls. Insert custom data into New Relic dashboards . Manage source maps. REST API With the REST API you can: Retrieve page load timing data and throughput. Add or list apps monitored by browser monitoring. Manage alerts conditions for your browser data. Query API To retrieve browser monitoring data, use the Query API. Account management APIs For APIs related to accounts and subscription usage, see the account-related APIs. Infrastructure monitoring The Infrastructure API resources include: Resource Details Query API To retrieve infrastructure data, use the Query API. This API can also be used to retrieve subscription usage data. Infrastructure alert API To manage your infrastructure alerts, use the Infrastructure alert API. Integrations SDK To make your own custom integrations for reporting data to infrastructure monitoring, use the Integrations SDK. NerdGraph You can use NerdGraph (our GraphQL API) to query your cloud integration data and make changes to cloud integration settings. Mobile monitoring Mobile API resources include: Resource Details Mobile agent APIs Mobile APIs let you custom instrument your own code and send events to New Relic. See the platform-specific documentation: iOS Android Unity REST API Use the REST API for such tasks as: Retrieve a list of monitored apps. Get subscription usage data. Get metric names and data. Get crash count and crash rate data. Manage New Relic alerts conditions for your mobile apps. Query API To retrieve Mobile data from New Relic, use the Query API. Account management APIs For account-related APIs, see Account APIs. Synthetic monitoring Synthetics API resources include: Resource Details Synthetics REST API The Synthetics REST API functionality includes: Create and manage synthetics monitors. Manage synthetics alert notifications. Add labels to monitors, and retrieve monitors with specific labels. Query API To retrieve synthetics event data, use the Query API. Alerts API To create and manage alert conditions that target synthetics monitors, use the Alerts API. Telemetry APIs for core data types We offer several APIs that allow you to get our core data types (metrics, logs, traces, and events) into New Relic without the use of an installed agent. Data type Description Trace API Send distributed tracing data to New Relic. Event API Send event data to New Relic. Metric API Send metrics to New Relic from any source (including other telemetry monitoring services). Log API Send your log data to New Relic. Account management, admin, and usage APIs Like any other New Relic product or service, you want to be confident that your APIs protect you and your customers' data privacy. The following are API resources related to New Relic account administration and usage. For more information about API capabilities, see the specific New Relic API. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website. Resource Details REST API REST API features include: Find your API keys, account ID, and information needed to use the REST API. Return a list of account users (original user model only). Get SLA report data for browser and application monitoring. Subscription usage You can use the Query API to retrieve subscription usage data. This can be helpful to see how usage compares to your current subscription level, or for doing departmental chargebacks. Partner API To retrieve information about your New Relic partner account, sub-accounts, and users, use the Partner API. Other APIs Insights API resources for Insights include: Resource Details Insert events API To report custom data use the Event insertion API. Query API To query your Insights data using NRQL-formatted queries, use the Query API. This API can also be used to retrieve subscription usage data. Dashboard API To create, read, update, and delete dashboards, use the Dashboard API. Other New Relic product APIs You can also report custom data from other New Relic features. For more information, see the API sections for other products. NerdGraph You can use NerdGraph (our GraphQL API) to query data with NRQL. Plugins Use the REST API for New Relic plugins and the API Explorer to: Get a list of plugins, including their names, IDs, and GUIDs. List one or more plugin components, their output, and their metric timeslice data. Developers and New Relic partners can also use New Relic's Plugin API to write an agent in any language that can work directly with the API for plugins. This allows you to send your own metric data to our plugins and view data received from the API in New Relic. See APIs in action For more on how you as a developer can optimize your ability to solve problems using New Relic, go to developer.newrelic.com.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 95.959946,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Introduction</em> <em>to</em> <em>New</em> <em>Relic</em> APIs",
        "sections": "<em>Introduction</em> <em>to</em> <em>New</em> <em>Relic</em> APIs",
        "tags": "Intro <em>to</em> APIs",
        "body": " documentation: iOS Android <em>Unity</em> REST API Use the REST API for such tasks as: Retrieve a list of monitored apps. Get subscription usage data. Get metric names and data. Get crash count and crash rate data. Manage <em>New</em> <em>Relic</em> alerts conditions for your <em>mobile</em> apps. Query API To retrieve <em>Mobile</em> data from"
      },
      "id": "609fa5cf196a67066022b194"
    },
    {
      "sections": [
        "User settings and authentication",
        "User authentication",
        "User settings",
        "Sign in with additional username",
        "Switch between accounts",
        "Remove or re-add a user name"
      ],
      "title": "User settings and authentication",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic Mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "0b40ac4c2e769279d25d0ebb2ea77cebda8d8ea7",
      "image": "https://docs.newrelic.com/static/88ff328efc4a127601923bc728fea229/8c557/device-ipad-switch-user.png",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/authentication-alerts/user-settings-authentication/",
      "published_at": "2021-05-22T17:32:23Z",
      "updated_at": "2021-05-16T06:27:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This explains how to authenticate your New Relic mobile app account, and how to add users to or remove them from your mobile device. User authentication Depending on your New Relic account, additional installation or authentication steps may be required when you install the New Relic mobile app. New Relic account Additional requirements New users If you do not already have a New Relic account: From your desktop web browser, create a New Relic account. Install your application with the appropriate New Relic agent. As part of new account setup, you will receive an email with a password reset link. The password reset link expires after 20 minutes for mobile apps. Existing New Relic users No additional requirements; your applications, hosts, installed plugins, and key transactions automatically appear after you sign in. Users with New Relic partner accounts Depending on the partner, you may need to complete a different authentication process. Azure Store users: Due to the deep integration between Azure Storefront and New Relic, Azure Storefront users cannot access their accounts on the New Relic Android or iOS apps. Users with SAML-SSO enabled accounts When you sign in to the New Relic mobile app, your session automatically redirects to your web browser. From there you can sign in to your New Relic SAML-SSO account. If you see any errors when using SAML-SSO accounts on your mobile device, verify that you are able to sign in to one.newrelic.com with a desktop web browser. If no, contact your administrator. If yes, get support at support.newrelic.com. User settings After you sign in, all New Relic accounts and applications associated with the user appear automatically. Sign in with additional username Follow the procedure for your mobile device. Mobile device To sign in to the app with an additional user name: Android To switch users: Log out from the Android device: Main menu > (selected username) > Logout > Confirm. Log in with a new account. iPhone From the app menu, select your account name, then select the Users menu. From the Users menu, select the plus icon. Sign in with the additional username. iPad To access the Users menu: Select the user icon or slide right. From the Users menu, select the plus icon. Sign in with the additional username. Switch between accounts To switch between accounts associated with your username: From the Users menu, select the user name. Select the account name. Remove or re-add a user name To remove a specific username from this device: From the Users menu, select Logout. To remove a user from this device, select the user's red minus icon. Select the user's Log out icon. To add a user again, sign in with that username again. Select the user icon or slide right to show the New Relic iPad app's Users menu.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 82.93094,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>New</em> <em>Relic</em> <em>Mobile</em> apps",
        "body": "This explains how to authenticate your <em>New</em> <em>Relic</em> <em>mobile</em> app account, and how to add users to or remove them from your <em>mobile</em> device. User authentication Depending on your <em>New</em> <em>Relic</em> account, additional installation or authentication steps may be required when you install the <em>New</em> <em>Relic</em> <em>mobile</em> app"
      },
      "id": "604415a728ccbc8fb52c6068"
    }
  ],
  "/docs/ios-device-id-obfuscation": [
    {
      "sections": [
        "Upgrading New Relic Mobile's tvOS SDK",
        "Tip",
        "Contents",
        "Replacing your tvOS framework",
        "For more help"
      ],
      "title": "Upgrading New Relic Mobile's tvOS SDK",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "New Relic Mobile iOS",
        "tvOS"
      ],
      "external_id": "f5d6d3d356952d185b96aa409605b79e3ace8ec9",
      "image": "https://docs.newrelic.com/static/a3b6801675529d8f4eba123cf08e8f1f/c1b63/Mobile_tvOS_replace-framework_1.png",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-ios/tvos/upgrading-new-relic-mobiles-tvos-sdk/",
      "published_at": "2021-05-21T23:15:09Z",
      "updated_at": "2021-05-16T01:00:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Tip Owner or Admins You must be the account Admin to install, configure, and upgrade New Relic Mobile functionality with your tvOS applications. For information about the latest version, refer to the release notes. Contents Replacing your tvOS framework Admins: You must replace the earlier version of your tvOS agent framework before upgrading to a newer version of the tvOS SDK for New Relic Mobile. Here is an example of the workflow to remove your existing tvOS agent framework so you can replace it with a newer version. From the Project Navigator (CMD 1) in Xcode, search for NewRelicAgentTVOS.framework. Right-click or control-click NewRelicAgentTVOS.framework, and select Show in Finder. Drag NewRelicAgentTVOS.framework to the trash. Verify that the Xcode project highlights the reference to NewRelicAgentTVOS.framework in red. Right-click or control-click NewRelicAgentTVOS.framework, and select Delete to remove the obsolete reference from the project. Follow standard installation procedures for tvOS app monitoring. For more help Additional documentation resources include: tvOS installation and configuration (standard installation and configuration procedures) CocoaPods installation and configuration (procedures specific to CocoaPods) Customizing your mobile app settings (Settings tab options to view your mobile app's application token, rename it, or install New Relic Mobile updates)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.76562,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "New Relic Mobile <em>iOS</em>",
        "body": "Tip Owner or Admins You must be the account Admin to install, configure, and upgrade New Relic Mobile functionality with your tv<em>OS</em> applications. For information about the latest version, refer to the release notes. Contents Replacing your tv<em>OS</em> framework Admins: You must replace the earlier version"
      },
      "id": "604416cc64441f805c378ecf"
    },
    {
      "sections": [
        "tvOS installation and configuration",
        "Tip",
        "Installing your tvOS application",
        "Configuring your tvOS application",
        "Executing a demo crash (optional)",
        "Changing the logging level (optional)",
        "For more help"
      ],
      "title": "tvOS installation and configuration",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "New Relic Mobile iOS",
        "tvOS"
      ],
      "external_id": "04798a275a7591bfbafb5437194cfbab4b33d8e3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-ios/tvos/tvos-installation-configuration/",
      "published_at": "2021-05-21T23:14:02Z",
      "updated_at": "2021-05-16T01:00:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Tip Owner or Admins Follow these instructions to install and configure New Relic Mobile functionality with your tvOS applications. Tip tvOS apps using Cocoapods have separate installation procedures. Installing your tvOS application As part of the installation process, New Relic Mobile automatically generates an application token. This is a 40-character hexadecimal string for authenticating each mobile app you monitor in New Relic Mobile. For Admins with existing New Relic accounts, follow these steps to install and configure your application. (If you do not have a New Relic account, see New Relic Mobile.) Go to one.newrelic.com and click Mobile. If applicable: From the Mobile Apps list, select Add a new app From the Get Started page, select tvOS as the platform for mobile monitoring. Type a name for your mobile app, and select Continue. Continue with the steps to configure New Relic Mobile. To complete the configuration process for a new mobile app later: Go to one.newrelic.com and click Mobile, then select See Instructions next to your mobile app name. To upgrade an existing tvOS installation: Go to one.newrelic.com > Mobile > (selected app) > Settings > Installation. Configuring your tvOS application These procedures to configure your tvOS app also appear on the Get Started page in New Relic. Download and unzip the tvOS SDK for New Relic Mobile. To add the New Relic tvOS Mobile Framework to your Xcode project: Use Finder to drag the NewRelicAgentTVOS.framework folder into your Xcode project, and drop it onto your Project in the Project Navigator window. Follow the prompts to copy items into destination and to create folder references. Add the SystemConfiguration.framework, libc++.tbd, and libz.tbd libraries to your Linker settings. To start the agent: Import the New Relic Mobile Agent header at the top of your prefix.pch. Add + [ NewRelic startWithApplicationToken: < appToken>] to the top of -application:didFinishLaunchingWithOptions: in your AppDelegate.m using the unique application token that is automatically generated. Add a build script to your target's Build Phases and paste the following, replacing PUT_NEW_RELIC_APP_TOKEN_HERE with your application token: SCRIPT=`/usr/bin/find \"${SRCROOT}\" -name newrelic_postbuild.sh | head -n 1` /bin/sh \"${SCRIPT}\" \"PUT_NEW_RELIC_APP_TOKEN_HERE\" Copy Clean and build your app, and then run it in the simulator or other device. Within a few minutes you will begin to see data for your iOS app: Go to one.newrelic.com > Mobile > (selected app). If you don't, see No data appears. Executing a demo crash (optional) If you have trouble getting your app to crash, the New Relic agent provides an API to execute a demo crash. Recommendation: Add one of these lines of code to a button click event handler as applicable: [NewRelic crashNow]; Copy OR [NewRelic crashNow:@\"<reason>\"]; Copy Changing the logging level (optional) Six log levels are available for mobile apps monitoring: none error warning info verbose ALL To increase your logging level in the app, add this method call before calling startWithApplicationToken: [NRLogger setLogLevels:NRLogLevelALL]; Copy For more help Additional documentation resources include: Mobile Apps index (standard menu functions available from your list of mobile applications in New Relic Mobile) Mobile Apps Overview page (a summary of your mobile app's performance) Customizing your mobile app settings (Settings tab options to view your mobile app's application token, rename it, change its alert thresholds, or install New Relic Mobile updates) Upgrading New Relic Mobile for tvOS apps (standard upgrade procedures) No data appears (basic troubleshooting steps)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.76562,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "tvOS <em>installation</em> and configuration",
        "sections": "tvOS <em>installation</em> and configuration",
        "tags": "New Relic Mobile <em>iOS</em>",
        "body": " or other <em>device</em>. Within a few minutes you will begin to see data for your <em>iOS</em> app: Go to one.newrelic.com &gt; Mobile &gt; (selected app). If you don&#x27;t, see No data appears. Executing a demo crash (optional) If you have trouble getting your app to crash, the New Relic agent provides an API to execute a demo"
      },
      "id": "60441ac5e7b9d26bb55799b6"
    },
    {
      "sections": [
        "Upgrade New Relic Mobile's iOS SDK to v4",
        "Getting started",
        "Update the agent"
      ],
      "title": "Upgrade New Relic Mobile's iOS SDK to v4",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "New Relic Mobile iOS",
        "Installation"
      ],
      "external_id": "fc588d45a0cb0a8ef32363a12e36f10cb4bd5a41",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-ios/installation/upgrade-new-relic-mobiles-ios-sdk-v4/",
      "published_at": "2021-05-21T23:44:15Z",
      "updated_at": "2021-05-16T00:56:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the fourth major version of the iOS agent, New Relic Mobile includes crash reporting. This new feature changes the standard installation procedures, including: A post build script is required to upload dSYMs so that crash reports can be properly interpreted. An additional framework (libc++.dylib) must be added to projects including New Relic. Getting started Before upgrading to agent version 4, you must identify which agent version your app currently includes by enabling logging in the New Relic agent. The version number appears in each New Relic log message. This example shows major version 4, build 73: 2014-10-01 14:04:26.554 MobileDemo [12345:6789012] NewRelic (4.73, 0x7fabc98760d0) : NRMAHarvester m:448 -NRMA ConfigureFromCollector:] ... Copy Update the agent After verifying and upgrading your agent version, follow standard iOS installation and configuration procedures, or go to one.newrelic.com and click Add more data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 251.17807,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Upgrade New Relic Mobile&#x27;s <em>iOS</em> SDK to v4",
        "sections": "Upgrade New Relic Mobile&#x27;s <em>iOS</em> SDK to v4",
        "tags": "New Relic Mobile <em>iOS</em>",
        "body": "With the fourth major version of the <em>iOS</em> agent, New Relic Mobile includes crash reporting. This new feature changes the standard installation procedures, including: A post build script is required to upload dSYMs so that crash reports can be properly interpreted. An additional framework (libc"
      },
      "id": "603ed6e5196a67e9a7a83df9"
    }
  ],
  "/docs/licenses/index": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-05-22T17:22:20Z",
      "updated_at": "2021-05-22T17:22:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 77.69547,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Licenses</em>",
        "body": " information about using New Relic features, visit New Relic University and newrelic.com&#x2F;resources. For more help Recommendations for learning more: See the Docs site&#x27;s landing page for <em>Licenses</em> documentation. Browse New Relic&#x27;s Explorers Hub for community discussions about New Relic <em>licenses</em>."
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-05-22T09:26:45Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 76.58589,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 73.374756,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Licenses</em>"
      },
      "id": "6044e6e528ccbc26f22c6084"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-05-22T11:00:26Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.20895,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.34784,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-05-22T17:24:07Z",
      "updated_at": "2021-05-22T17:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into the Insights event database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.34229,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ebacc64441f77774e8872"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-05-22T11:00:26Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.20894,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.34775,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-05-22T17:24:07Z",
      "updated_at": "2021-05-22T17:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into the Insights event database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.34221,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ebacc64441f77774e8872"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-05-22T11:00:26Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.20894,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.34775,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-05-22T17:24:07Z",
      "updated_at": "2021-05-22T17:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into the Insights event database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.34221,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ebacc64441f77774e8872"
    }
  ],
  "/docs/licenses/license-information/faq/new-relic-one-pricing-plan-frequently-asked-questions": [
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.34766,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-05-22T17:24:07Z",
      "updated_at": "2021-05-22T17:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into the Insights event database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.34212,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ebacc64441f77774e8872"
    },
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-05-22T17:22:20Z",
      "updated_at": "2021-05-22T17:22:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.03622,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " <em>information</em> about using New Relic features, visit New Relic University and newrelic.com&#x2F;resources. For more help Recommendations for learning more: See the Docs site&#x27;s landing page for <em>Licenses</em> documentation. Browse New Relic&#x27;s Explorers Hub for community discussions about New Relic <em>licenses</em>."
      },
      "id": "603ea419e7b9d27b942a07b4"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-05-22T17:22:20Z",
      "updated_at": "2021-05-22T17:22:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 352.32336,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-05-22T17:24:08Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.16464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-05-22T12:00:01Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.15976,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings": [
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-05-22T17:24:08Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.16463,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Acceptable use policy",
        "You will not, and not to allow third parties, in connection with your use of the New Relic Properties to:",
        "Harm New Relic’s Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2021-05-22T12:00:01Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic’s mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (“AUP”). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the “New Relic Properties”) and is part of the New Relic documentation, which can be found at https://docs.newrelic.com (the “Documentation”). You will not, and not to allow third parties, in connection with your use of the New Relic Properties to: Harm New Relic’s Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relic’s or a third party’s intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using “botnets”, “robots,” “spiders” and “offline readers”); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other “hidden text”; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including “scraping” or using any data mining methods); Sharing your New Relic Properties account or login credentials with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other user’s account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by “spoofing” or “phishing”); Collecting or harvesting any personally identifiable information, including account names, from any other user’s account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other people’s private and confidential information without their express permission; Using anyone’s name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (“HIPAA”); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any “personal information” as such term is defined under the Children’s Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, driver’s license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; or (vii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R. §§ 120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. “Third-Party Services” means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if you’re a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers’ compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term “content” means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, “you” may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and “we” means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.15976,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Social Security numbers, driver’s <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-05-22T12:00:01Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.15976,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/government-addendum": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-05-22T17:22:20Z",
      "updated_at": "2021-05-22T17:22:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 352.32318,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-05-22T17:24:08Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.16463,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Acceptable use policy",
        "You will not, and not to allow third parties, in connection with your use of the New Relic Properties to:",
        "Harm New Relic’s Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2021-05-22T12:00:01Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic’s mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (“AUP”). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the “New Relic Properties”) and is part of the New Relic documentation, which can be found at https://docs.newrelic.com (the “Documentation”). You will not, and not to allow third parties, in connection with your use of the New Relic Properties to: Harm New Relic’s Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relic’s or a third party’s intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using “botnets”, “robots,” “spiders” and “offline readers”); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other “hidden text”; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including “scraping” or using any data mining methods); Sharing your New Relic Properties account or login credentials with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other user’s account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by “spoofing” or “phishing”); Collecting or harvesting any personally identifiable information, including account names, from any other user’s account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other people’s private and confidential information without their express permission; Using anyone’s name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (“HIPAA”); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any “personal information” as such term is defined under the Children’s Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, driver’s license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; or (vii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R. §§ 120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. “Third-Party Services” means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if you’re a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers’ compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term “content” means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, “you” may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and “we” means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.15976,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Social Security numbers, driver’s <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-05-22T17:22:20Z",
      "updated_at": "2021-05-22T17:22:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 352.32303,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-05-22T17:24:08Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.16463,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Acceptable use policy",
        "You will not, and not to allow third parties, in connection with your use of the New Relic Properties to:",
        "Harm New Relic’s Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2021-05-22T12:00:01Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic’s mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (“AUP”). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the “New Relic Properties”) and is part of the New Relic documentation, which can be found at https://docs.newrelic.com (the “Documentation”). You will not, and not to allow third parties, in connection with your use of the New Relic Properties to: Harm New Relic’s Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relic’s or a third party’s intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using “botnets”, “robots,” “spiders” and “offline readers”); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other “hidden text”; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including “scraping” or using any data mining methods); Sharing your New Relic Properties account or login credentials with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other user’s account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by “spoofing” or “phishing”); Collecting or harvesting any personally identifiable information, including account names, from any other user’s account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other people’s private and confidential information without their express permission; Using anyone’s name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (“HIPAA”); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any “personal information” as such term is defined under the Children’s Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, driver’s license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; or (vii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R. §§ 120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. “Third-Party Services” means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if you’re a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers’ compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term “content” means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, “you” may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and “we” means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.15974,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Social Security numbers, driver’s <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/new-relics-provision-services": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-05-22T17:22:20Z",
      "updated_at": "2021-05-22T17:22:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 352.32303,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-05-22T17:24:08Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.16463,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Acceptable use policy",
        "You will not, and not to allow third parties, in connection with your use of the New Relic Properties to:",
        "Harm New Relic’s Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2021-05-22T12:00:01Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic’s mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (“AUP”). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the “New Relic Properties”) and is part of the New Relic documentation, which can be found at https://docs.newrelic.com (the “Documentation”). You will not, and not to allow third parties, in connection with your use of the New Relic Properties to: Harm New Relic’s Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relic’s or a third party’s intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using “botnets”, “robots,” “spiders” and “offline readers”); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other “hidden text”; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including “scraping” or using any data mining methods); Sharing your New Relic Properties account or login credentials with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other user’s account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by “spoofing” or “phishing”); Collecting or harvesting any personally identifiable information, including account names, from any other user’s account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other people’s private and confidential information without their express permission; Using anyone’s name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (“HIPAA”); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any “personal information” as such term is defined under the Children’s Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, driver’s license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; or (vii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R. §§ 120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. “Third-Party Services” means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if you’re a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers’ compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term “content” means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, “you” may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and “we” means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.15974,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Social Security numbers, driver’s <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/peoples-republic-china": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-05-22T17:22:20Z",
      "updated_at": "2021-05-22T17:22:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 352.32288,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "Acceptable use policy",
        "You will not, and not to allow third parties, in connection with your use of the New Relic Properties to:",
        "Harm New Relic’s Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2021-05-22T12:00:01Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic’s mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (“AUP”). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the “New Relic Properties”) and is part of the New Relic documentation, which can be found at https://docs.newrelic.com (the “Documentation”). You will not, and not to allow third parties, in connection with your use of the New Relic Properties to: Harm New Relic’s Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relic’s or a third party’s intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using “botnets”, “robots,” “spiders” and “offline readers”); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other “hidden text”; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including “scraping” or using any data mining methods); Sharing your New Relic Properties account or login credentials with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other user’s account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by “spoofing” or “phishing”); Collecting or harvesting any personally identifiable information, including account names, from any other user’s account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other people’s private and confidential information without their express permission; Using anyone’s name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (“HIPAA”); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any “personal information” as such term is defined under the Children’s Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, driver’s license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; or (vii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R. §§ 120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. “Third-Party Services” means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if you’re a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers’ compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term “content” means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, “you” may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and “we” means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.15974,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Social Security numbers, driver’s <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-05-22T12:00:01Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.15974,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    }
  ],
  "/docs/licenses/license-information/other-licenses/services-licenses": [
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.34738,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-05-22T17:24:07Z",
      "updated_at": "2021-05-22T17:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into the Insights event database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.34186,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ebacc64441f77774e8872"
    },
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2021-05-22T17:22:20Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic\"), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS “Add-on” means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the “Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.21034,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    }
  ],
  "/docs/licenses/license-information/product-definitions/legacy-product-definitions": [
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 336.26508,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One pricing: <em>Definitions</em>",
        "sections": "New Relic One pricing: <em>Definitions</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " pricing, invoicing related <em>information</em>, and <em>product</em>-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing."
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-05-22T17:22:20Z",
      "updated_at": "2021-05-22T17:22:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.03587,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Product</em> training",
        "tags": "<em>License</em> <em>information</em>",
        "body": " <em>information</em> about using New Relic features, visit New Relic University and newrelic.com&#x2F;resources. For more help Recommendations for learning more: See the Docs site&#x27;s landing page for <em>Licenses</em> documentation. Browse New Relic&#x27;s Explorers Hub for community discussions about New Relic <em>licenses</em>."
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2021-05-22T14:07:26Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing plan. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the “Product Usage Ratio”) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Product’s list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 138.1319,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "sections": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "tags": "<em>License</em> <em>information</em>",
        "body": " will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more <em>information</em> about units of measures, see <em>Product</em> <em>definitions</em>. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units"
      },
      "id": "603ea32a28ccbc7e22eba768"
    }
  ],
  "/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions": [
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-05-22T17:24:07Z",
      "updated_at": "2021-05-22T17:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into the Insights event database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 336.25497,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Original <em>product</em>-based pricing <em>definitions</em>",
        "sections": "Original <em>product</em>-based pricing <em>definitions</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": "This is a glossary of terms that appear in contracts for our original <em>product</em>-based pricing. For New Relic One pricing plan terms, see New Relic One pricing <em>definitions</em>. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app"
      },
      "id": "603ebacc64441f77774e8872"
    },
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-05-22T17:22:20Z",
      "updated_at": "2021-05-22T17:22:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.03587,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Product</em> training",
        "tags": "<em>License</em> <em>information</em>",
        "body": " <em>information</em> about using New Relic features, visit New Relic University and newrelic.com&#x2F;resources. For more help Recommendations for learning more: See the Docs site&#x27;s landing page for <em>Licenses</em> documentation. Browse New Relic&#x27;s Explorers Hub for community discussions about New Relic <em>licenses</em>."
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2021-05-22T14:07:26Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing plan. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the “Product Usage Ratio”) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Product’s list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 138.1319,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "sections": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "tags": "<em>License</em> <em>information</em>",
        "body": " will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more <em>information</em> about units of measures, see <em>Product</em> <em>definitions</em>. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units"
      },
      "id": "603ea32a28ccbc7e22eba768"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Data Security",
        "2. Data Access",
        "3. Server Security",
        "4. Network Security",
        "5. Security Audits",
        "6. Security and Incident Response",
        "7. Disaster Recovery",
        "8. Copies and Removal",
        "9. Disclosure by Law"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Data Security 1.1. New Relic shall establish and maintain data security procedures and other safeguards designed to protect against the loss, theft or other unauthorized access or alteration of Customer Data in the possession or under the control of New Relic or to which New Relic has access, which are no less rigorous than accepted security standards in the industry. 1.2. New Relic shall maintain an information security policy that outlines a definition of information security and its overall objectives; a framework for setting control objectives and controls, including the structure of risk assessment and risk management; a brief explanation of the compliance requirements, and procedures for managing information security incidents. 2. Data Access 2.1. Access to Customer Data stored on New Relic’s systems shall not be granted to members of New Relic unless they have been uniquely identified and have sufficient credentials. 2.2. Access permissions shall be established in a manner that allows for the minimum access level(s) required for each employee. 2.3. Access to Customer Data shall be logged with sufficient information to determine the nature and scope of any inappropriate access. 3. Server Security 3.1. New Relic shall establish and follow reasonable server configuration guidelines and processes to prevent unauthorized access to Customer Data. 3.2. New Relic shall establish and follow reasonable configuration change management procedures for its servers containing Customer Data. 4. Network Security 4.1. New Relic network architecture shall be designed to limit site access and restrict the availability of information services that are considered to be vulnerable to attack. 4.2. New Relic shall utilize SSL certificates for all Internet activity. By default, Customer Data transmitted to and from the New Relic network shall be sent over encrypted medium or an encrypted format. 4.3. New Relic network shall use IDS technologies for network intrusion detection. 4.4. Access to New Relic systems containing Customer Data shall be restricted to authorized personnel. 5. Security Audits 5.1. New Relic shall conduct at least annually a SOC 2 or industry equivalent audit. New Relic shall provide to Customer audit results upon request, and shall explain and provide remediation plans to correct any problems to the extent reasonably possible. 6. Security and Incident Response 6.1. New Relic shall maintain an Information Security Incident Response plan, and make that plan available to Customer if requested. 6.2. In the event of an actual theft, loss, or unauthorized access of Customer Data by New Relic’s personnel and/or any unauthorized individual or entity, New Relic shall: (a) investigate such breach, (b) attempt to cure such breach, and (c) provide notification to Customer that describes such breach. 7. Disaster Recovery 7.1. New Relic shall have in effect a disaster recovery plan designed to respond to both a component failure of New Relic equipment within its data center and a catastrophic loss of service. This plan shall include documented policies and procedures to restore service in the event of either type of failure. 7.2. New Relic shall establish and follow backup and restore procedures for servers containing Customer Data. 8. Copies and Removal 8.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of this Agreement for any reason: (a) New Relic shall, and shall cause its personnel, to cease and desist all access and use of any Customer Data, (b) New Relic shall delete all copies of Customer Data within ninety (90) days. 9. Disclosure by Law 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.35419,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Relic has access, which are no less rigorous than accepted security standards in the industry. 1.2. New Relic shall maintain an <em>information</em> security policy that outlines a definition of <em>information</em> security and its overall objectives; a framework for setting control objectives and controls"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2021-05-22T14:04:30Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to New Relic Full Stack Observability Pro or Enterprise Products, the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to New Relic Full Stack Observability Standard, any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customer’s access or use of the Service in accordance with this Agreement. This describes Customer’s sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.32716,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.10114,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/security-policy": [
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2021-05-22T14:04:30Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to New Relic Full Stack Observability Pro or Enterprise Products, the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to New Relic Full Stack Observability Standard, any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customer’s access or use of the Service in accordance with this Agreement. This describes Customer’s sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.32716,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (“New Relic”) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the “Policy”) governing your use and participation in New Relic’s program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the “Pre-Release Service(s)”). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as “you” or “your”) as of your date of first use of the Pre-Release Service(s) (the “Effective Date”), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated ‘Documentation’ page of the New Relic website (the “Documentation”) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (“Feedback”). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (“Confidential Information”). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., “personal information” of children as defined by the Children’s Online Privacy Protection Act, “protected health information” as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, “special categories of data” as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relic’s request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively “Unintended Effects”); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED “AS IS.” TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorney’s fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.33366,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.10114,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/service-level-availability-commitment": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Data Security",
        "2. Data Access",
        "3. Server Security",
        "4. Network Security",
        "5. Security Audits",
        "6. Security and Incident Response",
        "7. Disaster Recovery",
        "8. Copies and Removal",
        "9. Disclosure by Law"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Data Security 1.1. New Relic shall establish and maintain data security procedures and other safeguards designed to protect against the loss, theft or other unauthorized access or alteration of Customer Data in the possession or under the control of New Relic or to which New Relic has access, which are no less rigorous than accepted security standards in the industry. 1.2. New Relic shall maintain an information security policy that outlines a definition of information security and its overall objectives; a framework for setting control objectives and controls, including the structure of risk assessment and risk management; a brief explanation of the compliance requirements, and procedures for managing information security incidents. 2. Data Access 2.1. Access to Customer Data stored on New Relic’s systems shall not be granted to members of New Relic unless they have been uniquely identified and have sufficient credentials. 2.2. Access permissions shall be established in a manner that allows for the minimum access level(s) required for each employee. 2.3. Access to Customer Data shall be logged with sufficient information to determine the nature and scope of any inappropriate access. 3. Server Security 3.1. New Relic shall establish and follow reasonable server configuration guidelines and processes to prevent unauthorized access to Customer Data. 3.2. New Relic shall establish and follow reasonable configuration change management procedures for its servers containing Customer Data. 4. Network Security 4.1. New Relic network architecture shall be designed to limit site access and restrict the availability of information services that are considered to be vulnerable to attack. 4.2. New Relic shall utilize SSL certificates for all Internet activity. By default, Customer Data transmitted to and from the New Relic network shall be sent over encrypted medium or an encrypted format. 4.3. New Relic network shall use IDS technologies for network intrusion detection. 4.4. Access to New Relic systems containing Customer Data shall be restricted to authorized personnel. 5. Security Audits 5.1. New Relic shall conduct at least annually a SOC 2 or industry equivalent audit. New Relic shall provide to Customer audit results upon request, and shall explain and provide remediation plans to correct any problems to the extent reasonably possible. 6. Security and Incident Response 6.1. New Relic shall maintain an Information Security Incident Response plan, and make that plan available to Customer if requested. 6.2. In the event of an actual theft, loss, or unauthorized access of Customer Data by New Relic’s personnel and/or any unauthorized individual or entity, New Relic shall: (a) investigate such breach, (b) attempt to cure such breach, and (c) provide notification to Customer that describes such breach. 7. Disaster Recovery 7.1. New Relic shall have in effect a disaster recovery plan designed to respond to both a component failure of New Relic equipment within its data center and a catastrophic loss of service. This plan shall include documented policies and procedures to restore service in the event of either type of failure. 7.2. New Relic shall establish and follow backup and restore procedures for servers containing Customer Data. 8. Copies and Removal 8.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of this Agreement for any reason: (a) New Relic shall, and shall cause its personnel, to cease and desist all access and use of any Customer Data, (b) New Relic shall delete all copies of Customer Data within ninety (90) days. 9. Disclosure by Law 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.35417,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Relic has access, which are no less rigorous than accepted security standards in the industry. 1.2. New Relic shall maintain an <em>information</em> security policy that outlines a definition of <em>information</em> security and its overall objectives; a framework for setting control objectives and controls"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (“New Relic”) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the “Policy”) governing your use and participation in New Relic’s program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the “Pre-Release Service(s)”). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as “you” or “your”) as of your date of first use of the Pre-Release Service(s) (the “Effective Date”), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated ‘Documentation’ page of the New Relic website (the “Documentation”) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (“Feedback”). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (“Confidential Information”). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., “personal information” of children as defined by the Children’s Online Privacy Protection Act, “protected health information” as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, “special categories of data” as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relic’s request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively “Unintended Effects”); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED “AS IS.” TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorney’s fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.33366,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.10104,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/data-collector-licenses": [
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-05-22T14:06:22Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.72232,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-05-22T14:05:25Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.71584,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-05-22T14:05:25Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (“Terms”) set forth the terms and conditions under which you (“Customer”) may use New Relic Diagnostics (the “Software”), as made available by New Relic, Inc. (“New Relic”). By clicking “accept” or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then “Customer” means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customer’s use of New Relic products to which Customer has a separate subscription (“New Relic Products”). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. “Usage Data” means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered “Customer Data”, “your Data” or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customer’s election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMER’S OWN RISK. SOFTWARE IS PROVIDED “AS IS,” WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELIC’S ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customer’s access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customer’s written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Department’s Table of Denial Orders or U.S. Treasury Department’s list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relic’s prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customer’s consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.78127,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-05-22T14:05:25Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.74387,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-05-22T14:06:22Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.72232,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-05-22T14:05:25Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.71584,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-premium-support": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-05-22T14:05:25Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.74387,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-05-22T14:06:22Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.72232,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-05-22T14:05:25Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (“Terms”) set forth the terms and conditions under which you (“Customer”) may use New Relic Diagnostics (the “Software”), as made available by New Relic, Inc. (“New Relic”). By clicking “accept” or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then “Customer” means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customer’s use of New Relic products to which Customer has a separate subscription (“New Relic Products”). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. “Usage Data” means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered “Customer Data”, “your Data” or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customer’s election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMER’S OWN RISK. SOFTWARE IS PROVIDED “AS IS,” WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELIC’S ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customer’s access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customer’s written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Department’s Table of Denial Orders or U.S. Treasury Department’s list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relic’s prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customer’s consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.78127,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-priority-support": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-05-22T14:05:25Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.74387,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-05-22T14:05:25Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.71584,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-05-22T14:05:25Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (“Terms”) set forth the terms and conditions under which you (“Customer”) may use New Relic Diagnostics (the “Software”), as made available by New Relic, Inc. (“New Relic”). By clicking “accept” or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then “Customer” means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customer’s use of New Relic products to which Customer has a separate subscription (“New Relic Products”). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. “Usage Data” means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered “Customer Data”, “your Data” or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customer’s election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMER’S OWN RISK. SOFTWARE IS PROVIDED “AS IS,” WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELIC’S ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customer’s access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customer’s written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Department’s Table of Denial Orders or U.S. Treasury Department’s list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relic’s prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customer’s consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.78127,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/usage-plans/new-relic-one-usage-plan-descriptions": [
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2021-05-22T14:07:26Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing plan. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the “Product Usage Ratio”) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Product’s list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.59657,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Product-based pricing <em>usage</em> and New Relic Platform Pricing <em>Usage</em> <em>Plan</em>",
        "sections": "Product-based pricing <em>usage</em> and New Relic Platform Pricing <em>Usage</em> <em>Plan</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " reference the <em>usage</em> <em>plans</em> set forth below or where a subscription has indeterminate product pricing or <em>usage</em> quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. <em>Usage</em>"
      },
      "id": "603ea32a28ccbc7e22eba768"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.34694,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Usage</em> <em>Plan</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https:&#x2F;&#x2F;newrelic.com&#x2F;termsandconditions&#x2F;unpaid. <em>Usage</em> <em>Plan</em> <em>Usage</em> <em>Plan</em> refers to the Service or Product"
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-05-22T17:24:07Z",
      "updated_at": "2021-05-22T17:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into the Insights event database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.3414,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing <em>plan</em> terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app"
      },
      "id": "603ebacc64441f77774e8872"
    }
  ],
  "/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan": [
    {
      "sections": [
        "New Relic One Usage plan descriptions",
        "Pay As You Go",
        "Annual Pool of Funds",
        "Applicable Invoicing and Order Terms",
        "User Accounts",
        "New Relic One Pro and Enterprise Service Level Availability Commitment",
        "New Relic One Pro and Enterprise Support Plans",
        "Free tier, ‘lite’, no-charge, preview access, New Relic One - Data, New Relic One - Standard User subscriptions"
      ],
      "title": "New Relic One Usage plan descriptions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "c18e1c6c294914c28bba48f9de025333210ed254",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/new-relic-one-usage-plan-descriptions/",
      "published_at": "2021-05-22T14:06:22Z",
      "updated_at": "2021-03-13T04:19:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Usage Plan applies to (i) your Pay As You Go subscription, or (ii) your Commitment Term for the Annual Pool of Funds subscription. New Relic may modify the Usage Plan from time to time. Any changes to the Usage Plan will become effective immediately for changes that provide a benefit or right to the Customer, all other changes will become effective if Customer assents or upon any new or renewal Commitment Term. Usage Plan - Effective as February 19, 2021: The Order and Usage Plan may contain defined terms that are denoted by capitalization. In the event that a capitalized term is not defined in either the Order or the Usage Plan, such terms shall have the meaning set forth in the New Relic One pricing definitions page. Pay As You Go By electing and subscribing to the Pay As You Go subscription model (“Pay As You Go” or “PAYG”), Customer commits to paying for the New Relic Products on a month-to-month consumption basis. Monthly Product Usage will be invoiced regardless if a PO is required or not. Any Customer dispute to Monthly Product Usage from the prior month must be in good-faith and received by New Relic in writing within three (3) business days of the start of the next month or such dispute notice will be considered invalid. The dispute notice will set forth in reasonable detail the information concerning the disputed charges. The parties will use good-faith efforts to promptly resolve any disputed charges. Customer’s usage of the Products in excess of the Free Tier each month shall be billed in arrears on the first business day of the following month based on the Customer’s Per Unit usage of each Product each month multiplied by the corresponding rates set forth in an Order and summed (“Monthly Product Usage”). Annual Pool of Funds By electing and subscribing to the Annual Pool of Funds subscription model (“Annual Pool of Funds” or “APoF”) for the Commitment Term, Customer commits in an Order to: (i) paying the Commitment Fee amounts described in the Subscription table and any additional commitment fees set forth; and (ii) the Monthly Discounted Fee Rates applying to Customer’s Monthly Product Usage. New Relic will invoice the Commitment Fee as per the ‘Billing Terms’ described in an Order. On a monthly cadence during the Commitment Term, Customer’s Per Unit usage of the Products will be multiplied by the corresponding Monthly Discounted Rate and summed (“Monthly Product Usage”). Monthly Product Usage will be deducted from the Commitment Fee amounts that are paid in advance. If Monthly Product Usage exceeds any such remaining unconsumed amounts, Customer will be invoiced for the difference (“Additional Usage”), including for any Monthly Product Usage during the last month of the Commitment Term. Payment of such invoices will be governed as set forth in the Terms. Any Customer dispute to Monthly Product Usage from the prior month must be in good-faith and received by New Relic in writing within three (3) business days of the start of the next month (including for any Monthly Product Usage for the last month of the Commitment Term) or such dispute notice will be considered invalid. The dispute notice will set forth in reasonable detail the information concerning the disputed charges. The parties will use good-faith efforts to promptly resolve any disputed charges. Any unconsumed balances from the Customer payment of each annual Commitment Fee and any additional payments, if applicable, as set out in an Order will expire and lapse at the end of each year of the Commitment Term. Applicable Invoicing and Order Terms All amounts stated in an Order are non-cancelable payment obligations of the Customer for the Commitment Term regardless of usage. Any fees paid are non-refundable and do not represent a deposit for, or a credit towards, the purchase of other products not specified in an Order or for any purchase after the Commitment Term. Customer acknowledges that a final payment for the full outstanding amount of any remaining unpaid Commitment Fee and/or Monthly Product Usage Fee may be invoiced upon each anniversary date of the term start date. Tax will be added where applicable. New Relic may review Customer's use of the Products at any time. If New Relic identifies any Customer usage of the Product(s) that is not in accordance with the Terms or Documentation, New Relic may suspend such unauthorized usage. All existing purchases and related pricing in effect prior to the execution of an Order shall remain in force. Unless otherwise stated in an Order, an Order does not modify or amend any existing purchases. Additional future products or quantities are not subject to promotional pricing unless otherwise stated in an Order. In the event that a Customer indicates in an Order that it requires a purchase order (“PO”) for its subscription, Customer agrees to provide the required PO prior to the provisioning of the Products. If a Customer does not indicate a PO is required, Customer agrees that New Relic may issue invoice(s) and is entitled to such payment without a PO reference. All Additional Usage fees will be invoiced regardless of a PO requirement. The Product(s) are deemed accepted upon its provisioning. User Accounts Use of the Products require Customer users to create Login Credentials. Customer user Login Credential information must be accurate, current, and complete. New Relic’s use and collection of Login Credentials (in accordance with its General Data Privacy Notice) is for account and product management and support of its customers. Customer and Customer users must abide by the New Relic Acceptable Use Policy (AUP) and Login Credentials may not be shared. Each Customer user must have their own user account. Entry into an Order indicates your agreement that the amount of provisioned users (at the rate specified in the Order) applies in lieu of and supersedes any other amount of users of the Products that may be specified in the agreement between Customer and New Relic. New Relic One Pro and Enterprise Service Level Availability Commitment With a subscription to New Relic Full Stack Observability Pro or Enterprise Products, you agree that during the Commitment Term the applicable service level availability commitment set forth on the ‘Service level availability commitment’ page in the Documentation shall apply to the Products. For clarity, if your agreement with New Relic contains a different service level availability commitment or remedies, the above does not apply to your subscription to the New Relic Full Stack Observability Pro or Enterprise Products. If you subscribe to any other New Relic Products with New Relic One pricing, any service level availability commitment or related remedies contained within your agreement with New Relic are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. New Relic One Pro and Enterprise Support Plans With a subscription to New Relic One Users (Full Stack Observability), New Relic provides an updated support plan commitment. By subscribing to New Relic One Users (Full Stack Observability), you agree that during the Commitment Term the applicable Support Plan set forth on the ‘Support plan’ page in the Documentation shall apply in lieu of, and supersedes and replaces, any other support related commitments that may be contained within your agreement with New Relic. For New Relic K.K. customers (Japan), the above does not currently apply to the support offerings provided by New Relic to you. Free tier, ‘lite’, no-charge, preview access, New Relic One - Data, New Relic One - Standard User subscriptions If you are using New Relic’s Products in the free tier only, or on a no-charge, or a ‘lite’ or ‘preview access’ basis you agree that the Unpaid Terms of Service will apply to such Product usage and replace and supersede any other terms. In addition, if your subscription contains the New Relic One - Standard User (Full Stack Observability Standard) Product, you agree that the Paid Terms of Service will apply to your subscription to the Products set forth in an Order and replace and supersede any other terms.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.70747,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One <em>Usage</em> <em>plan</em> descriptions",
        "sections": "New Relic One <em>Usage</em> <em>plan</em> descriptions",
        "tags": "<em>License</em> <em>information</em>",
        "body": "The <em>Usage</em> <em>Plan</em> applies to (i) your Pay As You Go subscription, or (ii) your Commitment Term for the Annual Pool of Funds subscription. New Relic may modify the <em>Usage</em> <em>Plan</em> from time to time. Any changes to the <em>Usage</em> <em>Plan</em> will become effective immediately for changes that provide a benefit or right"
      },
      "id": "6044e74ee7b9d2a4515799c8"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-05-22T17:25:09Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.34694,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Usage</em> <em>Plan</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https:&#x2F;&#x2F;newrelic.com&#x2F;termsandconditions&#x2F;unpaid. <em>Usage</em> <em>Plan</em> <em>Usage</em> <em>Plan</em> refers to the Service or Product"
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-05-22T17:24:07Z",
      "updated_at": "2021-05-22T17:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into the Insights event database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.3414,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing <em>plan</em> terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app"
      },
      "id": "603ebacc64441f77774e8872"
    }
  ],
  "/docs/licenses/product-or-service-licenses/miscellaneous/help-center-documentation-licenses": [
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-05-22T09:26:45Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.09427,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "Python agent licenses"
      ],
      "title": "Python agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "6949c60ba3a446b93561658282def2bb7a8721c3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/python-agent-licenses/",
      "published_at": "2021-05-22T09:07:28Z",
      "updated_at": "2021-05-05T16:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Python Agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library Copyright License asgiref Copyright © Django Software Foundation and individual contributors. The BSD 3-Clause License six Copyright © 2010-2013 Benjamin Peterson The MIT License time.monotonic Copyright © 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013 Python Software Foundation; All Rights Reserved Python Software Foundation urllib3 Copyright © 2008-2019 Andrey Petrov and contributors (see CONTRIBUTORS.txt) The MIT License wrapt Copyright © 2013-2019, Graham Dumpleton All rights reserved. The BSD 2-Clause License The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.69603,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Python agent <em>licenses</em>",
        "sections": "Python agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Python Agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library Copyright License"
      },
      "id": "6044e7bbe7b9d242575799d1"
    },
    {
      "sections": [
        "Android application licenses"
      ],
      "title": "Android application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "68c9bdc9dec6f02240f002494309519e41619f29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/android-application-licenses/",
      "published_at": "2021-05-22T14:08:19Z",
      "updated_at": "2021-05-05T16:29:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Android app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Third Party Dependencies License Copyright Android-FlowLayout Apache 2.0 Copyright © 2011, Artem Votincev (apmem.org) AVLoadingIndicatorView Apache 2.0 Copyright © 2015, Jack Wang BottomNavigationViewEx MIT Copyright © 2017, ittianyu Butterknife Apache 2.0 Copyright © 2013, Jake Wharton Crouton Apache 2.0 Copyright © 2012 - 2014, Benjamin Weiss CWAC-SafeRoom Apache 2.0 The copyrights are owned by CommonsWare for things unique to this library and a combination of CommonsWare and the Android Open Source Project for code modified from the Architecture Components' Framework* set of classes. Dagger 2 Apache 2.0 Copyright © 2012, The Dagger Authors Dragtop Layout Apache 2.0 Copyright © 2015, chenupt EventBus Apache 2.0 Copyright © 2012-2017 Markus Junginger, greenrobot FlexibleAdapter Apache 2.0 Copyright © 2015-2018 Davide Steduto, Davidea Solutions Sprl Gson Apache 2.0 Copyright © 2008, Google Inc. markwon Apache 2.0 Copyright © 2019 Dimitry Ivanov (legal@noties.io) mockk Apache 2.0 Copyright © [ 2017] [ github.com/mockk] leakcanary Apache 2.0 Copyright © 2015 Square, Inc. mockito mockito MIT Copyright © 2007 Mockito contributors mosby Apache 2.0 Copyright © 2015 Hannes Dorfmann moshi Apache 2.0 Copyright © 2015 Square, Inc. MPAndroidChart Apache 2.0 Copyright © 2019 Philipp Jahoda New Relic Mobile Agent OKHttp Apache 2.0 Copyright © 2019 Square, Inc. okio Apache 2.0 Copyright © 2013 Square, Inc. Picasso Apache 2.0 Copyright © 2013 Square, Inc. RESTMock Apache 2.0 Copyright © 2016 Appflate.io Retrofit Apache 2.0 Copyright © 2013 Square, Inc. RxJava Apache 2.0 Copyright © 2016-present, RxJava Contributors. Segment IO MIT Copyright © 2016 Segment, Inc. Snackyaml Apache 2.0 Copyright © 2008, www.snakeyaml.org. StickyHeaders Apache 2.0 Copyright © 2014 Emil Sjölander TableView Apache 2.0 Copyright © 2017 Evren Coşkun Transitions-Everywhere Apache 2.0 The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.51782,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Android application <em>licenses</em>",
        "sections": "Android application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Android app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Third Party Dependencies"
      },
      "id": "603e9e30196a67b71fa83d96"
    }
  ],
  "/docs/licenses/product-or-service-licenses/mobile-app-licenses/android-application-licenses": [
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-05-22T11:57:14Z",
      "updated_at": "2021-05-05T16:26:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2018 Alamofire Software Foundation AlamofireObjectMapper MIT Copyright © 2015 Tristan Himmelman Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 259.68005,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS <em>application</em> <em>licenses</em>",
        "sections": "iOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic iOS <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "tvOS application licenses"
      ],
      "title": "tvOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "0b3ac8ec42cef00f5a4d3ddf354e4be38ad0595f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/tvos-application-licenses/",
      "published_at": "2021-05-22T06:13:14Z",
      "updated_at": "2021-05-05T16:26:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic for TV app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation (http://alamofire.org/) Analytics MIT Copyright © 2016 Segment.io, Inc. CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me LoginManagerSDK New Relic License © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License © 2010-2021 New Relic, Inc. All rights reserved. UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. iOS-fontawesome CC BY 3.0 & MIT Copyright © 2012 Alex Usbergo. All rights reserved. The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 259.68002,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "tvOS <em>application</em> <em>licenses</em>",
        "sections": "tvOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic for TV <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "6072d619196a6795b664a75c"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-05-22T09:26:45Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.81482,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses": [
    {
      "sections": [
        "Android application licenses"
      ],
      "title": "Android application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "68c9bdc9dec6f02240f002494309519e41619f29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/android-application-licenses/",
      "published_at": "2021-05-22T14:08:19Z",
      "updated_at": "2021-05-05T16:29:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Android app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Third Party Dependencies License Copyright Android-FlowLayout Apache 2.0 Copyright © 2011, Artem Votincev (apmem.org) AVLoadingIndicatorView Apache 2.0 Copyright © 2015, Jack Wang BottomNavigationViewEx MIT Copyright © 2017, ittianyu Butterknife Apache 2.0 Copyright © 2013, Jake Wharton Crouton Apache 2.0 Copyright © 2012 - 2014, Benjamin Weiss CWAC-SafeRoom Apache 2.0 The copyrights are owned by CommonsWare for things unique to this library and a combination of CommonsWare and the Android Open Source Project for code modified from the Architecture Components' Framework* set of classes. Dagger 2 Apache 2.0 Copyright © 2012, The Dagger Authors Dragtop Layout Apache 2.0 Copyright © 2015, chenupt EventBus Apache 2.0 Copyright © 2012-2017 Markus Junginger, greenrobot FlexibleAdapter Apache 2.0 Copyright © 2015-2018 Davide Steduto, Davidea Solutions Sprl Gson Apache 2.0 Copyright © 2008, Google Inc. markwon Apache 2.0 Copyright © 2019 Dimitry Ivanov (legal@noties.io) mockk Apache 2.0 Copyright © [ 2017] [ github.com/mockk] leakcanary Apache 2.0 Copyright © 2015 Square, Inc. mockito mockito MIT Copyright © 2007 Mockito contributors mosby Apache 2.0 Copyright © 2015 Hannes Dorfmann moshi Apache 2.0 Copyright © 2015 Square, Inc. MPAndroidChart Apache 2.0 Copyright © 2019 Philipp Jahoda New Relic Mobile Agent OKHttp Apache 2.0 Copyright © 2019 Square, Inc. okio Apache 2.0 Copyright © 2013 Square, Inc. Picasso Apache 2.0 Copyright © 2013 Square, Inc. RESTMock Apache 2.0 Copyright © 2016 Appflate.io Retrofit Apache 2.0 Copyright © 2013 Square, Inc. RxJava Apache 2.0 Copyright © 2016-present, RxJava Contributors. Segment IO MIT Copyright © 2016 Segment, Inc. Snackyaml Apache 2.0 Copyright © 2008, www.snakeyaml.org. StickyHeaders Apache 2.0 Copyright © 2014 Emil Sjölander TableView Apache 2.0 Copyright © 2017 Evren Coşkun Transitions-Everywhere Apache 2.0 The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 259.68384,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Android <em>application</em> <em>licenses</em>",
        "sections": "Android <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Android <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Third Party Dependencies"
      },
      "id": "603e9e30196a67b71fa83d96"
    },
    {
      "sections": [
        "tvOS application licenses"
      ],
      "title": "tvOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "0b3ac8ec42cef00f5a4d3ddf354e4be38ad0595f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/tvos-application-licenses/",
      "published_at": "2021-05-22T06:13:14Z",
      "updated_at": "2021-05-05T16:26:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic for TV app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation (http://alamofire.org/) Analytics MIT Copyright © 2016 Segment.io, Inc. CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me LoginManagerSDK New Relic License © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License © 2010-2021 New Relic, Inc. All rights reserved. UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. iOS-fontawesome CC BY 3.0 & MIT Copyright © 2012 Alex Usbergo. All rights reserved. The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 259.68,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "tvOS <em>application</em> <em>licenses</em>",
        "sections": "tvOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic for TV <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "6072d619196a6795b664a75c"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-05-22T09:26:45Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.81479,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/product-or-service-licenses/mobile-app-licenses/tvos-application-licenses": [
    {
      "sections": [
        "Android application licenses"
      ],
      "title": "Android application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "68c9bdc9dec6f02240f002494309519e41619f29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/android-application-licenses/",
      "published_at": "2021-05-22T14:08:19Z",
      "updated_at": "2021-05-05T16:29:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Android app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Third Party Dependencies License Copyright Android-FlowLayout Apache 2.0 Copyright © 2011, Artem Votincev (apmem.org) AVLoadingIndicatorView Apache 2.0 Copyright © 2015, Jack Wang BottomNavigationViewEx MIT Copyright © 2017, ittianyu Butterknife Apache 2.0 Copyright © 2013, Jake Wharton Crouton Apache 2.0 Copyright © 2012 - 2014, Benjamin Weiss CWAC-SafeRoom Apache 2.0 The copyrights are owned by CommonsWare for things unique to this library and a combination of CommonsWare and the Android Open Source Project for code modified from the Architecture Components' Framework* set of classes. Dagger 2 Apache 2.0 Copyright © 2012, The Dagger Authors Dragtop Layout Apache 2.0 Copyright © 2015, chenupt EventBus Apache 2.0 Copyright © 2012-2017 Markus Junginger, greenrobot FlexibleAdapter Apache 2.0 Copyright © 2015-2018 Davide Steduto, Davidea Solutions Sprl Gson Apache 2.0 Copyright © 2008, Google Inc. markwon Apache 2.0 Copyright © 2019 Dimitry Ivanov (legal@noties.io) mockk Apache 2.0 Copyright © [ 2017] [ github.com/mockk] leakcanary Apache 2.0 Copyright © 2015 Square, Inc. mockito mockito MIT Copyright © 2007 Mockito contributors mosby Apache 2.0 Copyright © 2015 Hannes Dorfmann moshi Apache 2.0 Copyright © 2015 Square, Inc. MPAndroidChart Apache 2.0 Copyright © 2019 Philipp Jahoda New Relic Mobile Agent OKHttp Apache 2.0 Copyright © 2019 Square, Inc. okio Apache 2.0 Copyright © 2013 Square, Inc. Picasso Apache 2.0 Copyright © 2013 Square, Inc. RESTMock Apache 2.0 Copyright © 2016 Appflate.io Retrofit Apache 2.0 Copyright © 2013 Square, Inc. RxJava Apache 2.0 Copyright © 2016-present, RxJava Contributors. Segment IO MIT Copyright © 2016 Segment, Inc. Snackyaml Apache 2.0 Copyright © 2008, www.snakeyaml.org. StickyHeaders Apache 2.0 Copyright © 2014 Emil Sjölander TableView Apache 2.0 Copyright © 2017 Evren Coşkun Transitions-Everywhere Apache 2.0 The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 259.68384,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Android <em>application</em> <em>licenses</em>",
        "sections": "Android <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Android <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Third Party Dependencies"
      },
      "id": "603e9e30196a67b71fa83d96"
    },
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-05-22T11:57:14Z",
      "updated_at": "2021-05-05T16:26:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2018 Alamofire Software Foundation AlamofireObjectMapper MIT Copyright © 2015 Tristan Himmelman Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 259.68002,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS <em>application</em> <em>licenses</em>",
        "sections": "iOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic iOS <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-05-22T09:26:45Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.81479,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ]
}