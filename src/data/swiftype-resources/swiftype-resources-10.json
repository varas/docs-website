{
  "/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/rest-api-calls-new-relic-infrastructure-alerts": [
    {
      "sections": [
        "Alert on infrastructure processes",
        "Important",
        "Examples",
        "Ensure enough processes are running to satisfy load",
        "Ensure that critical services run constantly",
        "Monitor startup for critical processes that require special attention",
        "Make sure a job doesn't take too long",
        "Validate that services started successfully",
        "Watch for runaway processes or configuration problems",
        "Create an infrastructure process running condition"
      ],
      "title": "Alert on infrastructure processes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "5fcbe11b9beb16723ff2521fca981f19a4c716ce",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/alert-infrastructure-processes/",
      "published_at": "2021-06-20T04:25:57Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use New Relic infrastructure's Process running alert condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process you expected to start on a host (such as a new program) is not actually running This feature's flexibility allows you to easily filter what hosts and processes to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Examples By applying filters to the hosts and processes that are important to your business, you can define alerting thresholds to decide when violations open and New Relic sends an email notification to you depending on the policy's incident preferences. These examples illustrate how to use infrastructure monitoring's Process running condition to monitor your processes. Ensure enough processes are running to satisfy load Problem: Some load balancers and application servers work by running many worker processes in parallel. Here, for example, you may want an alert violation when fewer than eight processes are running for a service like gunicorn. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Ensure that critical services run constantly Problem: A service, such as a database or application server, is expected to be running constantly on certain hosts, and you need to know when it has stopped. Solution: Use the No processes are running (default) threshold. Monitor startup for critical processes that require special attention Problem: You have processes requiring special attention due to security or potential performance impact. Solution: Use the At least one process is running threshold with condition filters set to a username and specific executable so that New Relic can open a violation when the process is running. Make sure a job doesn't take too long Problem: You have a job that runs periodically, and you want to open a violation when it has been running longer than an expected number of minutes. Solution: Use the At least one process is running threshold. Validate that services started successfully Problem: When provisioning new hosts, you want to open a violation if a required service fails to successfully start up. Solution: Use the No processes are running (default) threshold. Watch for runaway processes or configuration problems Problem: Sometimes problems with processes can be solved with changes to your configuration. For example, you have more than one Chef process running, and you may need to address an issue with how that service is configured. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Create an infrastructure process running condition To define the Process running alert criteria: Follow standard procedures to create an infrastructure alert condition. Select Process running as the Alert type. Filter what hosts and processes you want the alert condition to apply to. Define the Critical threshold for triggering the alert notification: minimum 1 minute, default 5 minutes, maximum 60 minutes. If you create the alert condition directly with infrastructure monitoring, New Relic will send an email notification when the defined threshold for the alert condition passes depending on the policy's incident preferences. Your alert policy defines which personnel or teams and which notification channels we use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.06607,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alert</em> on <em>infrastructure</em> processes",
        "sections": "<em>Alert</em> on <em>infrastructure</em> processes",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use New Relic <em>infrastructure</em>&#x27;s Process running <em>alert</em> condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process you expected to start"
      },
      "id": "603eb49128ccbca939eba74a"
    },
    {
      "sections": [
        "Create infrastructure \"host not reporting\" condition",
        "Features",
        "Create \"host not reporting\" condition",
        "Investigate the problem",
        "Intentional outages"
      ],
      "title": "Create infrastructure \"host not reporting\" condition",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "0a74e7e65e3eeb5268eac310c11802ca2e78a614",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-alerts/infrastructure-alert-conditions/create-infrastructure-host-not-reporting-condition/",
      "published_at": "2021-06-20T20:45:36Z",
      "updated_at": "2021-03-16T08:30:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use Infrastructure monitoring's Host not reporting condition to notify you when we've stopped receiving data from an infrastructure agent. This feature allows you to dynamically alert on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of alerts notifications. Features You can define conditions based on the sets of hosts most important to you, and configure thresholds appropriate for each filter set. The Host not reporting event triggers when data from the infrastructure agent doesn't reach our collector within the time frame you specify. This feature's flexibility allows you to easily customize what to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Host not reporting condition Features What to monitor You can use filter sets to select which hosts you want to be monitored with the alert condition. The condition will also automatically apply to any hosts you add in the future that match these filters. How to notify Conditions are contained in policies. You can select an existing policy or create a new policy with email notifications from the Infrastructure monitoring UI. If you want to create a new policy with other types of notification channels, use the UI. When to notify Email addresses (identified in the policy) will be notified automatically about threshold violations for any host matching the filters you have applied, depending on the policy's incident preferences. Where to troubleshoot The link at the top of the email notification will take you to the infrastructure Events page centered on the time when the host disconnected. Additional links in the email will take you to additional detail. Create \"host not reporting\" condition To define the Host not reporting condition criteria: Follow standard procedures to create an infrastructure condition. Select Host not reporting as the Alert type. Define the Critical threshold for triggering the notification: minimum 5 minutes, maximum 60 minutes. Enable 'Don't trigger alerts for hosts that perform a clean shutdown' option, if you want to prevent false alerts when you have hosts set to shut down via command line. Currently this feature is supported on all Windows systems and Linux systems using systemd. Depending on the policy's incident preferences, it will define which notification channels to use when the defined Critical threshold for the condition passes. To avoid \"false positives,\" the host must stop reporting for the entire time period before a violation is opened. Example: You create a condition to open a violation when any of the filtered set of hosts stop reporting data for seven minutes. If any host stops reporting for five minutes, then resumes reporting, the condition does not open a violation. If any host stops reporting for seven minutes, even if the others are fine, the condition does open a violation. Investigate the problem To further investigate why a host is not reporting data: Review the details in the email notification. Use the link from the email notification to monitor ongoing changes in your environment from Infrastructure monitoring's Events page. For example, use the Events page to help determine if a host disconnected right after a root user made a configuration change to the host. Optional: Use the email notification's Acknowledge link to verify you are aware of and taking ownership of the alerting incident. Use the email links to examine additional details in the Incident details page. Intentional outages We can distinguish between unexpected situations and planned situations with the option Don't trigger alerts for hosts that perform a clean shutdown. Use this option for situations such as: Host has been taken offline intentionally. Host has planned downtime for maintenance. Host has been shut down or decommissioned. Autoscaling hosts or shutting down instances in a cloud console. We rely on Linux and Windows shutdown signals to flag a clean shutdown. We've confirmed that these scenarios are detected by the agent: AWS Auto-scaling event with EC2 instances that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) User-initiated shutdown of Windows systems User-initiated shutdown of Linux systems that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) We know that these scenarios are not detected by the agent: User-initiated shutdown of Linux systems that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other modern Linux systems that still use Upstart or SysV init systems. AWS Auto-scaling event with EC2 instances that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other more modern Linux systems that still use Upstart or SysV init systems.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.06546,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "sections": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use <em>Infrastructure</em> monitoring&#x27;s Host not reporting condition to notify you when we&#x27;ve stopped receiving data from an <em>infrastructure</em> agent. This feature allows you to dynamically <em>alert</em> on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of <em>alerts</em>"
      },
      "id": "603ea06c196a67cd47a83dc1"
    },
    {
      "sections": [
        "Infrastructure alerting examples",
        "Examples: Infrastructure pages",
        "Examples: Threshold options",
        "Integrations providers",
        "CPU, disk, load average, memory, swap",
        "Byte size"
      ],
      "title": "Infrastructure alerting examples",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "1ec5f86b745413b2a8d6a5b676ecbe622c674ab1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/infrastructure-alerting-examples/",
      "published_at": "2021-06-20T04:26:54Z",
      "updated_at": "2021-03-16T07:35:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Alert type field in infrastructure monitoring's Settings > Alerts page shows what options you can select to create infrastructure alert conditions. You can also create alert conditions from any infrastructure chart by selecting the ellipses icon and then Create alert. Examples: Infrastructure pages Here are some examples of how to create alert conditions within the context of the Infrastructure monitoring UI page you are currently viewing. To create an alerts condition from any chart, select the ellipses icon and then Create alert. New Relic will automatically select the appropriate Alert type. Example Problem and solution High CPU usage Problem: Your Ops team monitors a filtered set of host clusters in your eastern region and notices that the CPU usage is constantly high. Solution: Use the CPU chart on Infrastructure monitoring's Hosts page to create an alert condition for system metrics. Virtual memory capacity Problem: Your night shift needs to be alerted when virtual memory for a set of background workers reaches an average of 10G for at least two minutes. Solution: Use the Top memory consumers chart on Infrastructure monitoring's Processes page to create an alert condition for process metrics. Limited bandwidth Problem: You want to monitor performance based on the average number of errors received or transmitted. Solution: Use the Top bandwidth chart on Infrastructure monitoring's Network page to create an alert condition for network metrics. I/O read and write operations Problem: You are testing a new set of hosts in your staging environment, and you want to be notified when their read or write capacity rises above your test threshold level. Solution: Use the Top I/O operations chart on Infrastructure monitoring's Storage page to create an alert condition for storage metrics. Host not reporting Problem: You want to be notified when we have stopped receiving data from an infrastructure agent. Solution: From the Hosts, Processes, Network, or Storage pages, create a host not reporting alert condition. Processes not running as expected Problem: You want to be notified if any of the processes on your hosts stop reporting. OR A process you expected to start on a host (such as a new program) is not actually running. Solution: From the Processes page (or from the Hosts, Network, or Storage pages), create a process running alert condition. Examples: Threshold options Use the thresholds dropdown for the selected Alert type to further define how you want to be alerted. Here are some examples of the options available. Integrations providers With infrastructure integrations, you can create an alert condition from your Integrations page. Depending on the type of provider selected (CloudFront, DynamoDB, EBS, etc.), options will vary from the Define thresholds dropdown; for example, bytes, errors, requests, CPU, connections, memory, records, latency, etc. CPU, disk, load average, memory, swap The System metrics thresholds dropdown allows you to select various criteria for CPU, disk, load average, memory, and swap metrics. Byte size The Network metrics thresholds provide flexibility with your business needs. Depending on the size of your network, you can easily set the threshold in bytes, KB, MB, GB, or TB.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.0586,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>alerting</em> examples",
        "sections": "<em>Infrastructure</em> <em>alerting</em> examples",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "The <em>Alert</em> type field in <em>infrastructure</em> monitoring&#x27;s Settings &gt; <em>Alerts</em> page shows what options you can select to create <em>infrastructure</em> <em>alert</em> <em>conditions</em>. You can also create <em>alert</em> <em>conditions</em> from any <em>infrastructure</em> chart by selecting the ellipses icon and then Create <em>alert</em>. Examples: <em>Infrastructure</em>"
      },
      "id": "603eb52a28ccbc9027eba7bb"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/events-heatmap-examine-patterns-time-range": [
    {
      "sections": [
        "Infrastructure Inventory page: Search your entire infrastructure",
        "Inventory item naming",
        "Tip",
        "Page functions",
        "Filter the data",
        "Search inventory",
        "View inventory item details",
        "View host's alert threshold violations",
        "Inventory data collection",
        "Linux built-in agent data",
        "Windows built-in agent data",
        "Amazon AWS cloud integrations inventory",
        "Inventory data retention",
        "Chart data attributes"
      ],
      "title": "Infrastructure Inventory page: Search your entire infrastructure",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "64aef10b24b74ac3c0f070358d37f3cab099e5b2",
      "image": "https://docs.newrelic.com/static/2d17c192725956ff09b5e987be5b997b/747d8/inventory-name-source-path.jpg",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-ui-pages/infra-ui-pages/infrastructure-inventory-page-search-your-entire-infrastructure/",
      "published_at": "2021-06-20T11:57:30Z",
      "updated_at": "2021-03-11T12:47:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic can collect detailed information about a system's configuration per host, including system modules, configuration files, metadata, packages, services, user sessions, etc. The Inventory page provides a real-time, filterable, searchable view into each host's configuration. Use the Inventory page to: Ensure a version update was applied successfully across all your hosts. Audit version discrepancies across your hosts. Quickly identify which hosts require an update to fix a security vulnerability. To view and search your inventory data: Go to one.newrelic.com > Infrastructure > Inventory. Inventory item naming The infrastructure inventory is a qualified namespace (structured like a directory) that organizes inventory items into names that resemble a source path. The inventory item name is comprised of three elements: Element Description Category Basic, top level type of data source, typically based on its role in the system. Common examples include config, package, kernel, user session, services, and modules. Source The specific data source for the inventory item. Label The name of the specific inventory item; for example, the filename, package name, or system setting name. Tip For detailed metadata and other information about your hosts, use tagging with New Relic One. Page functions Use Inventory page functions to find information about a particular item on your hosts: Filter the data Use Filter Sets to show only hosts matching certain criteria. Search inventory Search for an inventory item using the search function. For example, if you want to find information related to OpenSSL, search openssl. The search term is matched again the inventory item name. View inventory item details Inventory item details provide host and system information for each host it resides on according to the New Relic inventory item name. If you have different versions of the same item on other hosts, New Relic detects that and flags them on the Inventory page with the variant hosts label and lists each host running each version. Item details are attributes (key/value pairs) that are dictated by their source. Specific attributes are generally stable over time, but new ones may be added and others could be deprecated. Attributes carry the critical metadata that are at the heart of each inventory item. Common inventory item attributes include: Variant hosts (hostname) Architecture Description Essential Priority Status Version View host's alert threshold violations To view one or more host's alert threshold violations, select the host's Critical icon or Warning icon. Inventory data collection Inventory is collected from the infrastructure agent's built-in data collectors, Amazon Elastic Compute Cloud (EC2) integrations, agent integrations provided by New Relic, and customer-built integrations. The data appears on the Inventory page and in other places within the Infrastructure monitoring's user interface. Linux built-in agent data The infrastructure agent collects this data for Linux systems. Category Source Data collected using... applications apm APM Language Agent metadata config selinux sestatus -b, semodule -l selinux-policies sestatus -b, semodule -l selinux-modules sestatus -b, semodule -l sshd /etc/sshd_config (PermitRootLogin, PermitEmptyPasswords, PasswordAuthentication, and ChallengeResponseAuthentication only) kernel modules /sbin/modinfo, /sbin/lsmod, /proc/modules sysctl /proc/sys metadata agent_config Agent's complete config file attributes Agent's custom_attributes host_aliases Agent's display_name, Cloud provider instance-id, os.Hostname(), hostname -f, hostname cloud_security_groups Cloud provider security-groups system uptime -s, /etc/redhat-release, /proc/cpuinfo, /etc/os-release, /proc/sys/kernel/random/boot_id, /proc/sys/kernel/osrelease, /sys/class/dmi/id/product_uuid, /sys/devices/virtual/dmi/id/sys_vendor, /sys/devices/virtual/dmi/id/product_name facter facter -p -j services daemontool ps -e, svstat systemd initctl list upstart systemctl -l, systemctl show, modinfo, lsmod supervisord /var/run/supervisor.sock unix socket connection, supervisor.getAllProcessInfo pidfile var/run, find -L -name, /proc/N/status, /proc/N/stat sessions users who system network_interfaces net.Interfaces() packages dpkg dpkg-query -W -f rpm rpm -qa Windows built-in agent data The infrastructure agent collects this data for Windows systems. Category Source Data collected using... applications apm APM language agent metadata metadata agent_config Agent's complete config file attributes Agent's custom_attributes host_aliases Agent's display_name, Cloud provider instance-id, os.Hostname(), Registry (SYSTEM \\ CurrentControlSet \\ Services \\ Tcpip \\ Parameters (Domain, DhcpDomain, Hostname) system kernel32.dll (GetPhysicallyInstalledSystemMemory), WMI (Win32_OperatingSystem, Win32_Processor), os.Hostname() services windows_services WMI (Win32_Service WHERE State = \"Running\" AND StartMode = \"Auto\") system network_interfaces net.Interfaces() packages windows_programs Registry (SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\, SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\) windows_updates WMI (Win32_QuickFixEngineering) (off by default) Amazon AWS cloud integrations inventory Data collected varies by Amazon Elastic Compute Cloud (EC2) integration. For more information, see New Relic's individual Amazon Integrations documentation. Inventory data retention Inventory data is real-time. If a host stops reporting, its inventory data still displays for up to 24 hours. Chart data attributes For a technical explanation about attributes used to populate the Inventory page, see Default infrastructure attributes and events. This includes a summary of common events by operating system.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.39355,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> Inventory page: Search your entire <em>infrastructure</em>",
        "sections": "<em>Infrastructure</em> Inventory page: Search your entire <em>infrastructure</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": ", Amazon Elastic Compute Cloud (EC2) integrations, agent integrations provided by New Relic, and customer-built integrations. The data appears on the Inventory page and in other places within the <em>Infrastructure</em> <em>monitoring</em>&#x27;s user interface. Linux built-in agent data The <em>infrastructure</em> agent collects"
      },
      "id": "60440a6d64441fdf50378ee7"
    },
    {
      "sections": [
        "Infrastructure Events page: Live feed of config changes",
        "Event types",
        "Events page features",
        "Chart data attributes"
      ],
      "title": "Infrastructure Events page: Live feed of config changes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "e4a87670c8671072ae7cc6531721f46edc7f925d",
      "image": "https://docs.newrelic.com/static/75373d03d819516d3cbe23f1ea65957b/c1b63/infra-events-ui.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/infrastructure-events-page-live-feed-every-config-change/",
      "published_at": "2021-06-20T04:28:07Z",
      "updated_at": "2021-03-11T11:47:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Events page is a live feed of important system and host activity, including inventory change events, configuration changes, and log analytics events. The event feed helps you understand correlations between these events and system performance. Search and filter through your events to decrease the mean time to detect and repair infrastructure issues. You can access the Events page by going to one.newrelic.com > Infrastructure > Events. Event types New Relic collects a variety of change events so you can understand each change in your environment: Events Comments Alert incidents When a violation is opened or closed, New Relic generates an event indicating the host and associated alert condition. Agent connection When an infrastructure agent connects to New Relic, our platform generates an Agent connected event. If New Relic doesn't receive data from an agent for three minutes, the platform generates an Agent disconnected event. Inventory changes These events are generated when inventory data is added, removed, or modified. Select the source icon to to understand which category corresponds to the altered inventory path. For additional details, select an inventory event to see a side-by-side comparison of the old and new state. Inventory events can include: Kernel (includes modules and configuration): Can be added, modified, or deleted. Metadata (includes various additional information about hosts): Can be added, modified, or deleted. Packages: Can be installed, removed, or modified. Services: Can be started, stopped, or restarted. Sessions (includes users): Can be connected or disconnected. Events page features To view the live event feed: Go to one.newrelic.com > Infrastructure > E * *vents. The Events * * page includes a heatmap, which provides a snapshot of the events occurring within the selected time range. one.newrelic.com > Infrastructure > Events: Use the Events to view important, real-time activity in your infrastructure. With the Events page, you can easily search through your event log to quickly find vulnerable packages. If you want to... Do this... Focus on specific events Use the Search events field to look for specific events, config changes or agent installations. To focus on a specific set of events, select or change the filter set. Search within a particular time range Enter a time range to the right of the search bar to investigate events within a specific time range. For example, if you encountered a CPU spike around 11am on the previous day, search Yesterday at 11 am to investigate the possible cause. Compare events with host load, memory, CPU, and more View the events feed on the Hosts page. To compare infrastructure events and performance for a specific time, select a range via the time picker or drag and select a range on a chart. View events specifically related to agents, config, metadata, services, or sessions Group or sort events by selecting the filter icon be the search bar. Drill down into additional details Select an event to view additional details, such as attributes and values. To drill down further, select View in Inventory to see additional details in the Inventory page. View host's alert threshold violation Select the host's Critical icon or Warning icon. Chart data attributes For a technical explanation of the attributes used to populate the Events page, see InfrastructureEvent attributes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.38725,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> Events page: Live feed of config changes",
        "sections": "<em>Infrastructure</em> Events page: Live feed of config changes",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": " the mean time to detect and repair <em>infrastructure</em> issues. You can access the Events page by going to one.newrelic.com &gt; <em>Infrastructure</em> &gt; Events. Event types New Relic collects a variety of change events so you can understand each change in your environment: Events Comments Alert incidents When"
      },
      "id": "6043fa6c28ccbc13742c60a5"
    },
    {
      "sections": [
        "Infrastructure monitoring Hosts page",
        "System tab",
        "System tab functions",
        "APM and infrastructure data",
        "Important",
        "Network tab",
        "Network tab functions",
        "Processes tab",
        "Tip",
        "Processes tab functions",
        "Storage tab",
        "Storage tab functions",
        "Docker containers tab",
        "Docker containers tab functions"
      ],
      "title": "Infrastructure monitoring Hosts page",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "41d8a2ac3ecbbdee164fd0bec6ac94bb3e8def64",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-ui-pages/infrastructure-ui/infrastructure-hosts-page/",
      "published_at": "2021-06-20T14:25:32Z",
      "updated_at": "2021-03-09T04:04:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Hosts page to better understand important performance metrics, like resource usage, network and processes performance, and data about your containers. You can filter your hosts by attributes and compare their performance with inventory change events to help determine root causes. You can also set alert notifications to help prevent problems. To view your hosts information, go to one.newrelic.com > Infrastructure > Hosts, then click any of the following tabs: System: Overview of your hosts' performance Network: Bandwidth and error data about your network interfaces Processes: Data about CPU percentage, I/O bytes, and memory usage for individual or groups of processes Storage: Resources' capacity and efficiency, including your devices' overall utilization, disk usage, or I/O operations Docker containers: CPU percentage, I/O bytes, and memory usage for individual or group of containers System tab Here are the default graphs shown on the Hosts page. For additional chart options, select the dropdown for any chart. CPU %: On the System tab, CPU Percent is a derived metric that is part of the SystemSample event. The CPU percentage is not collected by New Relic, but derived from several other metrics. Specifically, the cpuPercent attribute is an aggregation of cpuUserPercent, cpuSystemPercent, cpuIoWaitPercent and cpuStealPercent. Load average five minute: represents an average of the number of tasks waiting to do work on a system over the last 5 minutes. Memory free %: compares the amount of free memory bytes to the amount of used memory bytes. For explanations of different chart metrics, see Default infrastructure attributes and events. Functions for adjusting chart data include: Select different host groups: Use the host filter. Change time range: Use the time picker above the charts, or drag and select a time range on a chart. Change the number of elements on charts: use the Chart shows dropdown. Change data used to order hosts: Use the table below the charts to order the chart elements by different metrics; for example, from highest to lowest CPU user % to lowest, or from lowest to highest Disk free. System tab functions Here are some of the things you can do from the System tab: If you want to... Do this... Filter and group hosts Use filter sets to show only hosts matching certain criteria, or use group by to aggregate the results. Understand host health Use the Health column of the table. To see more details about specific violations, select the health status icons. Find root causes of issues Use the Events heatmap at the top of the page to compare performance to important events happening in your infrastructure. For more, see Events heatmap. Set an alert condition for a metric Mouse over a chart, select and then Create alert. View host's alert threshold violation If present, select the host's Critical icon or Warning icon. APM and infrastructure data If you have APM data associated with your infrastructure data, there are several ways to access your APM data on the Hosts page: Use the hosts filter to filter on hosts running specific applications. In the host list below the charts, select the Application column to filter on specific applications. From the chart selector dropdown beside a chart's name, select one of the application-related charts. Important APM charts in infrastructure monitoring do not have View query or Create alert options like the other infrastructure charts do. For more about using APM and infrastructure monitoring together, see APM data in infrastructure. Network tab The Network page provides real-time visibility into the health and performance of individual hosts, web servers, or other groups of resources across your network. Default charts show bandwidth metrics by packet, bandwidth by bytes, and errors per second. Details about individual interfaces can help you: Examine saturation levels across your network or for selected filter sets. Compare load balances between specific resources. Identify unexpected differences in capacity patterns between similar hosts. Evaluate the top five network errors that New Relic automatically presents for the selected time period. This real-time network data can then help you determine whether to resolve errors by reconfiguring your network, rebalancing the loads, or taking other preventative maintenance actions before needing to make a purchase decision. From the Network tab you can view bandwidth and error data about your network interfaces. The Network page includes an Events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. For a technical explanation of the attributes used to populate the Network page chart, see NetworkSample attributes. Network tab functions Here are some of the things you can do from the Network tab: If you want to... Do this... Filter and group Use filter sets to show only hosts matching certain criteria, or use Group by to aggregate the results. Select a time range Use the time picker to change the range of time selected. You can also click and drag on the chart to select a custom time range. When you select a time range, it carries over when you go from one infrastructure page to another. Change metrics displayed in chart Use the sorting dropdown to switch what metric the chart is displaying for the chosen process groupings, including: Received Bytes Received Dropped Received Errors Received Packets Transmitted Bytes Transmitted Dropped Transmitted Errors Transmitted Packets Search and filter the list Type in the Search interfaces search bar to filter the list to only those items containing the text you've typed. You can also filter the list by simply selecting the list item or user name you want to filter for. The chosen filters are displayed beside the filter icon above the chart. Add items to chart The chart, by default, displays the top five results. Use the Chart shows dropdown to display more results. To remove an item from a chart, select its name below the chart. Set an alert condition for a metric Mouse over a chart, select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Processes tab Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Use the Processes tab to get information about processes running on your hosts, and to set alerts on process metrics. The Processes tab shows data such as CPU percentage, I/O bytes, and memory usage for individual processes or groupings of processes. The page also includes an events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. Use this information to: Pinpoint processes that are causing performance issues. Create alerts for conditions such as CPU percentage and memory usage. On the Processes page, CPU percent is scoped to individual processes, rather than hosts. Because of this, the CPU percent metric does not take into account the resources of the entire system. Instead, it shows how much of a single CPU core each process is taking. Example Here's an example of how to pinpoint an issue and set an alert: You notice on the Hosts page that a cluster has a CPU percentage much higher than normal. You go to the Processes page and filter down to that cluster's processes. You notice that several instances of the same process have excessive CPU percentage. After fixing the issue, you decide to create an alert condition that triggers when the CPU percentage for that process type exceeds a certain threshold. For a technical explanation of the attributes used to populate the Processes page chart, see ProcessSample attributes. Tip You cannot start, stop, or restart processes from the UI. Instead, follow standard procedures to start, stop, check, or restart the infrastructure agent manually. Processes tab functions Here are some of the things you can do from the Processes tab: If you want to... Do this... Only show hosts matching certain criteria Use filter sets. Aggregate results or group by host attributes Use Group by to change how the processes are grouped in the chart and the list. The dropdown contains host-related attributes such as hostID and entityName. If you are monitoring AWS EC2 instances, EC2-related attributes such as awsRegion will be available. Select a time range Use either of these options: Select any of the time picker options at the top of the page. Click and drag on the chart to select a custom time range. After you select a time range, it carries over from one infrastructure page to another. View Docker metrics To view process usage by Docker containers, see Docker instrumentation. Change charts Select the dropdown beside the chart's name to switch what metric the chart displays. Choices include: Memory: Resident or virtual size CPU percentage Disk I/O: Read or write Search and filter the list Use either of these options: Type in the Search processes search bar. Select the list item or user name you want to filter for. The selected filters appear beside the filter icon above the chart, where you can select and remove them as needed. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Storage tab The Storage page allows you to monitor your resources' capacity and efficiency, including your devices' overall utilization, disk usage, or I/O operations. This can help you to: Examine unexpected patterns; for example, a cluster of mirrored machines that do not process I/O tasks uniformly. Monitor usage levels before disk space completely runs out. Set alert conditions to notify you about problems with one or more hosts; for example, long processing times for read or write operations, disk availability or utilization based on percentage or bytes, etc. Make informed decisions about where to redistribute hosts with higher or lower than normal processing requests. Use data to help plan for additions or changes to your infrastructure budget before an emergency arises. The Storage page includes an Events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. For a technical explanation of the attributes used to populate the Storage page chart, see StorageSample attributes. Storage tab functions Here are some of the things you can do from the Storage tab: If you want to... Do this... Filter and group Use filter sets to show only hosts matching certain criteria, or use Group by to aggregate the results. Select a time range Use the time picker on the upper right to change the range of time selected. You can also click and drag on the chart to select a custom time range. When you select a time range, it carries over when you go from one infrastructure page to another. Change metrics displayed in chart Use the sorting dropdown to switch what metric the chart is displaying for the chosen process groupings. Choices include: Total Utilization % Read Utilization % Write Utilization % Disk Used Bytes Disk Free Bytes I/O Read Operations I/O Write Operations Search and filter the list Type in the Search devices search bar to filter the list. You can also filter the list by simply selecting the list item or user name you want to filter for. The chosen filters are displayed beside the filter icon above the chart. Add/remove chart items The chart, by default, displays the top five results. Use the Chart shows dropdown to display more results. To remove an item, select its name below the chart. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Docker containers tab Use the Docker containers tab to get information about the containers running on your hosts, and to set alerts on container metrics. The Docker containers tab shows data such as CPU percentage, I/O bytes, and memory usage for individual containers or groupings of containers. The page also includes an events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. Use this information to: Identify containers that are experiencing performance issues. Create alerts for conditions such as CPU percentage and memory usage. Docker containers tab functions Here are some of the things you can do from the Docker containers tab: If you want to... Do this... Only show hosts matching certain criteria Use filter sets. Aggregate results or group by host attributes Use Group by to change how the processes are grouped in the chart and the list. The dropdown contains host-related attributes such as image and operatingSystem. Select a time range Use either of these options: Select any of the time picker options at the top of the page. Click and drag on the chart to select a custom time range. After you select a time range, it carries over from one infrastructure page to another. View Docker integration dashboard To open the Docker integration dashboard, click the Dashboard link above the data table. Change charts Select the dropdown beside the chart's name to switch what metric the chart displays. Choices include: CPU: Used cores, kernel percentage Memory: Size limit, cache bytes I/O: Write count per second, total bytes Network: Errors per second, packets Process: Process count, restart count Search and filter the list Use either of these options: Type in the Search processes search bar. Select the list item or user name you want to filter for. The selected filters appear beside the filter icon above the chart, where you can select and remove them as needed. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. View host's alert threshold violation Select the host's Critical icon or Warning icon.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.04369,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>monitoring</em> Hosts page",
        "sections": "<em>Infrastructure</em> <em>monitoring</em> Hosts page",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": "-related charts. Important APM charts in <em>infrastructure</em> <em>monitoring</em> do not have View query or Create alert options like the other <em>infrastructure</em> charts do. For more about using APM and <em>infrastructure</em> <em>monitoring</em> together, see APM data in <em>infrastructure</em>. Network tab The Network page provides real-time"
      },
      "id": "60440a6d196a675f6c960f58"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/infrastructure-events-page-live-feed-every-config-change": [
    {
      "sections": [
        "Events heatmap: Examine patterns in time range"
      ],
      "title": "Events heatmap: Examine patterns in time range",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "bc50e789884c9c4eea404d558d4070519a3eab0c",
      "image": "https://docs.newrelic.com/static/96c3e087c9dfb8b4cb4ad72b79c47e94/c1b63/infra-events-timeline.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/events-heatmap-examine-patterns-time-range/",
      "published_at": "2021-06-20T04:28:07Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "he events heatmap provides a snapshot of the infrastructure events occurring within the same time range as the displayed metrics. The darker the color on the heatmap, the more events occurred during that time period. By comparing the heatmap to the charts on the infrastructure page, you can quickly pinpoint issues in your ecosystem. For example, if a massive CPU spike occurs, you can click on the events heatmap for that time range to find the event that caused it. From there you can dive deeper to uncover the real issue. one.newrelic.com > Infrastructure: The heatmap on Infrastructure monitoring UI pages visually shows patterns with events occurring at the same time period for the displayed metrics. The heatmap appears on several Infrastructure UI pages, including: System Network Processes Storage Events",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.15712,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": " <em>monitoring</em> <em>UI</em> pages visually shows patterns with events occurring at the same time period for the displayed metrics. The heatmap appears on several <em>Infrastructure</em> <em>UI</em> pages, including: System Network Processes Storage Events"
      },
      "id": "603e8455196a67833da83dc2"
    },
    {
      "sections": [
        "Infrastructure Inventory page: Search your entire infrastructure",
        "Inventory item naming",
        "Tip",
        "Page functions",
        "Filter the data",
        "Search inventory",
        "View inventory item details",
        "View host's alert threshold violations",
        "Inventory data collection",
        "Linux built-in agent data",
        "Windows built-in agent data",
        "Amazon AWS cloud integrations inventory",
        "Inventory data retention",
        "Chart data attributes"
      ],
      "title": "Infrastructure Inventory page: Search your entire infrastructure",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "64aef10b24b74ac3c0f070358d37f3cab099e5b2",
      "image": "https://docs.newrelic.com/static/2d17c192725956ff09b5e987be5b997b/747d8/inventory-name-source-path.jpg",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-ui-pages/infra-ui-pages/infrastructure-inventory-page-search-your-entire-infrastructure/",
      "published_at": "2021-06-20T11:57:30Z",
      "updated_at": "2021-03-11T12:47:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic can collect detailed information about a system's configuration per host, including system modules, configuration files, metadata, packages, services, user sessions, etc. The Inventory page provides a real-time, filterable, searchable view into each host's configuration. Use the Inventory page to: Ensure a version update was applied successfully across all your hosts. Audit version discrepancies across your hosts. Quickly identify which hosts require an update to fix a security vulnerability. To view and search your inventory data: Go to one.newrelic.com > Infrastructure > Inventory. Inventory item naming The infrastructure inventory is a qualified namespace (structured like a directory) that organizes inventory items into names that resemble a source path. The inventory item name is comprised of three elements: Element Description Category Basic, top level type of data source, typically based on its role in the system. Common examples include config, package, kernel, user session, services, and modules. Source The specific data source for the inventory item. Label The name of the specific inventory item; for example, the filename, package name, or system setting name. Tip For detailed metadata and other information about your hosts, use tagging with New Relic One. Page functions Use Inventory page functions to find information about a particular item on your hosts: Filter the data Use Filter Sets to show only hosts matching certain criteria. Search inventory Search for an inventory item using the search function. For example, if you want to find information related to OpenSSL, search openssl. The search term is matched again the inventory item name. View inventory item details Inventory item details provide host and system information for each host it resides on according to the New Relic inventory item name. If you have different versions of the same item on other hosts, New Relic detects that and flags them on the Inventory page with the variant hosts label and lists each host running each version. Item details are attributes (key/value pairs) that are dictated by their source. Specific attributes are generally stable over time, but new ones may be added and others could be deprecated. Attributes carry the critical metadata that are at the heart of each inventory item. Common inventory item attributes include: Variant hosts (hostname) Architecture Description Essential Priority Status Version View host's alert threshold violations To view one or more host's alert threshold violations, select the host's Critical icon or Warning icon. Inventory data collection Inventory is collected from the infrastructure agent's built-in data collectors, Amazon Elastic Compute Cloud (EC2) integrations, agent integrations provided by New Relic, and customer-built integrations. The data appears on the Inventory page and in other places within the Infrastructure monitoring's user interface. Linux built-in agent data The infrastructure agent collects this data for Linux systems. Category Source Data collected using... applications apm APM Language Agent metadata config selinux sestatus -b, semodule -l selinux-policies sestatus -b, semodule -l selinux-modules sestatus -b, semodule -l sshd /etc/sshd_config (PermitRootLogin, PermitEmptyPasswords, PasswordAuthentication, and ChallengeResponseAuthentication only) kernel modules /sbin/modinfo, /sbin/lsmod, /proc/modules sysctl /proc/sys metadata agent_config Agent's complete config file attributes Agent's custom_attributes host_aliases Agent's display_name, Cloud provider instance-id, os.Hostname(), hostname -f, hostname cloud_security_groups Cloud provider security-groups system uptime -s, /etc/redhat-release, /proc/cpuinfo, /etc/os-release, /proc/sys/kernel/random/boot_id, /proc/sys/kernel/osrelease, /sys/class/dmi/id/product_uuid, /sys/devices/virtual/dmi/id/sys_vendor, /sys/devices/virtual/dmi/id/product_name facter facter -p -j services daemontool ps -e, svstat systemd initctl list upstart systemctl -l, systemctl show, modinfo, lsmod supervisord /var/run/supervisor.sock unix socket connection, supervisor.getAllProcessInfo pidfile var/run, find -L -name, /proc/N/status, /proc/N/stat sessions users who system network_interfaces net.Interfaces() packages dpkg dpkg-query -W -f rpm rpm -qa Windows built-in agent data The infrastructure agent collects this data for Windows systems. Category Source Data collected using... applications apm APM language agent metadata metadata agent_config Agent's complete config file attributes Agent's custom_attributes host_aliases Agent's display_name, Cloud provider instance-id, os.Hostname(), Registry (SYSTEM \\ CurrentControlSet \\ Services \\ Tcpip \\ Parameters (Domain, DhcpDomain, Hostname) system kernel32.dll (GetPhysicallyInstalledSystemMemory), WMI (Win32_OperatingSystem, Win32_Processor), os.Hostname() services windows_services WMI (Win32_Service WHERE State = \"Running\" AND StartMode = \"Auto\") system network_interfaces net.Interfaces() packages windows_programs Registry (SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\, SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\) windows_updates WMI (Win32_QuickFixEngineering) (off by default) Amazon AWS cloud integrations inventory Data collected varies by Amazon Elastic Compute Cloud (EC2) integration. For more information, see New Relic's individual Amazon Integrations documentation. Inventory data retention Inventory data is real-time. If a host stops reporting, its inventory data still displays for up to 24 hours. Chart data attributes For a technical explanation about attributes used to populate the Inventory page, see Default infrastructure attributes and events. This includes a summary of common events by operating system.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.39355,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> Inventory page: Search your entire <em>infrastructure</em>",
        "sections": "<em>Infrastructure</em> Inventory page: Search your entire <em>infrastructure</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": ", Amazon Elastic Compute Cloud (EC2) integrations, agent integrations provided by New Relic, and customer-built integrations. The data appears on the Inventory page and in other places within the <em>Infrastructure</em> <em>monitoring</em>&#x27;s user interface. Linux built-in agent data The <em>infrastructure</em> agent collects"
      },
      "id": "60440a6d64441fdf50378ee7"
    },
    {
      "sections": [
        "Infrastructure monitoring Hosts page",
        "System tab",
        "System tab functions",
        "APM and infrastructure data",
        "Important",
        "Network tab",
        "Network tab functions",
        "Processes tab",
        "Tip",
        "Processes tab functions",
        "Storage tab",
        "Storage tab functions",
        "Docker containers tab",
        "Docker containers tab functions"
      ],
      "title": "Infrastructure monitoring Hosts page",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "41d8a2ac3ecbbdee164fd0bec6ac94bb3e8def64",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-ui-pages/infrastructure-ui/infrastructure-hosts-page/",
      "published_at": "2021-06-20T14:25:32Z",
      "updated_at": "2021-03-09T04:04:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Hosts page to better understand important performance metrics, like resource usage, network and processes performance, and data about your containers. You can filter your hosts by attributes and compare their performance with inventory change events to help determine root causes. You can also set alert notifications to help prevent problems. To view your hosts information, go to one.newrelic.com > Infrastructure > Hosts, then click any of the following tabs: System: Overview of your hosts' performance Network: Bandwidth and error data about your network interfaces Processes: Data about CPU percentage, I/O bytes, and memory usage for individual or groups of processes Storage: Resources' capacity and efficiency, including your devices' overall utilization, disk usage, or I/O operations Docker containers: CPU percentage, I/O bytes, and memory usage for individual or group of containers System tab Here are the default graphs shown on the Hosts page. For additional chart options, select the dropdown for any chart. CPU %: On the System tab, CPU Percent is a derived metric that is part of the SystemSample event. The CPU percentage is not collected by New Relic, but derived from several other metrics. Specifically, the cpuPercent attribute is an aggregation of cpuUserPercent, cpuSystemPercent, cpuIoWaitPercent and cpuStealPercent. Load average five minute: represents an average of the number of tasks waiting to do work on a system over the last 5 minutes. Memory free %: compares the amount of free memory bytes to the amount of used memory bytes. For explanations of different chart metrics, see Default infrastructure attributes and events. Functions for adjusting chart data include: Select different host groups: Use the host filter. Change time range: Use the time picker above the charts, or drag and select a time range on a chart. Change the number of elements on charts: use the Chart shows dropdown. Change data used to order hosts: Use the table below the charts to order the chart elements by different metrics; for example, from highest to lowest CPU user % to lowest, or from lowest to highest Disk free. System tab functions Here are some of the things you can do from the System tab: If you want to... Do this... Filter and group hosts Use filter sets to show only hosts matching certain criteria, or use group by to aggregate the results. Understand host health Use the Health column of the table. To see more details about specific violations, select the health status icons. Find root causes of issues Use the Events heatmap at the top of the page to compare performance to important events happening in your infrastructure. For more, see Events heatmap. Set an alert condition for a metric Mouse over a chart, select and then Create alert. View host's alert threshold violation If present, select the host's Critical icon or Warning icon. APM and infrastructure data If you have APM data associated with your infrastructure data, there are several ways to access your APM data on the Hosts page: Use the hosts filter to filter on hosts running specific applications. In the host list below the charts, select the Application column to filter on specific applications. From the chart selector dropdown beside a chart's name, select one of the application-related charts. Important APM charts in infrastructure monitoring do not have View query or Create alert options like the other infrastructure charts do. For more about using APM and infrastructure monitoring together, see APM data in infrastructure. Network tab The Network page provides real-time visibility into the health and performance of individual hosts, web servers, or other groups of resources across your network. Default charts show bandwidth metrics by packet, bandwidth by bytes, and errors per second. Details about individual interfaces can help you: Examine saturation levels across your network or for selected filter sets. Compare load balances between specific resources. Identify unexpected differences in capacity patterns between similar hosts. Evaluate the top five network errors that New Relic automatically presents for the selected time period. This real-time network data can then help you determine whether to resolve errors by reconfiguring your network, rebalancing the loads, or taking other preventative maintenance actions before needing to make a purchase decision. From the Network tab you can view bandwidth and error data about your network interfaces. The Network page includes an Events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. For a technical explanation of the attributes used to populate the Network page chart, see NetworkSample attributes. Network tab functions Here are some of the things you can do from the Network tab: If you want to... Do this... Filter and group Use filter sets to show only hosts matching certain criteria, or use Group by to aggregate the results. Select a time range Use the time picker to change the range of time selected. You can also click and drag on the chart to select a custom time range. When you select a time range, it carries over when you go from one infrastructure page to another. Change metrics displayed in chart Use the sorting dropdown to switch what metric the chart is displaying for the chosen process groupings, including: Received Bytes Received Dropped Received Errors Received Packets Transmitted Bytes Transmitted Dropped Transmitted Errors Transmitted Packets Search and filter the list Type in the Search interfaces search bar to filter the list to only those items containing the text you've typed. You can also filter the list by simply selecting the list item or user name you want to filter for. The chosen filters are displayed beside the filter icon above the chart. Add items to chart The chart, by default, displays the top five results. Use the Chart shows dropdown to display more results. To remove an item from a chart, select its name below the chart. Set an alert condition for a metric Mouse over a chart, select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Processes tab Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Use the Processes tab to get information about processes running on your hosts, and to set alerts on process metrics. The Processes tab shows data such as CPU percentage, I/O bytes, and memory usage for individual processes or groupings of processes. The page also includes an events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. Use this information to: Pinpoint processes that are causing performance issues. Create alerts for conditions such as CPU percentage and memory usage. On the Processes page, CPU percent is scoped to individual processes, rather than hosts. Because of this, the CPU percent metric does not take into account the resources of the entire system. Instead, it shows how much of a single CPU core each process is taking. Example Here's an example of how to pinpoint an issue and set an alert: You notice on the Hosts page that a cluster has a CPU percentage much higher than normal. You go to the Processes page and filter down to that cluster's processes. You notice that several instances of the same process have excessive CPU percentage. After fixing the issue, you decide to create an alert condition that triggers when the CPU percentage for that process type exceeds a certain threshold. For a technical explanation of the attributes used to populate the Processes page chart, see ProcessSample attributes. Tip You cannot start, stop, or restart processes from the UI. Instead, follow standard procedures to start, stop, check, or restart the infrastructure agent manually. Processes tab functions Here are some of the things you can do from the Processes tab: If you want to... Do this... Only show hosts matching certain criteria Use filter sets. Aggregate results or group by host attributes Use Group by to change how the processes are grouped in the chart and the list. The dropdown contains host-related attributes such as hostID and entityName. If you are monitoring AWS EC2 instances, EC2-related attributes such as awsRegion will be available. Select a time range Use either of these options: Select any of the time picker options at the top of the page. Click and drag on the chart to select a custom time range. After you select a time range, it carries over from one infrastructure page to another. View Docker metrics To view process usage by Docker containers, see Docker instrumentation. Change charts Select the dropdown beside the chart's name to switch what metric the chart displays. Choices include: Memory: Resident or virtual size CPU percentage Disk I/O: Read or write Search and filter the list Use either of these options: Type in the Search processes search bar. Select the list item or user name you want to filter for. The selected filters appear beside the filter icon above the chart, where you can select and remove them as needed. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Storage tab The Storage page allows you to monitor your resources' capacity and efficiency, including your devices' overall utilization, disk usage, or I/O operations. This can help you to: Examine unexpected patterns; for example, a cluster of mirrored machines that do not process I/O tasks uniformly. Monitor usage levels before disk space completely runs out. Set alert conditions to notify you about problems with one or more hosts; for example, long processing times for read or write operations, disk availability or utilization based on percentage or bytes, etc. Make informed decisions about where to redistribute hosts with higher or lower than normal processing requests. Use data to help plan for additions or changes to your infrastructure budget before an emergency arises. The Storage page includes an Events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. For a technical explanation of the attributes used to populate the Storage page chart, see StorageSample attributes. Storage tab functions Here are some of the things you can do from the Storage tab: If you want to... Do this... Filter and group Use filter sets to show only hosts matching certain criteria, or use Group by to aggregate the results. Select a time range Use the time picker on the upper right to change the range of time selected. You can also click and drag on the chart to select a custom time range. When you select a time range, it carries over when you go from one infrastructure page to another. Change metrics displayed in chart Use the sorting dropdown to switch what metric the chart is displaying for the chosen process groupings. Choices include: Total Utilization % Read Utilization % Write Utilization % Disk Used Bytes Disk Free Bytes I/O Read Operations I/O Write Operations Search and filter the list Type in the Search devices search bar to filter the list. You can also filter the list by simply selecting the list item or user name you want to filter for. The chosen filters are displayed beside the filter icon above the chart. Add/remove chart items The chart, by default, displays the top five results. Use the Chart shows dropdown to display more results. To remove an item, select its name below the chart. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Docker containers tab Use the Docker containers tab to get information about the containers running on your hosts, and to set alerts on container metrics. The Docker containers tab shows data such as CPU percentage, I/O bytes, and memory usage for individual containers or groupings of containers. The page also includes an events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. Use this information to: Identify containers that are experiencing performance issues. Create alerts for conditions such as CPU percentage and memory usage. Docker containers tab functions Here are some of the things you can do from the Docker containers tab: If you want to... Do this... Only show hosts matching certain criteria Use filter sets. Aggregate results or group by host attributes Use Group by to change how the processes are grouped in the chart and the list. The dropdown contains host-related attributes such as image and operatingSystem. Select a time range Use either of these options: Select any of the time picker options at the top of the page. Click and drag on the chart to select a custom time range. After you select a time range, it carries over from one infrastructure page to another. View Docker integration dashboard To open the Docker integration dashboard, click the Dashboard link above the data table. Change charts Select the dropdown beside the chart's name to switch what metric the chart displays. Choices include: CPU: Used cores, kernel percentage Memory: Size limit, cache bytes I/O: Write count per second, total bytes Network: Errors per second, packets Process: Process count, restart count Search and filter the list Use either of these options: Type in the Search processes search bar. Select the list item or user name you want to filter for. The selected filters appear beside the filter icon above the chart, where you can select and remove them as needed. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. View host's alert threshold violation Select the host's Critical icon or Warning icon.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.04369,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>monitoring</em> Hosts page",
        "sections": "<em>Infrastructure</em> <em>monitoring</em> Hosts page",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": "-related charts. Important APM charts in <em>infrastructure</em> <em>monitoring</em> do not have View query or Create alert options like the other <em>infrastructure</em> charts do. For more about using APM and <em>infrastructure</em> <em>monitoring</em> together, see APM data in <em>infrastructure</em>. Network tab The Network page provides real-time"
      },
      "id": "60440a6d196a675f6c960f58"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/agent-not-starting-there-are-no-logs": [
    {
      "sections": [
        "Infrastructure agent logging behavior",
        "Logging severity levels",
        "Important",
        "Log formatting",
        "Log rotation",
        "Logrotate config file sample",
        "Tip",
        "Smart verbose mode",
        "Logging before Infrastructure agent v1.4.9",
        "Integration log management",
        "Integration STDERR expected format"
      ],
      "title": "Infrastructure agent logging behavior",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot logs"
      ],
      "external_id": "0dc6570e893e47c4d5b5c4232283432926c6476a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-logs/infrastructure-agent-logging-behavior/",
      "published_at": "2021-06-20T12:36:07Z",
      "updated_at": "2021-03-16T07:31:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure agent gathers its own data as well as integrations's logs and consolidates them in a single source. By default, logs appear in standard-output and are added to a log file. To disable logs in standard output, see the agent's config options. Logging severity levels Infrastructure uses a subset of the standard Syslog severity levels: ERROR: Error conditions met WARN: Warning conditions met INFO: Informational messages DEBUG: Contains debug-level messages (useful when troubleshooting) Important DEBUG level is only shown when the verbose mode is enabled. Log formatting For infrastructure agent v1.4.9 or higher, log messages are inlined with context values. This offers better grouping and filtering; for example: containerized agent found in container containerID: VALUE Copy By default, Infrastructure logs are formatted as text: In foreground mode, log output is colored, without a timestamp: DEBUG Sending deltas divided in blocks component=PatchSender mentityKey=ohaimaci mnumberOfBlocks=1 Copy In background mode, logs are timestamped output, used when running as a service or dumping logs to a file: time=\"2019-07-12T09:54:15+02:00\" level=info msg=\"Agent service manager shutdown completed successfully.\" component=AgentService service=newrelic-infra Copy Alternatively, logs can be formatted as a JSON file: {\"context\":{},\"level\":\"info\",\"msg\":\"upstart_interval_sec: 0\",\"timestamp\":\"2019-07-11T18:24:03+02:00\"} {\"context\":{},\"level\":\"info\",\"msg\":\"plugin_dir: \",\"timestamp\":\"2019-07-11T18:24:03+02:00\"} Copy To change the log format, see the agent configuration settings. Log rotation The infrastructure agent does not provide any native log rotation or compression mechanism. Instead, we encourage you to use consolidated log rotation tools, such as the Linux logrotate tool, which is usually installed by default in most Linux distributions. Logrotate can be configured as an entry in /etc/logrotate.conf, or as a file in the /etc/logrotate.d directory. Logrotate config file sample A sample logrotate config file looks like this: /var/log/newrelic-infra/newrelic-infra.log { copytruncate compress daily dateext maxage 7 } Copy Where: /var/log/newrelic-infra/newrelic-infra.log: The Infrastructure agent log file. It must match the log_file configuration parameter in the /etc/newrelic-infra.yml file. copytruncate: Indicates that the log file is truncated but not deleted when it is rotated. This configuration option is mandatory, otherwise the log file will be deleted and won’t be recreated. compress: Compresses (usually in Gzip format) the rotated log files. daily: The agent rotates logs daily. dateext: Appends a date (by default, in the format YYYYMMDD) to the rotated log file (e.g. newrelic-infra.log-20190708.gz) maxage 7: Makes logrotate remove rotated files after 7 days. Tip For a complete description of the logrotate configuration options, see the Linux Logrotate documentation. Since logrotate is usually executed automatically as a cron job, verify that there is a logrotate entry in cron (for example, /etc/cron.daily/logrotate) similar to: #!/bin/sh /usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \"ALERT exited abnormally with [$EXITVALUE]\" fi exit 0 Copy Smart verbose mode For infrastructure agent versions 1.9.0 or higher, you can enable smart verbose mode for logs. Smart verbose mode prevents debug messages being logged until an error message is logged. Once an error has been logged, the cached debug messages are logged, but only the most recent number of configured debug messages. For example, if you have a configured limit of 10, after an error is logged, only the 10 most recent debug messages are logged, and older logs are discarded. For more information on how to enable smart verbose mode and the debug message limit, see Infrastructure configuration settings. Logging before Infrastructure agent v1.4.9 Here is a comparison of functionality for Infrastructure agent versions before and after v1.4.9: Agent v1.4.9 and higher Before v1.4.9 Foreground mode logged. The agent couldn't log some entries in foreground mode because the logging service wasn't able to write data until the agent was completely configured. Logs in text and JSON formats. Logs in text only. Logs displayed as inline text. Logs displayed as static literals in a single, decontextualized line. Integration log management Integrations write JSON payloads into STDOUT and plain-text (JSON structured in the future) logs into STDERR. The infrastructure agent handles integration STDERR lines and forward this output into the agent one, usually the service log. Agent handles each STDERR line as follows: when agent runs in verbose mode: it just forwards the full STDERR line as a DEBUG agent log entry placing integration line contexts within the ` msg ` field. otherwise: it parses the line against the expected format (see below) and only logs as agent ERROR level, entries produced by integrations with ` fatal ` or ` error ` severity levels. In this case fields are extracted and forwarded in structured manner (therefore if JSON output is enabled for the agent fields become queryable. Integration STDERR expected format A line is expected to be a list of key-value pairs separated by an equal character. Keys can contain any character, whereas values can have three different formats: string: < quote>any character including escaped quotes \\ \" < quote> map: & { any character} word: any character except spaces Internally agent used this regex to extract the fields: ([^\\s]*?)=(\".*?[^\\\\]\"|&{.*?}|[^\\s]*) Copy For instance, this line: time=\"2015-03-26T01:27:38-04:00\" level=error msg=\"Foo bar baz\" foo=bar Copy Will generate a structured agent log line with these fields: - \"time\": \"2015-03-26T01:27:38-04:00\" - \"level\": \"error\" - \"msg\": \"Foo bar baz\" - \"foo\": \"bar\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.17586,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> agent <em>logging</em> behavior",
        "sections": "<em>Infrastructure</em> agent <em>logging</em> behavior",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "New Relic&#x27;s <em>infrastructure</em> agent gathers its own data as well as integrations&#x27;s <em>logs</em> and consolidates them in a single source. By default, <em>logs</em> appear in standard-output and are added to a <em>log</em> file. To disable <em>logs</em> in standard output, see the agent&#x27;s config options. Logging severity levels"
      },
      "id": "603eb3a228ccbc6badeba7a5"
    },
    {
      "sections": [
        "Generate logs for troubleshooting the infrastructure agent",
        "Problem",
        "Important",
        "Solution",
        "Smart verbose mode",
        "Forward the agent logs to New Relic Logs",
        "Notes for specific systems",
        "Containerized agent on CoreOS"
      ],
      "title": "Generate logs for troubleshooting the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot logs"
      ],
      "external_id": "a0c2ca22e3fca2b3add8c94d211adffce686661c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-logs/generate-logs-troubleshooting-infrastructure/",
      "published_at": "2021-06-20T11:12:41Z",
      "updated_at": "2021-03-16T06:35:54Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When troubleshooting your infrastructure agent, generate verbose logs for a few minutes to find and investigate errors. This can be useful for your own troubleshooting or when working with New Relic Support. Important Verbose logging generates a lot of data very quickly. When finished generating logs, be sure to set verbose: 0 to reduce disk space consumption. If you have New Relic infrastructure agent 1.4.0 or higher, you can automate this process by using the newrelic-infra-ctl command. For more information, see the troubleshooting binary documentation. Solution Generating verbose log files requires editing your configuration file. For a sample config file that includes all applicable settings, see the example template. To generate detailed logs: Step Procedures Edit your newrelic-infra.yml file: Enable verbose logging: verbose: 1. (If you use a containerized infrastructure agent on CoreOS, see system-specific notes.) Set log_file to a convenient log file location. Restart the agent so the agent notices the new settings. Let your host run at normal load for about three minutes to generate sufficient logging data. Return your settings to default: Disable verbose logging by setting verbose: 0 in newrelic-infra.yml. Optional: Disable logging to a custom file by removing the log_file line from newrelic-infra.yml. Restart the agent so the agent notices the new settings. Examine the log file for errors. If you need to send your log file to New Relic Support: Include the line in your log file that contains the agent version: New Relic infrastructure agent version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your newrelic-infra.yml. Smart verbose mode Sometimes errors don't occur until after quite some time has passed. This makes debugging difficult, because typically verbose logs are only enabled for a short period time; otherwise there will be many debug logs. For example, if an error occurs one hour after the infrastructure agent has started, getting debug logs around the time of the error can be tricky or impractical. As of infrastructure agent v1.9.0 or higher, you can use smart verbose mode for logs. Smart verbose mode only logs the most recent debug messages after an error has been logged. This allows you to leave smart verbose mode running until an error occurs, without logging lots of irrelevant debug messages, and only logging the most recent debug messages. (The number of messages is determined by your configuration.) For more information on smart verbose mode, see the Infrastructure agent logging behavior docs, and use the Infrastructure configuration settings documentation for details on how to enable smart verbose mode. Forward the agent logs to New Relic Logs The Infrastructure agent can be configured to send its own logs to New Relic Logs. This can be useful for troubleshooting issues with log forwarding, the Infrastructure agent, or when contacting support. For details on how to enable log forwarding for the Infrastructure agent, see Troubleshoot log forwarding. Notes for specific systems These are some additional notes and requirements for specific systems, used to supplement the general logging instructions: Containerized agent on CoreOS If you are using a containerized infrastructure agent on CoreOS: Choose one of these options to change the log level to verbose: Recommended: Set the environment variable NRIA_VERBOSE to 1. Running this on the command line would look like: -e NRIA_VERBOSE=1 Copy OR Edit the config file to set verbose: 1. (Editing the config file in a container is not recommended, because it requires rebuilding the image twice: once to add verbose logging and once to remove it.) Use journalctl to collect the logs: journalctl -u newrelic-infra > newrelic-infra.log Copy Set the verbose logging level back to 0 after collecting logs for a few minutes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.16942,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Generate <em>logs</em> for <em>troubleshooting</em> the <em>infrastructure</em> agent",
        "sections": "Generate <em>logs</em> for <em>troubleshooting</em> the <em>infrastructure</em> agent",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " verbose mode. Forward the agent <em>logs</em> to New Relic <em>Logs</em> The <em>Infrastructure</em> agent can be configured to send its own <em>logs</em> to New Relic <em>Logs</em>. This can be useful for <em>troubleshooting</em> issues with <em>log</em> forwarding, the <em>Infrastructure</em> agent, or when contacting support. For details on how to enable <em>log</em> forwarding"
      },
      "id": "603e910028ccbc6304eba76d"
    },
    {
      "sections": [
        "Incorrect host name reported",
        "Problem",
        "Solution",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows",
        "Cause"
      ],
      "title": "Incorrect host name reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "d6a81c3fae24464898bea92df4c6a57945b6c731",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-host-name-reported/",
      "published_at": "2021-06-20T04:30:32Z",
      "updated_at": "2021-05-16T07:48:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example: override_hostname: correct-host.domain.com Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Cause The New Relic infrastructure agent tries to resolve its fully qualified domain name against a domain name server, which may not be properly configured or not controlled by the same user as the New Relic infrastructure agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 148.24158,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example"
      },
      "id": "6043fd9028ccbc23872c60c5"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/apm-data-missing-infrastructure": [
    {
      "sections": [
        "Incorrect host name reported",
        "Problem",
        "Solution",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows",
        "Cause"
      ],
      "title": "Incorrect host name reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "d6a81c3fae24464898bea92df4c6a57945b6c731",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-host-name-reported/",
      "published_at": "2021-06-20T04:30:32Z",
      "updated_at": "2021-05-16T07:48:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example: override_hostname: correct-host.domain.com Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Cause The New Relic infrastructure agent tries to resolve its fully qualified domain name against a domain name server, which may not be properly configured or not controlled by the same user as the New Relic infrastructure agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.43298,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example"
      },
      "id": "6043fd9028ccbc23872c60c5"
    },
    {
      "sections": [
        "Reduce the infrastructure agent's CPU footprint",
        "Problem",
        "Solution",
        "Reduce event sampling",
        "Important",
        "Reduce agent plugin reporting",
        "How to enable and disable plugins",
        "Disable SELinux semodule -l (Linux only)",
        "Reduce or disable Sysctl (Linux only)",
        "Additional plugins to reduce or disable",
        "Review on-host integrations"
      ],
      "title": "Reduce the infrastructure agent's CPU footprint",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "4eea817bfabb6b698ea3ce001b8c5eeca20d475e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint/",
      "published_at": "2021-06-20T04:30:32Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The New Relic infrastructure agent is consuming too much CPU. Solution The New Relic infrastructure agent is designed to report a broad range of system data with minimal CPU and memory consumption. However, if you have a need to reduce your CPU consumption, you can disable or decrease the sampling frequency of various samplers and plugins. This topic highlights some newrelic-infra.yml configurations that may reduce your CPU usage: Reduce event sampling Reduce agent plugin reporting Review on-host integrations Reduce event sampling The infrastructure agent reports several default events at specific frequencies. To lower the overhead, you can reduce the sampling frequency in seconds, or you can completely disable the samplers by setting the corresponding property value to -1. Important We don't recommend a sample rate larger than 60 seconds because you may see gaps in the New Relic user interface charts. The table below lists some samplers to configure: Event Sampling frequency Allow/deny list Network Network sampling rate Not available Process Process sampling rate Allow list (Windows only) Storage Storage sampling rate Deny list System System sampling rate Not available Reduce agent plugin reporting The infrastructure agent has built-in plugins that collect inventory data (specific system configuration and state information). For some systems, the CPU consumption may be relatively high if the plugins are gathering a lot of data. To reduce the footprint, you can disable or decrease the sampling frequency for specific plugins that report data you don’t want. How to enable and disable plugins Disable a single plugin: To disable a plugin, set the corresponding property value to -1. Disable all plugins: disable_all_plugins: true Enable selected plugins: To enable certain plugins, insert an exception in disable_all_plugins. For example, the following configuration disables all plugins, but the Network Interfaces plugin reports every 120 seconds: disable_all_plugins: true network_interface_interval_sec: 120 Copy Disable SELinux semodule -l (Linux only) The SELinux plugin periodically invokes the semodule -l system command to get information about the existing SELinux modules. In most CentOS/RedHat distributions, this command will generate CPU consumption peaks. To disable this functionality, insert the following configuration option in your /etc/newrelic-infra.yml file: selinux_enable_semodule: false Reduce or disable Sysctl (Linux only) The Sysctl plugin walks the whole /sys directory structure and reads values from all the files there. Disabling it or reducing the interval may decrease some CPU System time in the Infrastructure agent. You can disable inventory frequency by setting it to a negative number or reduce the frequeny by setting the sysctl_interval_sec configuration value to the number of seconds between consecutive executions of the plugin. For example, to execute the plugin once every 10 minutes: sysctl_interval_sec: 600 Copy To disable the Sysctl plugin: sysctl_interval_sec: -1 Copy The current default value for the sysctl_interval_sec property is 60. Additional plugins to reduce or disable The following inventory plugins are not especially CPU consuming, but you can still reduce their frequency or disable them by setting the corresponding configuration options. Linux plugins For configuration of these Linux plugins, see Plugin variables: Cloud Security Groups Daemon Tools DPKG Facter Kernel Modules Network interfaces RPM SELinux Supervisord Sysctl Systemd SysV Upstart Users SSHD configuration Windows plugins For configuration of these Windows plugins, see Plugin variables: Network interfaces Windows services Windows updates Review on-host integrations If you use infrastructure on-host integrations, this may have additional impacts on CPU usage. The nature of the impact and the methods to adjust the impact depend on the integration you're using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the monitoring load by adding additional infrastructure agents. For example, the Kafka integration allows a multi-agent deployment.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.51582,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "sections": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " on the integration you&#x27;re using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the <em>monitoring</em> load by adding additional <em>infrastructure</em> agents. For example, the Kafka integration allows a multi-agent deployment."
      },
      "id": "603eb9dc64441fbf1f4e8847"
    },
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2021-06-20T12:36:07Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the New Relic infrastructure agent and waited a few minutes, but no data appears in the Infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Insights, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: New Relic APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.50851,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Infrastructure</em>)",
        "sections": "No data appears (<em>Infrastructure</em>)",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " the <em>infrastructure</em> agent for the first time, the latency for data appearing in the <em>Infrastructure</em> <em>monitoring</em> UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, <em>monitor</em> the <em>Infrastructure</em> UI for a longer period before"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/https-proxy-configuration-missing": [
    {
      "sections": [
        "Incorrect host name reported",
        "Problem",
        "Solution",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows",
        "Cause"
      ],
      "title": "Incorrect host name reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "d6a81c3fae24464898bea92df4c6a57945b6c731",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-host-name-reported/",
      "published_at": "2021-06-20T04:30:32Z",
      "updated_at": "2021-05-16T07:48:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example: override_hostname: correct-host.domain.com Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Cause The New Relic infrastructure agent tries to resolve its fully qualified domain name against a domain name server, which may not be properly configured or not controlled by the same user as the New Relic infrastructure agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.43298,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example"
      },
      "id": "6043fd9028ccbc23872c60c5"
    },
    {
      "sections": [
        "Reduce the infrastructure agent's CPU footprint",
        "Problem",
        "Solution",
        "Reduce event sampling",
        "Important",
        "Reduce agent plugin reporting",
        "How to enable and disable plugins",
        "Disable SELinux semodule -l (Linux only)",
        "Reduce or disable Sysctl (Linux only)",
        "Additional plugins to reduce or disable",
        "Review on-host integrations"
      ],
      "title": "Reduce the infrastructure agent's CPU footprint",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "4eea817bfabb6b698ea3ce001b8c5eeca20d475e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint/",
      "published_at": "2021-06-20T04:30:32Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The New Relic infrastructure agent is consuming too much CPU. Solution The New Relic infrastructure agent is designed to report a broad range of system data with minimal CPU and memory consumption. However, if you have a need to reduce your CPU consumption, you can disable or decrease the sampling frequency of various samplers and plugins. This topic highlights some newrelic-infra.yml configurations that may reduce your CPU usage: Reduce event sampling Reduce agent plugin reporting Review on-host integrations Reduce event sampling The infrastructure agent reports several default events at specific frequencies. To lower the overhead, you can reduce the sampling frequency in seconds, or you can completely disable the samplers by setting the corresponding property value to -1. Important We don't recommend a sample rate larger than 60 seconds because you may see gaps in the New Relic user interface charts. The table below lists some samplers to configure: Event Sampling frequency Allow/deny list Network Network sampling rate Not available Process Process sampling rate Allow list (Windows only) Storage Storage sampling rate Deny list System System sampling rate Not available Reduce agent plugin reporting The infrastructure agent has built-in plugins that collect inventory data (specific system configuration and state information). For some systems, the CPU consumption may be relatively high if the plugins are gathering a lot of data. To reduce the footprint, you can disable or decrease the sampling frequency for specific plugins that report data you don’t want. How to enable and disable plugins Disable a single plugin: To disable a plugin, set the corresponding property value to -1. Disable all plugins: disable_all_plugins: true Enable selected plugins: To enable certain plugins, insert an exception in disable_all_plugins. For example, the following configuration disables all plugins, but the Network Interfaces plugin reports every 120 seconds: disable_all_plugins: true network_interface_interval_sec: 120 Copy Disable SELinux semodule -l (Linux only) The SELinux plugin periodically invokes the semodule -l system command to get information about the existing SELinux modules. In most CentOS/RedHat distributions, this command will generate CPU consumption peaks. To disable this functionality, insert the following configuration option in your /etc/newrelic-infra.yml file: selinux_enable_semodule: false Reduce or disable Sysctl (Linux only) The Sysctl plugin walks the whole /sys directory structure and reads values from all the files there. Disabling it or reducing the interval may decrease some CPU System time in the Infrastructure agent. You can disable inventory frequency by setting it to a negative number or reduce the frequeny by setting the sysctl_interval_sec configuration value to the number of seconds between consecutive executions of the plugin. For example, to execute the plugin once every 10 minutes: sysctl_interval_sec: 600 Copy To disable the Sysctl plugin: sysctl_interval_sec: -1 Copy The current default value for the sysctl_interval_sec property is 60. Additional plugins to reduce or disable The following inventory plugins are not especially CPU consuming, but you can still reduce their frequency or disable them by setting the corresponding configuration options. Linux plugins For configuration of these Linux plugins, see Plugin variables: Cloud Security Groups Daemon Tools DPKG Facter Kernel Modules Network interfaces RPM SELinux Supervisord Sysctl Systemd SysV Upstart Users SSHD configuration Windows plugins For configuration of these Windows plugins, see Plugin variables: Network interfaces Windows services Windows updates Review on-host integrations If you use infrastructure on-host integrations, this may have additional impacts on CPU usage. The nature of the impact and the methods to adjust the impact depend on the integration you're using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the monitoring load by adding additional infrastructure agents. For example, the Kafka integration allows a multi-agent deployment.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.51582,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "sections": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " on the integration you&#x27;re using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the <em>monitoring</em> load by adding additional <em>infrastructure</em> agents. For example, the Kafka integration allows a multi-agent deployment."
      },
      "id": "603eb9dc64441fbf1f4e8847"
    },
    {
      "sections": [
        "APM data missing from infrastructure monitoring",
        "Problem",
        "Solution",
        "Restart the app server.",
        "Make sure the hostnames are the same in APM and Infrastructure.",
        "Check for replacement host FQDN recognition problems."
      ],
      "title": "APM data missing from infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "68c926e4922c558a2ab2b0f9557f2fe7973ee0af",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/apm-data-missing-infrastructure/",
      "published_at": "2021-06-20T04:29:09Z",
      "updated_at": "2021-03-16T07:35:33Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When APM and infrastructure agents are installed on the same hosts and use the same New Relic license key, APM data should appear in Infrastructure, and vice versa. If you do not see this APM-Infrastructure linkage, follow these troubleshooting tips. Solution If you completed the APM/Infrastructure integration but do not see New Relic APM data in Infrastructure, try these troubleshooting procedures. Restart the app server. If you have not restarted your APM-monitored application in a few weeks or months, the data streams from Infrastructure and APM may not be linked. Restart your app server. Generate some traffic for your app. Wait a few minutes, and then check for APM data in infrastructure monitoring. Make sure the hostnames are the same in APM and Infrastructure. If the hostnames are different in APM and infrastructure monitoring, New Relic cannot integrate the data. One common cause for this issue is that the default hostnames are different. For example, infrastructure monitoring uses a host's FQDN (such as myhost1.example.com), while APM uses the host's name (such as myhost1). Go to one[.newrelic.com](http://one.newrelic.com) > APM > (select an app). From the app's APM Overview page, look at the app's associated host name. Compare that name with the same host's name in infrastructure monitoring. If the names are different, either set the APM agent host's display_name to match its FQDN, or set the host's display_name in Infrastructure to match the one set in APM. Check for replacement host FQDN recognition problems. If the APM-Infrastructure integration previously worked but has stopped, the server may have been replaced by another server that has the same FQDN. If both servers existed simultaneously for a period of time, New Relic cannot automatically recognize the new server. That will break the connection between APM and infrastructure data. To solve this problem, get help at support.newrelic.com. To prevent this problem, make sure there is a time gap between taking down an old server going down and creating a new server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.50912,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "sections": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem When APM and <em>infrastructure</em> agents are installed on the same hosts and use the same New Relic license key, APM data should appear in <em>Infrastructure</em>, and vice versa. If you do not see this APM-<em>Infrastructure</em> linkage, follow these <em>troubleshooting</em> tips. Solution If you completed the APM"
      },
      "id": "603e9100e7b9d2b2962a07e8"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-host-name-reported": [
    {
      "sections": [
        "Reduce the infrastructure agent's CPU footprint",
        "Problem",
        "Solution",
        "Reduce event sampling",
        "Important",
        "Reduce agent plugin reporting",
        "How to enable and disable plugins",
        "Disable SELinux semodule -l (Linux only)",
        "Reduce or disable Sysctl (Linux only)",
        "Additional plugins to reduce or disable",
        "Review on-host integrations"
      ],
      "title": "Reduce the infrastructure agent's CPU footprint",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "4eea817bfabb6b698ea3ce001b8c5eeca20d475e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint/",
      "published_at": "2021-06-20T04:30:32Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The New Relic infrastructure agent is consuming too much CPU. Solution The New Relic infrastructure agent is designed to report a broad range of system data with minimal CPU and memory consumption. However, if you have a need to reduce your CPU consumption, you can disable or decrease the sampling frequency of various samplers and plugins. This topic highlights some newrelic-infra.yml configurations that may reduce your CPU usage: Reduce event sampling Reduce agent plugin reporting Review on-host integrations Reduce event sampling The infrastructure agent reports several default events at specific frequencies. To lower the overhead, you can reduce the sampling frequency in seconds, or you can completely disable the samplers by setting the corresponding property value to -1. Important We don't recommend a sample rate larger than 60 seconds because you may see gaps in the New Relic user interface charts. The table below lists some samplers to configure: Event Sampling frequency Allow/deny list Network Network sampling rate Not available Process Process sampling rate Allow list (Windows only) Storage Storage sampling rate Deny list System System sampling rate Not available Reduce agent plugin reporting The infrastructure agent has built-in plugins that collect inventory data (specific system configuration and state information). For some systems, the CPU consumption may be relatively high if the plugins are gathering a lot of data. To reduce the footprint, you can disable or decrease the sampling frequency for specific plugins that report data you don’t want. How to enable and disable plugins Disable a single plugin: To disable a plugin, set the corresponding property value to -1. Disable all plugins: disable_all_plugins: true Enable selected plugins: To enable certain plugins, insert an exception in disable_all_plugins. For example, the following configuration disables all plugins, but the Network Interfaces plugin reports every 120 seconds: disable_all_plugins: true network_interface_interval_sec: 120 Copy Disable SELinux semodule -l (Linux only) The SELinux plugin periodically invokes the semodule -l system command to get information about the existing SELinux modules. In most CentOS/RedHat distributions, this command will generate CPU consumption peaks. To disable this functionality, insert the following configuration option in your /etc/newrelic-infra.yml file: selinux_enable_semodule: false Reduce or disable Sysctl (Linux only) The Sysctl plugin walks the whole /sys directory structure and reads values from all the files there. Disabling it or reducing the interval may decrease some CPU System time in the Infrastructure agent. You can disable inventory frequency by setting it to a negative number or reduce the frequeny by setting the sysctl_interval_sec configuration value to the number of seconds between consecutive executions of the plugin. For example, to execute the plugin once every 10 minutes: sysctl_interval_sec: 600 Copy To disable the Sysctl plugin: sysctl_interval_sec: -1 Copy The current default value for the sysctl_interval_sec property is 60. Additional plugins to reduce or disable The following inventory plugins are not especially CPU consuming, but you can still reduce their frequency or disable them by setting the corresponding configuration options. Linux plugins For configuration of these Linux plugins, see Plugin variables: Cloud Security Groups Daemon Tools DPKG Facter Kernel Modules Network interfaces RPM SELinux Supervisord Sysctl Systemd SysV Upstart Users SSHD configuration Windows plugins For configuration of these Windows plugins, see Plugin variables: Network interfaces Windows services Windows updates Review on-host integrations If you use infrastructure on-host integrations, this may have additional impacts on CPU usage. The nature of the impact and the methods to adjust the impact depend on the integration you're using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the monitoring load by adding additional infrastructure agents. For example, the Kafka integration allows a multi-agent deployment.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.51582,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "sections": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " on the integration you&#x27;re using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the <em>monitoring</em> load by adding additional <em>infrastructure</em> agents. For example, the Kafka integration allows a multi-agent deployment."
      },
      "id": "603eb9dc64441fbf1f4e8847"
    },
    {
      "sections": [
        "APM data missing from infrastructure monitoring",
        "Problem",
        "Solution",
        "Restart the app server.",
        "Make sure the hostnames are the same in APM and Infrastructure.",
        "Check for replacement host FQDN recognition problems."
      ],
      "title": "APM data missing from infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "68c926e4922c558a2ab2b0f9557f2fe7973ee0af",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/apm-data-missing-infrastructure/",
      "published_at": "2021-06-20T04:29:09Z",
      "updated_at": "2021-03-16T07:35:33Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When APM and infrastructure agents are installed on the same hosts and use the same New Relic license key, APM data should appear in Infrastructure, and vice versa. If you do not see this APM-Infrastructure linkage, follow these troubleshooting tips. Solution If you completed the APM/Infrastructure integration but do not see New Relic APM data in Infrastructure, try these troubleshooting procedures. Restart the app server. If you have not restarted your APM-monitored application in a few weeks or months, the data streams from Infrastructure and APM may not be linked. Restart your app server. Generate some traffic for your app. Wait a few minutes, and then check for APM data in infrastructure monitoring. Make sure the hostnames are the same in APM and Infrastructure. If the hostnames are different in APM and infrastructure monitoring, New Relic cannot integrate the data. One common cause for this issue is that the default hostnames are different. For example, infrastructure monitoring uses a host's FQDN (such as myhost1.example.com), while APM uses the host's name (such as myhost1). Go to one[.newrelic.com](http://one.newrelic.com) > APM > (select an app). From the app's APM Overview page, look at the app's associated host name. Compare that name with the same host's name in infrastructure monitoring. If the names are different, either set the APM agent host's display_name to match its FQDN, or set the host's display_name in Infrastructure to match the one set in APM. Check for replacement host FQDN recognition problems. If the APM-Infrastructure integration previously worked but has stopped, the server may have been replaced by another server that has the same FQDN. If both servers existed simultaneously for a period of time, New Relic cannot automatically recognize the new server. That will break the connection between APM and infrastructure data. To solve this problem, get help at support.newrelic.com. To prevent this problem, make sure there is a time gap between taking down an old server going down and creating a new server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.50912,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "sections": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem When APM and <em>infrastructure</em> agents are installed on the same hosts and use the same New Relic license key, APM data should appear in <em>Infrastructure</em>, and vice versa. If you do not see this APM-<em>Infrastructure</em> linkage, follow these <em>troubleshooting</em> tips. Solution If you completed the APM"
      },
      "id": "603e9100e7b9d2b2962a07e8"
    },
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2021-06-20T12:36:07Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the New Relic infrastructure agent and waited a few minutes, but no data appears in the Infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Insights, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: New Relic APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.5085,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Infrastructure</em>)",
        "sections": "No data appears (<em>Infrastructure</em>)",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " the <em>infrastructure</em> agent for the first time, the latency for data appearing in the <em>Infrastructure</em> <em>monitoring</em> UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, <em>monitor</em> the <em>Infrastructure</em> UI for a longer period before"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint": [
    {
      "sections": [
        "Incorrect host name reported",
        "Problem",
        "Solution",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows",
        "Cause"
      ],
      "title": "Incorrect host name reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "d6a81c3fae24464898bea92df4c6a57945b6c731",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-host-name-reported/",
      "published_at": "2021-06-20T04:30:32Z",
      "updated_at": "2021-05-16T07:48:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example: override_hostname: correct-host.domain.com Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Cause The New Relic infrastructure agent tries to resolve its fully qualified domain name against a domain name server, which may not be properly configured or not controlled by the same user as the New Relic infrastructure agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.43298,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example"
      },
      "id": "6043fd9028ccbc23872c60c5"
    },
    {
      "sections": [
        "APM data missing from infrastructure monitoring",
        "Problem",
        "Solution",
        "Restart the app server.",
        "Make sure the hostnames are the same in APM and Infrastructure.",
        "Check for replacement host FQDN recognition problems."
      ],
      "title": "APM data missing from infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "68c926e4922c558a2ab2b0f9557f2fe7973ee0af",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/apm-data-missing-infrastructure/",
      "published_at": "2021-06-20T04:29:09Z",
      "updated_at": "2021-03-16T07:35:33Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When APM and infrastructure agents are installed on the same hosts and use the same New Relic license key, APM data should appear in Infrastructure, and vice versa. If you do not see this APM-Infrastructure linkage, follow these troubleshooting tips. Solution If you completed the APM/Infrastructure integration but do not see New Relic APM data in Infrastructure, try these troubleshooting procedures. Restart the app server. If you have not restarted your APM-monitored application in a few weeks or months, the data streams from Infrastructure and APM may not be linked. Restart your app server. Generate some traffic for your app. Wait a few minutes, and then check for APM data in infrastructure monitoring. Make sure the hostnames are the same in APM and Infrastructure. If the hostnames are different in APM and infrastructure monitoring, New Relic cannot integrate the data. One common cause for this issue is that the default hostnames are different. For example, infrastructure monitoring uses a host's FQDN (such as myhost1.example.com), while APM uses the host's name (such as myhost1). Go to one[.newrelic.com](http://one.newrelic.com) > APM > (select an app). From the app's APM Overview page, look at the app's associated host name. Compare that name with the same host's name in infrastructure monitoring. If the names are different, either set the APM agent host's display_name to match its FQDN, or set the host's display_name in Infrastructure to match the one set in APM. Check for replacement host FQDN recognition problems. If the APM-Infrastructure integration previously worked but has stopped, the server may have been replaced by another server that has the same FQDN. If both servers existed simultaneously for a period of time, New Relic cannot automatically recognize the new server. That will break the connection between APM and infrastructure data. To solve this problem, get help at support.newrelic.com. To prevent this problem, make sure there is a time gap between taking down an old server going down and creating a new server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.50912,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "sections": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem When APM and <em>infrastructure</em> agents are installed on the same hosts and use the same New Relic license key, APM data should appear in <em>Infrastructure</em>, and vice versa. If you do not see this APM-<em>Infrastructure</em> linkage, follow these <em>troubleshooting</em> tips. Solution If you completed the APM"
      },
      "id": "603e9100e7b9d2b2962a07e8"
    },
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2021-06-20T12:36:07Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the New Relic infrastructure agent and waited a few minutes, but no data appears in the Infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Insights, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: New Relic APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.5085,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Infrastructure</em>)",
        "sections": "No data appears (<em>Infrastructure</em>)",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " the <em>infrastructure</em> agent for the first time, the latency for data appearing in the <em>Infrastructure</em> <em>monitoring</em> UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, <em>monitor</em> the <em>Infrastructure</em> UI for a longer period before"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/time-gaps-missing-data": [
    {
      "sections": [
        "Incorrect host name reported",
        "Problem",
        "Solution",
        "Restart the agent with SystemD",
        "Restart the agent with System V",
        "Restart the agent with Upstart",
        "Restart the agent in Windows",
        "Cause"
      ],
      "title": "Incorrect host name reported",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "d6a81c3fae24464898bea92df4c6a57945b6c731",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-host-name-reported/",
      "published_at": "2021-06-20T04:30:32Z",
      "updated_at": "2021-05-16T07:48:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The agent is working, but the infrastructure monitoring UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example: override_hostname: correct-host.domain.com Copy Use your init system to restart the agent service: Restart the agent with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: sudo systemctl restart newrelic-infra Copy Restart the agent with System V Use System V commands with Debian 7: sudo /etc/init.d/newrelic-infra restart Copy Restart the agent with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: sudo initctl restart newrelic-infra Copy Restart the agent in Windows net stop newrelic-infra net start newrelic-infra Copy Cause The New Relic infrastructure agent tries to resolve its fully qualified domain name against a domain name server, which may not be properly configured or not controlled by the same user as the New Relic infrastructure agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.43297,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The agent is working, but the <em>infrastructure</em> <em>monitoring</em> UI shows the wrong hostname. Solution To set the correct hostname, try the following steps: Edit the newrelic-infra.yml configuration file and add the override_hostname option, whose value is your expected hostname. For example"
      },
      "id": "6043fd9028ccbc23872c60c5"
    },
    {
      "sections": [
        "Reduce the infrastructure agent's CPU footprint",
        "Problem",
        "Solution",
        "Reduce event sampling",
        "Important",
        "Reduce agent plugin reporting",
        "How to enable and disable plugins",
        "Disable SELinux semodule -l (Linux only)",
        "Reduce or disable Sysctl (Linux only)",
        "Additional plugins to reduce or disable",
        "Review on-host integrations"
      ],
      "title": "Reduce the infrastructure agent's CPU footprint",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "4eea817bfabb6b698ea3ce001b8c5eeca20d475e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint/",
      "published_at": "2021-06-20T04:30:32Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The New Relic infrastructure agent is consuming too much CPU. Solution The New Relic infrastructure agent is designed to report a broad range of system data with minimal CPU and memory consumption. However, if you have a need to reduce your CPU consumption, you can disable or decrease the sampling frequency of various samplers and plugins. This topic highlights some newrelic-infra.yml configurations that may reduce your CPU usage: Reduce event sampling Reduce agent plugin reporting Review on-host integrations Reduce event sampling The infrastructure agent reports several default events at specific frequencies. To lower the overhead, you can reduce the sampling frequency in seconds, or you can completely disable the samplers by setting the corresponding property value to -1. Important We don't recommend a sample rate larger than 60 seconds because you may see gaps in the New Relic user interface charts. The table below lists some samplers to configure: Event Sampling frequency Allow/deny list Network Network sampling rate Not available Process Process sampling rate Allow list (Windows only) Storage Storage sampling rate Deny list System System sampling rate Not available Reduce agent plugin reporting The infrastructure agent has built-in plugins that collect inventory data (specific system configuration and state information). For some systems, the CPU consumption may be relatively high if the plugins are gathering a lot of data. To reduce the footprint, you can disable or decrease the sampling frequency for specific plugins that report data you don’t want. How to enable and disable plugins Disable a single plugin: To disable a plugin, set the corresponding property value to -1. Disable all plugins: disable_all_plugins: true Enable selected plugins: To enable certain plugins, insert an exception in disable_all_plugins. For example, the following configuration disables all plugins, but the Network Interfaces plugin reports every 120 seconds: disable_all_plugins: true network_interface_interval_sec: 120 Copy Disable SELinux semodule -l (Linux only) The SELinux plugin periodically invokes the semodule -l system command to get information about the existing SELinux modules. In most CentOS/RedHat distributions, this command will generate CPU consumption peaks. To disable this functionality, insert the following configuration option in your /etc/newrelic-infra.yml file: selinux_enable_semodule: false Reduce or disable Sysctl (Linux only) The Sysctl plugin walks the whole /sys directory structure and reads values from all the files there. Disabling it or reducing the interval may decrease some CPU System time in the Infrastructure agent. You can disable inventory frequency by setting it to a negative number or reduce the frequeny by setting the sysctl_interval_sec configuration value to the number of seconds between consecutive executions of the plugin. For example, to execute the plugin once every 10 minutes: sysctl_interval_sec: 600 Copy To disable the Sysctl plugin: sysctl_interval_sec: -1 Copy The current default value for the sysctl_interval_sec property is 60. Additional plugins to reduce or disable The following inventory plugins are not especially CPU consuming, but you can still reduce their frequency or disable them by setting the corresponding configuration options. Linux plugins For configuration of these Linux plugins, see Plugin variables: Cloud Security Groups Daemon Tools DPKG Facter Kernel Modules Network interfaces RPM SELinux Supervisord Sysctl Systemd SysV Upstart Users SSHD configuration Windows plugins For configuration of these Windows plugins, see Plugin variables: Network interfaces Windows services Windows updates Review on-host integrations If you use infrastructure on-host integrations, this may have additional impacts on CPU usage. The nature of the impact and the methods to adjust the impact depend on the integration you're using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the monitoring load by adding additional infrastructure agents. For example, the Kafka integration allows a multi-agent deployment.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.51582,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "sections": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " on the integration you&#x27;re using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the <em>monitoring</em> load by adding additional <em>infrastructure</em> agents. For example, the Kafka integration allows a multi-agent deployment."
      },
      "id": "603eb9dc64441fbf1f4e8847"
    },
    {
      "sections": [
        "APM data missing from infrastructure monitoring",
        "Problem",
        "Solution",
        "Restart the app server.",
        "Make sure the hostnames are the same in APM and Infrastructure.",
        "Check for replacement host FQDN recognition problems."
      ],
      "title": "APM data missing from infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "68c926e4922c558a2ab2b0f9557f2fe7973ee0af",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/apm-data-missing-infrastructure/",
      "published_at": "2021-06-20T04:29:09Z",
      "updated_at": "2021-03-16T07:35:33Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When APM and infrastructure agents are installed on the same hosts and use the same New Relic license key, APM data should appear in Infrastructure, and vice versa. If you do not see this APM-Infrastructure linkage, follow these troubleshooting tips. Solution If you completed the APM/Infrastructure integration but do not see New Relic APM data in Infrastructure, try these troubleshooting procedures. Restart the app server. If you have not restarted your APM-monitored application in a few weeks or months, the data streams from Infrastructure and APM may not be linked. Restart your app server. Generate some traffic for your app. Wait a few minutes, and then check for APM data in infrastructure monitoring. Make sure the hostnames are the same in APM and Infrastructure. If the hostnames are different in APM and infrastructure monitoring, New Relic cannot integrate the data. One common cause for this issue is that the default hostnames are different. For example, infrastructure monitoring uses a host's FQDN (such as myhost1.example.com), while APM uses the host's name (such as myhost1). Go to one[.newrelic.com](http://one.newrelic.com) > APM > (select an app). From the app's APM Overview page, look at the app's associated host name. Compare that name with the same host's name in infrastructure monitoring. If the names are different, either set the APM agent host's display_name to match its FQDN, or set the host's display_name in Infrastructure to match the one set in APM. Check for replacement host FQDN recognition problems. If the APM-Infrastructure integration previously worked but has stopped, the server may have been replaced by another server that has the same FQDN. If both servers existed simultaneously for a period of time, New Relic cannot automatically recognize the new server. That will break the connection between APM and infrastructure data. To solve this problem, get help at support.newrelic.com. To prevent this problem, make sure there is a time gap between taking down an old server going down and creating a new server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.50912,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "sections": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem When APM and <em>infrastructure</em> agents are installed on the same hosts and use the same New Relic license key, APM data should appear in <em>Infrastructure</em>, and vice versa. If you do not see this APM-<em>Infrastructure</em> linkage, follow these <em>troubleshooting</em> tips. Solution If you completed the APM"
      },
      "id": "603e9100e7b9d2b2962a07e8"
    }
  ],
  "/docs/insights-usage-ui-page": [
    {
      "sections": [
        "New Relic One pricing and billing",
        "Important",
        "How the New Relic One pricing plan works",
        "Tip",
        "Billing and usage in the UI",
        "Billing calculation details",
        "Data usage calculation",
        "Full user count billing details",
        "Data retention",
        "Billing periods",
        "Usage plan details",
        "Query and alert on usage data",
        "Free tier",
        "Non-profit use of New Relic",
        "Cancel or downgrade"
      ],
      "title": "New Relic One pricing and billing ",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One pricing and billing"
      ],
      "external_id": "03d43f14ae24579c81b601571242aef540833c8c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-pricing-billing/new-relic-one-pricing-billing/",
      "published_at": "2021-06-20T02:16:06Z",
      "updated_at": "2021-06-20T02:16:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "An explanation of how New Relic One pricing works, and how to view and manage billing. Important This document explains the New Relic One pricing plan. If you’re on our original pricing plan, see Original pricing. Not sure which you're on? See Overview of pricing. How the New Relic One pricing plan works Tip To use our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Starting July 30, 2020, all of our new customers are on a pricing plan that we call New Relic One pricing. Customers on our original pricing plan are able to transition to this pricing. For New Relic One pricing, billing is based on these factors: The amount of data ingested. 100 GBs per month is free. $0.25 per GB ingested above that. The number of provisioned full users, defined as users with access to Full Stack Observability features. Basic users are free. The cost of each full user depends on your edition: Standard, Pro, or Enterprise. Standard edition includes one full user for free, and a max of five. Pro and Enterprise give access to more account and user management features, more support, longer data retention, and other features. For Applied Intelligence, our intelligent alert/detection system: the number of incident events above the free 1000 per month. (Note that our alerting functionality is available for free and doesn't count towards this limit.) For a summary of what's included for free, see Free edition. For an overview of pricing, see our Pricing page. Keep reading for details about New Relic One pricing and billing. Billing and usage in the UI For how to view and manage billing and usage in the UI, see Pricing and billing UI. If you need more detail than the usage UI shows, you can also run queries of your usage data and set up alerts. Billing calculation details For accounts on New Relic One pricing, some high-level billing information is displayed in the UI. Here are some more details about how billing works: Data usage calculation One pricing factor is your ingested data. In this context, “ingested” refers to the data actually saved to your account after we apply various data trimming and data transformation rules. In other words, it’s not the size of the raw data sent to New Relic, but the size of the data that actually ends up stored. To view and manage your usage, go to the usage UI. Other calculation details: In the context of our pricing plan, a GB is defined as 1 billion bytes. Monthly data ingested is rounded down to the nearest integer. For example, if your account uses 100.9 GBs during a month, that’s counted as 100 GBs. For more on how data is ingested, see Manage data ingest. For how to query usage, see Query and alert on usage. Full user count billing details For accounts with New Relic One pricing, the monthly count of provisioned full users is one billing factor. To give an example: if you're on the Pro pricing edition and your organization has 100 full users during the month of January, you'd be billed for 100 full users for that month. A full user counts as a billable user the moment they're added to a New Relic organization (provisioned), whether or not that user has logged into or used New Relic yet. A user's user type is meant to be long-term setting determined by a user's expected New Relic duties and responsibilities. Because user type is a billing factor, we have restrictions around how often a full user can be downgraded to a basic user: a full user can downgrade to a basic user a maximum of two times in a rolling 12-month period. If a full user has been changed to a basic user two times in that 12-month period, that user won't be able to return to being a basic user until the start of the next 12-month period. To learn reasons for assigning one user type or another, see Tips on assigning user type. More user-related billing details: You can see your full user count in the UI. The count of full users is prorated based on the start of a New Relic subscription, or based on when a user is created as a full user or converted to a full user. For organizations on our original account/user model that have a master/sub-account structure, the count of billable users in the UI may differ from the list of users you see. For more on this, see User count discrepancy. A user can have a maximum of either three concurrent active sessions, or three unique IP addresses in use at any given time. The Standard edition of the New Relic One pricing plan includes one free full user. Users with duplicate email addresses are only counted once. For organizations on our original user model, a user may be set as a basic user in one account, and as a full user in another account. In such cases, the full user status takes precedence and that user is considered a full user. For how to query usage data, see Query and alert on usage. For more on user capabilities, see Users and roles. Data retention See Data retention. Billing periods For pay-as-you-go customers, billing occurs at the end of the month (UTC), and you can see this tracked in the UI. When you input your credit card and start to be charged, your end-of-month bill will take into account all activity (billable data usage and users) that occurred since the beginning of that month. For example: if you input your credit card in the middle of the month, and so far at that point your account has 200 GBs of usage for that month, that 200 GBs counts towards your end-of-month bill. For how to query user-related usage, see Query and alert on usage. Usage plan details There are two New Relic One pricing usage plans: Pay-as-you-go: This plan bills at the end of each month. There are no commitments and you can cancel at any time. For details, see Usage plans. Annual pool of funds: This plan applies to some customers who have subscribed for a year or more. For details, see Usage plans. For some frequently asked questions, see Pricing FAQs. Query and alert on usage data To create detailed queries of your usage, and get notifications when you are close to hitting certain usage levels, see Query usage data. Free tier If your organization is on New Relic One pricing and on the Standard pricing edition for Full Stack Observability, you can use New Relic free, forever, if you stay under the free allowed limits. Here's a summary of what Standard edition gets access to for free: A single account (Pro and Enterprise Full Stack Observability editions can have multiple accounts per organization). Up to 100 GBs of ingested data per month. One full user, and unlimited basic users. Access to alerts and Applied Intelligence (up to 1,000 Incident Intelligence events per month). To upgrade to Pro or Enterprise, or to learn more about pricing, see New Relic pricing. Non-profit use of New Relic If you’re a non-profit and want to use New Relic at special pricing, see our Non-profit eligibility docs. Cancel or downgrade See Downgrade account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.20961,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Billing and <em>usage</em> in the <em>UI</em>",
        "body": "&#x27;s included for free, see Free edition. For an overview of pricing, see our Pricing <em>page</em>. Keep reading for details about New Relic One pricing and billing. Billing and <em>usage</em> in the <em>UI</em> For how to view and manage billing and <em>usage</em> in the <em>UI</em>, see Pricing and billing <em>UI</em>. If you need more detail than the <em>usage</em> <em>UI</em>"
      },
      "id": "6043f69a64441f7b26378eda"
    },
    {
      "sections": [
        "Versions analysis",
        "Versions analysis details",
        "Viewing drill-down details",
        "How version numbers are obtained",
        "Android",
        "iOS",
        "For more help"
      ],
      "title": "Versions analysis",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "Mobile monitoring UI",
        "Usage pages"
      ],
      "external_id": "3906aba3231864c2adb43694636f085ae5332d0e",
      "image": "https://docs.newrelic.com/static/f359bb98f6fbf2a5c90dd604778a5dcd/c1b63/screen-versions_0.png",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/mobile-monitoring-ui/usage-pages/versions-analysis/",
      "published_at": "2021-06-20T23:31:15Z",
      "updated_at": "2021-05-16T04:04:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Mobile includes a comparative analysis of adoption and performance between versions of your application, including top versions by: Interaction time Active sessions Error rate The Versions page also includes a table comparing each version by date created, average memory, average CPU, average sessions per minute, and average requests per minute (RPM) per active app. You can also drill down into additional details about a specific version. Versions analysis details one.newrelic.com > Mobile > (select an app) > Usage > Versions: The Versions analysis includes color-coded charts of mobile app usage, plus a table that summarizes mobile app versions and their averages for memory, CPU, active users, and network RPM (requests per minute). The Versions page provides a list of all versions of your app that have been detected, plus overview information on all versions active in the last seven days. To view the comparative analysis: Go to one.newrelic.com > Mobile > (select an app) > Usage > Versions. To select the time period, use the time picker below the New Relic menu bar. Optional: Select the Sort by options. To view details only for a specific version, select its name. The Versions page provides a list of all versions of your app that have been active in the selected time window. Use any of New Relic's standard user interface functions and page functions to drill down into detailed information. Viewing drill-down details one.newrelic.com > Mobile > (select an app) > Usage > Versions > (selected version): Here is an example of details for a selected version. The details page provides further insight into how the selected version compares to a reference version (a recent or popular version), and the average of other versions of your app. Time series show the comparison across error rate, response time, active sessions, and memory usage. To exit the details page, select the Close (X) button. How version numbers are obtained The way that New Relic Mobile obtains the version number varies by platform: Android The Android agent obtains the version information from the android:versionName property in the manifest. iOS The iOS agent uses both CFBundleShortVersionString and CFBundleVersion properties to obtain the app version. The agent accesses those properties through iOS APIs. It does not obtain them by reading the info.plist file. For more help Additional documentation resources include: Devices page (performance details about the top devices using your mobile application, including comparisons by interaction time, HTTP request time, network failures, and active sessions) OS versions page (performance details about the top operating system versions using your mobile application)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 96.52519,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Usage</em> <em>pages</em>",
        "body": ") &gt; <em>Usage</em> &gt; Versions &gt; (selected version): Here is an example of details for a selected version. The details <em>page</em> provides further <em>insight</em> into how the selected version compares to a reference version (a recent or popular version), and the average of other versions of your app. Time series show"
      },
      "id": "603eaeeae7b9d262be2a080c"
    },
    {
      "sections": [
        "Monthly uniques report",
        "Monthly uniques report details",
        "Device tracking",
        "For more help"
      ],
      "title": "Monthly uniques report",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "Mobile monitoring UI",
        "Usage pages"
      ],
      "external_id": "8865375eef0e6bab4c0b40fa9edde33da93a752c",
      "image": "https://docs.newrelic.com/static/14ffff087d1c8f0d7c26b59739057f07/c1b63/screen-mobile-monthly-uniques.png",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/mobile-monitoring-ui/usage-pages/monthly-uniques-report/",
      "published_at": "2021-06-20T23:31:15Z",
      "updated_at": "2021-05-15T18:57:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Mobile includes a monthly report with a bar chart tracking the number of devices running your app for each month over the last year. To view the report: Go to one.newrelic.com > Mobile > (select an app) > Usage > Monthly uniques. Monthly uniques report details To see the total number of unique devices for any month, mouse over the month's bar in the chart. The current month's device count is a month-to-date value and does not indicate the full month's usage. one.newrelic.com > Mobile > (select an app) > Usage > Monthly uniques: This report provides a bar chart tracking the number of devices running your app for each month over the last year. Use any of New Relic's standard user interface functions and page functions to drill down into detailed information. Unique users are used to calculate your monthly usage, which is used to calculate your subscription level. Device tracking New Relic Mobile does not use hardware identifiers for unique install tracking. On iOS we use the IdentifierForVendor property. (Versions 5.3.4 and lower used the SecureUDID library.) On Android we generate a unique GUID when the application is installed. For more help Additional documentation resources include the Devices page (performance details about the top devices using your mobile application, including comparisons by HTTP request time, active users, and network failures).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 96.35455,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Usage</em> <em>pages</em>",
        "body": " of devices running your app for each month over the last year. Use any of New Relic&#x27;s standard user interface functions and <em>page</em> functions to drill down into detailed information. Unique users are used to calculate your monthly <em>usage</em>, which is used to calculate your subscription level. Device tracking"
      },
      "id": "6044141964441f5cb1378f32"
    }
  ],
  "/docs/instrument-errors-c-sdk": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/release-notes/agent-release-notes/c-sdk-release-notes/c-sdk-100/",
      "sections": [
        "C SDK v1.0.0",
        "New Features",
        "End of Life Notice"
      ],
      "published_at": "2021-06-20T13:02:27Z",
      "title": "C SDK v1.0.0",
      "updated_at": "2021-03-16T12:13:41Z",
      "type": "docs",
      "external_id": "bc68847dddf3b5226d442626a141c3090bbb2809",
      "document_type": "release_notes",
      "popularity": 1,
      "body": "New Features This is the first release of the New Relic C SDK! If your application does not use other New Relic APM agent languages, you can use the C SDK to take advantage of New Relic's monitoring capabilities and features to instrument a wide range of applications. For more information, see: Documentation: How to get started with the C SDK, install and configure it, instrument transactions, segments, and errors, use the C SDK API, and do some basic troubleshooting. GitHub: SDK files, data structure, field definitions and parameters, code examples, functions, variables. End of Life Notice The previous APM Agent SDK is deprecated beta software. If you were previously using the Agent SDK, you can switch to the C SDK. Check the compatibility and requirements, and then instrument, compile and link your application's code to use the C SDK. The C SDK currently does not support New Relic's HSM feature; this may impact how you schedule your transition away from the Agent SDK.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 879.06604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>C</em> <em>SDK</em> v1.0.0",
        "sections": "<em>C</em> <em>SDK</em> v1.0.0",
        "body": ": Documentation: How to get started with the <em>C</em> <em>SDK</em>, install and configure it, <em>instrument</em> transactions, segments, and <em>errors</em>, use the <em>C</em> <em>SDK</em> API, and do some basic troubleshooting. GitHub: <em>SDK</em> files, data structure, field definitions and parameters, code examples, functions, variables. End of Life Notice"
      },
      "id": "603ec0fae7b9d223882a07b9"
    },
    {
      "sections": [
        "Instrument your app with the C SDK",
        "Instrument a transaction",
        "Instrument segments",
        "Instrument calls to external services",
        "Instrument calls to arbitrary code (custom segments)",
        "Instrument calls to datastores",
        "Tip",
        "Report slow query traces for datastore segments (SQL only)",
        "Important",
        "Instrument errors",
        "Avoid metric grouping issues"
      ],
      "title": "Instrument your app with the C SDK",
      "type": "docs",
      "tags": [
        "Agents",
        "C SDK",
        "Instrumentation"
      ],
      "external_id": "dc21642bac9d779820a40eea8601434c4242f425",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/c-sdk/instrumentation/instrument-your-app-c-sdk/",
      "published_at": "2021-06-20T03:08:42Z",
      "updated_at": "2021-03-16T13:43:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to monitor any application on Linux using a language that can import C libraries, you must: Create a config using newrelic_new_app_config(), connect to the daemon using newrelic_init(), and connect your application using newrelic_create_app(). For more information, see the C SDK installation procedures. Manually instrument transactions using the C SDK, as described in this document. New Relic defines a web or non-web transaction as one logical unit of work in a software application. After you manually instrument transactions in your source code by adding New Relic functions, you can view the data on the Transactions page in New Relic. You can also instrument segments of a transaction and errors. Instrument a transaction To instrument a transaction so you can monitor it, wrap the New Relic functions that start and stop instrumentation around the transaction. The function that you use depends on whether you want to instrument a web or non-web transaction. In the following example, the app is created after a call to newrelic_create_app(). For more information, see the C SDK installation procedures as well as the C SDK libnewrelic.h documentation on GitHub. Add the following code immediately before the transaction that you want to monitor, supplying the required parameters. For web transactions: // Example code: newrelic_txn_t *txn; /* ... */ txn = newrelic_start_web_transaction(app, \"NAME_YOUR_TRANSACTION\"); Copy For non-web transactions: // Example code: newrelic_txn_t *txn; /* ... */ txn = newrelic_start_non_web_transaction(app, \"NAME_YOUR_TRANSACTION\"); Copy Add newrelic_end_transaction() immediately after the web or non-web transaction that you want to monitor, supplying a pointer the transaction, &txn, as a parameter. Instrument segments Once you instrument a transaction using the C SDK, you can instrument segments in it. By instrumenting segments, you can monitor the individual functions and calls inside a transaction. Segments example You have a transaction associated with a checkout process, which processes both shipping information and credit card information. You can instrument your application to break that transaction up into two segments: one segment for shipping and one segment for payment. You can instrument segments to monitor the following kinds of calls: External services using external segments Custom segments for arbitrary code Datastores using datastore segments Slow query traces (SQL databases only) For more information, see the C SDK installation procedures as well as the C SDK libnewrelic.h documentation on GitHub. Instrument calls to external services To monitor calls to external services, instrument external segments that are within an instrumented transaction. External segments appear in the Transactions page's Breakdown table and the External services page. To instrument an external segment, wrap the New Relic functions that start and stop instrumentation around the function you want to monitor: Instrument a transaction. Create a newrelic_external_segment_params_t that describes the external segment, supplying the required parameters. Add newrelic_start_external_segment() immediately before the function you want to monitor, supplying the required parameters. Add newrelic_end_segment() immediately after the function you want to monitor, supplying the required parameters. For more information, see the C SDK installation procedures as well as the C SDK libnewrelic.h documentation on GitHub. Instrument calls to arbitrary code (custom segments) To monitor calls to arbitrary code, instrument custom segments that are within an instrumented transaction. Custom segments appear in the Breakdown table on the Transactions page. To instrument a custom segment, wrap the New Relic functions that start and stop instrumentation around the function you want to monitor: Instrument a transaction. Add newrelic_start_segment() immediately before the function you want to monitor, supplying the required parameters. Add newrelic_end_segment() immediately after the function you want to monitor, supplying the required parameters. For more information, see the C SDK installation procedures as well as the C SDK libnewrelic.h documentation on GitHub. Instrument calls to datastores To monitor calls to datastores, instrument the datastore segments within an instrumented transaction. Datastore segments appear in the Breakdown table and Databases tab on the Transactions page in New Relic. You can also view datastore segments as a databaseDuration attribute of APM Transaction events. To instrument a datastore segment, wrap the New Relic functions that start and stop instrumentation around the function you want to monitor: Instrument a transaction. Create a newrelic_datastore_segment_params_t that describes the datastore segment. Add newrelic_start_datastore_segment() immediately before the function you want to monitor, supplying the required parameters. Add newrelic_end_segment() immediately after the function you want to monitor, supplying the required parameters. For more information, see the C SDK installation procedures as well as the C SDK libnewrelic.h documentation on GitHub. Tip To configure how the database name and database instance are reported, use the newrelic_datastore_segment_config_t. Report slow query traces for datastore segments (SQL only) Important You can report slow query traces for SQL databases only. To report slow query trace data for datastore segments that take longer than the time you specify, enable these settings in your newrelic_app_config_t: Enable slow query tracing by setting transaction_tracer.datastore_reporting.enabled to true. To set the threshold, add a length of time in microseconds to transaction_tracer.datastore_reporting.threshold_us. Then, if a datastore call takes longer than the threshold, the C SDK reports it as a slow query. To view slow query trace details, use the Databases and Slow queries pages in New Relic. For more information, see the C SDK installation procedures as well as the C SDK libnewrelic.h documentation on GitHub. Instrument errors In order to use the C SDK to monitor errors in transactions, you must manually instrument your source code by adding the newrelic_notice_error() function to it. Transaction errors and error traces appear on the Error analytics page in New Relic. The C SDK reports the total number of errors and up to 100 error traces per minute. You can also view, query, and visualize transaction errors as APM TransactionError events. Tip To include function calls in error traces, use GNU's -rdynamic linker flag to link your apps when compiling. The -rdynamic linker flag gives you more meaningful error traces. To instrument errors in transactions: Start a transaction. Record an error with newrelic_notice_error(), supplying the required parameters. End the transaction, supplying the required parameters. For more information, see the C SDK installation procedures as well as the C SDK libnewrelic.h documentation on GitHub. Avoid metric grouping issues When an account or application sends many individual metrics that could be better managed in groups, New Relic uses the term metric grouping issue or MGI to describe this situation. If your application sends unnecessarily large amounts of data to New Relic, this reduces the effectiveness of charts, tables, and reports. Metric grouping issues occur most commonly with web transactions, especially if the name is based on URLs. To help prevent this situation, see Metric grouping issues.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 386.88705,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Instrument</em> your app <em>with</em> <em>the</em> <em>C</em> <em>SDK</em>",
        "sections": "<em>Instrument</em> your app <em>with</em> <em>the</em> <em>C</em> <em>SDK</em>",
        "tags": "<em>C</em> <em>SDK</em>",
        "body": " the <em>C</em> <em>SDK</em> installation procedures as well as the <em>C</em> <em>SDK</em> libnewrelic.h documentation on GitHub. <em>Instrument</em> <em>errors</em> In order to use the <em>C</em> <em>SDK</em> to monitor <em>errors</em> in transactions, you must manually <em>instrument</em> your source code by adding the newrelic_notice_<em>error</em>() function to it. Transaction <em>errors</em> and <em>error</em>"
      },
      "id": "603ec08fe7b9d229232a0810"
    },
    {
      "sections": [
        "Manage errors in APM: Collect, ignore, or mark as expected",
        "Collect errors not instrumented by default",
        "Ignore errors",
        "Tip",
        "Ignore errors using server-side configuration in the UI",
        "Ignore errors using agent configuration",
        "Expected errors (Java, Node.js, Python, Ruby, and .NET only)",
        "View errors in the UI"
      ],
      "title": "Manage errors in APM: Collect, ignore, or mark as expected",
      "type": "docs",
      "tags": [
        "Agents",
        "Manage APM agents",
        "Agent data"
      ],
      "external_id": "ebd20f78d5084c48b1669ae33569da3f829fec6b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/manage-apm-agents/agent-data/manage-errors-apm-collect-ignore-or-mark-expected/",
      "published_at": "2021-06-20T04:57:42Z",
      "updated_at": "2021-05-27T19:09:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "APM agents automatically report error data for supported frameworks. To optimize error reporting and alerting, you can further manage errors in order to: Catch errors that we don't instrument by default. Ignore errors that you don't want reported at all. Filter out noise from expected errors so you can focus on the errors that are affecting performance. (Java, Ruby, Node, Python, and .NET agents only) Collect errors not instrumented by default APM agents include API calls to report (or \"notice\") errors. These are useful when APM doesn't instrument your framework automatically or when there are particular errors that aren't caught for your supported framework. To learn how to get an APM agent to report an error, see the agent-specific API documentation: C SDK: newrelic_notice_error() Go: NoticeError() Java: NoticeError() .NET: NoticeError() Node.js: noticeError() PHP: newrelic_notice_error() Python: notice_error() Ruby: notice_error() Ignore errors Sometimes the APM agent instruments an error that you don't want reported, such as errors that contain sensitive information like user login errors. If you don't want an error to report to the our collector, you can ignore the error, and the APM agent discards the error entirely. Tip For Java, Ruby, Node.js, and Python: If you want to report errors to APM but don't want those errors to affect your Apdex or error rate, mark them as expected instead. There are two ways to ignore errors: through the agent configuration or through server-side configuration in the UI: Ignore errors using server-side configuration in the UI This option depends on whether the agent supports server-side configuration. If it is not already enabled, enable server-side configuration. Go to the Server-side configuration menu for the application that has errors that you want to ignore. Under Error collection, look for Ignore from error collection. Add the HTTP code or the Error class for the errors that you want to ignore. Select Save server-side configuration. Ignore errors using agent configuration To ignore an error using the agent configuration, see the configuration documentation for your agent: C SDK: Not available. For more information, see the C SDK errors example on GitHub. Go: ErrorCollector.IgnoreStatusCodes. Java: error_collector.ignore_classes, error_collector.ignore_classes.message, or error_collector.ignore_status_codes. For additional information, see Java agent error configuration. .NET: ignoreErrors or ignoreStatusCodes. Node.js: ignore_status_codes, ignore_classes, or ignore_messages. PHP: error_collector.ignore_exceptions or error_collector.ignore_errors. Python: error_collector.ignore_classes or error_collector.ignore_status_codes. Ruby: error_collector.ignore_errors. Expected errors (Java, Node.js, Python, Ruby, and .NET only) For the Ruby and Java agents, you can mark errors as expected. These errors will be reported to APM and available for viewing, but they won't affect the Apdex or error rate (or alert conditions based on error rate). To configure errors as expected, see the agent-specific documentation: Java Ruby Node.js .NET Python If expected errors are enabled, APM's Error analytics page will, by default, have a filter applied with the error.expected attribute set to false, meaning expected errors will not be displayed. To view expected errors, turn off the error.expected filter. To view expected errors, query your data: To view charts of expected errors, create a query for the error.expected attribute. To create alert conditions for NRQL queries, use the error.expected attribute. View errors in the UI Among other places, error data appears in these parts of the UI: Error analytics page: shows in-depth charts and visual analysis of errors. APM Overview page: shows a high-level view of your application, which includes errors. Alert conditions: can be based on error rate. The transactionError event: contains underlying error data, which can be used in NRQL queries.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 122.25795,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Manage <em>errors</em> in APM: <em>Collect</em>, ignore, or mark as expected",
        "sections": "<em>Collect</em> <em>errors</em> not <em>instrumented</em> by default",
        "body": " there are particular <em>errors</em> that aren&#x27;t caught for your supported framework. To learn how to get an APM agent to report an <em>error</em>, see the agent-specific API documentation: <em>C</em> <em>SDK</em>: newrelic_notice_<em>error</em>() Go: Notice<em>Error</em>() Java: Notice<em>Error</em>() .NET: Notice<em>Error</em>() Node.js: notice<em>Error</em>() PHP"
      },
      "id": "6043f7d6196a67c1d4960f72"
    }
  ],
  "/docs/instrument-transactions-c-sdk": [
    {
      "sections": [
        "Instrument your app with the C SDK",
        "Instrument a transaction",
        "Instrument segments",
        "Instrument calls to external services",
        "Instrument calls to arbitrary code (custom segments)",
        "Instrument calls to datastores",
        "Tip",
        "Report slow query traces for datastore segments (SQL only)",
        "Important",
        "Instrument errors",
        "Avoid metric grouping issues"
      ],
      "title": "Instrument your app with the C SDK",
      "type": "docs",
      "tags": [
        "Agents",
        "C SDK",
        "Instrumentation"
      ],
      "external_id": "dc21642bac9d779820a40eea8601434c4242f425",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/c-sdk/instrumentation/instrument-your-app-c-sdk/",
      "published_at": "2021-06-20T03:08:42Z",
      "updated_at": "2021-03-16T13:43:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to monitor any application on Linux using a language that can import C libraries, you must: Create a config using newrelic_new_app_config(), connect to the daemon using newrelic_init(), and connect your application using newrelic_create_app(). For more information, see the C SDK installation procedures. Manually instrument transactions using the C SDK, as described in this document. New Relic defines a web or non-web transaction as one logical unit of work in a software application. After you manually instrument transactions in your source code by adding New Relic functions, you can view the data on the Transactions page in New Relic. You can also instrument segments of a transaction and errors. Instrument a transaction To instrument a transaction so you can monitor it, wrap the New Relic functions that start and stop instrumentation around the transaction. The function that you use depends on whether you want to instrument a web or non-web transaction. In the following example, the app is created after a call to newrelic_create_app(). For more information, see the C SDK installation procedures as well as the C SDK libnewrelic.h documentation on GitHub. Add the following code immediately before the transaction that you want to monitor, supplying the required parameters. For web transactions: // Example code: newrelic_txn_t *txn; /* ... */ txn = newrelic_start_web_transaction(app, \"NAME_YOUR_TRANSACTION\"); Copy For non-web transactions: // Example code: newrelic_txn_t *txn; /* ... */ txn = newrelic_start_non_web_transaction(app, \"NAME_YOUR_TRANSACTION\"); Copy Add newrelic_end_transaction() immediately after the web or non-web transaction that you want to monitor, supplying a pointer the transaction, &txn, as a parameter. Instrument segments Once you instrument a transaction using the C SDK, you can instrument segments in it. By instrumenting segments, you can monitor the individual functions and calls inside a transaction. Segments example You have a transaction associated with a checkout process, which processes both shipping information and credit card information. You can instrument your application to break that transaction up into two segments: one segment for shipping and one segment for payment. You can instrument segments to monitor the following kinds of calls: External services using external segments Custom segments for arbitrary code Datastores using datastore segments Slow query traces (SQL databases only) For more information, see the C SDK installation procedures as well as the C SDK libnewrelic.h documentation on GitHub. Instrument calls to external services To monitor calls to external services, instrument external segments that are within an instrumented transaction. External segments appear in the Transactions page's Breakdown table and the External services page. To instrument an external segment, wrap the New Relic functions that start and stop instrumentation around the function you want to monitor: Instrument a transaction. Create a newrelic_external_segment_params_t that describes the external segment, supplying the required parameters. Add newrelic_start_external_segment() immediately before the function you want to monitor, supplying the required parameters. Add newrelic_end_segment() immediately after the function you want to monitor, supplying the required parameters. For more information, see the C SDK installation procedures as well as the C SDK libnewrelic.h documentation on GitHub. Instrument calls to arbitrary code (custom segments) To monitor calls to arbitrary code, instrument custom segments that are within an instrumented transaction. Custom segments appear in the Breakdown table on the Transactions page. To instrument a custom segment, wrap the New Relic functions that start and stop instrumentation around the function you want to monitor: Instrument a transaction. Add newrelic_start_segment() immediately before the function you want to monitor, supplying the required parameters. Add newrelic_end_segment() immediately after the function you want to monitor, supplying the required parameters. For more information, see the C SDK installation procedures as well as the C SDK libnewrelic.h documentation on GitHub. Instrument calls to datastores To monitor calls to datastores, instrument the datastore segments within an instrumented transaction. Datastore segments appear in the Breakdown table and Databases tab on the Transactions page in New Relic. You can also view datastore segments as a databaseDuration attribute of APM Transaction events. To instrument a datastore segment, wrap the New Relic functions that start and stop instrumentation around the function you want to monitor: Instrument a transaction. Create a newrelic_datastore_segment_params_t that describes the datastore segment. Add newrelic_start_datastore_segment() immediately before the function you want to monitor, supplying the required parameters. Add newrelic_end_segment() immediately after the function you want to monitor, supplying the required parameters. For more information, see the C SDK installation procedures as well as the C SDK libnewrelic.h documentation on GitHub. Tip To configure how the database name and database instance are reported, use the newrelic_datastore_segment_config_t. Report slow query traces for datastore segments (SQL only) Important You can report slow query traces for SQL databases only. To report slow query trace data for datastore segments that take longer than the time you specify, enable these settings in your newrelic_app_config_t: Enable slow query tracing by setting transaction_tracer.datastore_reporting.enabled to true. To set the threshold, add a length of time in microseconds to transaction_tracer.datastore_reporting.threshold_us. Then, if a datastore call takes longer than the threshold, the C SDK reports it as a slow query. To view slow query trace details, use the Databases and Slow queries pages in New Relic. For more information, see the C SDK installation procedures as well as the C SDK libnewrelic.h documentation on GitHub. Instrument errors In order to use the C SDK to monitor errors in transactions, you must manually instrument your source code by adding the newrelic_notice_error() function to it. Transaction errors and error traces appear on the Error analytics page in New Relic. The C SDK reports the total number of errors and up to 100 error traces per minute. You can also view, query, and visualize transaction errors as APM TransactionError events. Tip To include function calls in error traces, use GNU's -rdynamic linker flag to link your apps when compiling. The -rdynamic linker flag gives you more meaningful error traces. To instrument errors in transactions: Start a transaction. Record an error with newrelic_notice_error(), supplying the required parameters. End the transaction, supplying the required parameters. For more information, see the C SDK installation procedures as well as the C SDK libnewrelic.h documentation on GitHub. Avoid metric grouping issues When an account or application sends many individual metrics that could be better managed in groups, New Relic uses the term metric grouping issue or MGI to describe this situation. If your application sends unnecessarily large amounts of data to New Relic, this reduces the effectiveness of charts, tables, and reports. Metric grouping issues occur most commonly with web transactions, especially if the name is based on URLs. To help prevent this situation, see Metric grouping issues.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 947.5017,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Instrument</em> your app <em>with</em> <em>the</em> <em>C</em> <em>SDK</em>",
        "sections": "<em>Instrument</em> your app <em>with</em> <em>the</em> <em>C</em> <em>SDK</em>",
        "tags": "<em>C</em> <em>SDK</em>",
        "body": " installation procedures. Manually <em>instrument</em> <em>transactions</em> using the <em>C</em> <em>SDK</em>, as described in this document. New Relic defines a web or non-web transaction as one logical unit of work in a software application. After you manually <em>instrument</em> <em>transactions</em> in your source code by adding New Relic functions, you can"
      },
      "id": "603ec08fe7b9d229232a0810"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/release-notes/agent-release-notes/c-sdk-release-notes/c-sdk-100/",
      "sections": [
        "C SDK v1.0.0",
        "New Features",
        "End of Life Notice"
      ],
      "published_at": "2021-06-20T13:02:27Z",
      "title": "C SDK v1.0.0",
      "updated_at": "2021-03-16T12:13:41Z",
      "type": "docs",
      "external_id": "bc68847dddf3b5226d442626a141c3090bbb2809",
      "document_type": "release_notes",
      "popularity": 1,
      "body": "New Features This is the first release of the New Relic C SDK! If your application does not use other New Relic APM agent languages, you can use the C SDK to take advantage of New Relic's monitoring capabilities and features to instrument a wide range of applications. For more information, see: Documentation: How to get started with the C SDK, install and configure it, instrument transactions, segments, and errors, use the C SDK API, and do some basic troubleshooting. GitHub: SDK files, data structure, field definitions and parameters, code examples, functions, variables. End of Life Notice The previous APM Agent SDK is deprecated beta software. If you were previously using the Agent SDK, you can switch to the C SDK. Check the compatibility and requirements, and then instrument, compile and link your application's code to use the C SDK. The C SDK currently does not support New Relic's HSM feature; this may impact how you schedule your transition away from the Agent SDK.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 895.69135,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>C</em> <em>SDK</em> v1.0.0",
        "sections": "<em>C</em> <em>SDK</em> v1.0.0",
        "body": ": Documentation: How to get started with the <em>C</em> <em>SDK</em>, install and configure it, <em>instrument</em> <em>transactions</em>, segments, and errors, use the <em>C</em> <em>SDK</em> API, and do some basic troubleshooting. GitHub: <em>SDK</em> files, data structure, field definitions and parameters, code examples, functions, variables. End of Life Notice"
      },
      "id": "603ec0fae7b9d223882a07b9"
    },
    {
      "sections": [
        "Guide to using the C SDK API",
        "Ensure your customization is thread-safe",
        "Monitor transactions",
        "Time specific methods using segments",
        "Enhance the metadata of a transaction",
        "Instrument calls to external services",
        "Collect or log errors",
        "Send custom data from your app",
        "Custom events",
        "Tip",
        "Custom event attributes",
        "Custom metrics",
        "Important",
        "Monitor desktop browser performance",
        "Change configuration settings"
      ],
      "title": "Guide to using the C SDK API",
      "type": "docs",
      "tags": [
        "Agents",
        "C SDK",
        "Instrumentation"
      ],
      "external_id": "fd96697be408715e6330a91b237c5fb6b5042bce",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/c-sdk/instrumentation/guide-using-c-sdk-api/",
      "published_at": "2021-06-20T03:08:42Z",
      "updated_at": "2021-03-16T09:07:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's C SDK monitors your applications and microservices to help you identify and solve performance issues. C applications run from a compiled, native binary file. In order to monitor transactions, you must manually instrument your code by adding New Relic methods to it. This guide helps you to decide which method to use. The method's code, required parameters, and examples reside in New Relic's C SDK documentation on GitHub. Ensure your customization is thread-safe The C SDK supports instrumentation of multi-threaded applications, but it must be initialized before instrumenting multiple threads. When calling any of the following functions, ensure that they are called on the main thread before any other C SDK functions are called: newrelic_configure_log newrelic_init Monitor transactions Before you manually instrument your code to monitor transactions: Check the C SDK compatibility and requirements for your app. Make sure you are using the latest version of the C SDK library, and update as needed. If you want to... Use this method... Start timing a web transaction newrelic_start_web_transaction() Start timing a non-web transaction newrelic_start_non_web_transaction() Stop timing a transaction newrelic_end_transaction() Prevent a transaction from reporting to New Relic newrelic_ignore_transaction() Time specific methods using segments If a transaction is already visible in New Relic, but you do not have enough data about a particular method that was called during that transaction, you can instrument segments. For example, if you want to time a method that has complex logic, you can create a segment for each of the methods in the transaction. To instrument a method within an existing transaction, create segments for any of the following: External services Functions or other arbitrary blocks of code (using custom segments) Datastores Slow query traces (SQL datastores only) If you want to... Use this method... Start timing a segment newrelic_start_datastore_segment()newrelic_start_external_segment()newrelic_start_segment() Stop timing a segment newrelic_end_segment() Manually parent segments newrelic_set_segment_parent() and newrelic_set_segment_parent_root() This is useful, for example, with an asynchronous process when you want to visualize a segment as a child of the transaction's top-level call. For more information, see the manual segment parenting documentation on GitHub. Segments are recorded on the active transaction. When adding a segment to an active transaction, you need access to the newrelic_txn_t* or transaction pointer, returned by newrelic_start_web_transaction() or newrelic_start_non_web_transaction(). Enhance the metadata of a transaction You can manage the metadata that New Relic reports for transactions. This is useful when you want a different level of detail for your transactions. For example: If you are experiencing a metric grouping issue, you can change the default names for your transactions to make them more identifiable. If you want to create dashboards for your transactions, you can add custom attributes. If you want to... Use this method... Add metadata (such as your customer's account name or subscription level) to your transaction Add custom attributes to your transaction based on their type: newrelic_add_attribute_int() newrelic_add_attribute_string() newrelic_add_attribute_long() newrelic_add_attribute_double() Instrument calls to external services Use these methods to collect data about your app's connections to other apps or databases: If you want to... Use this method... See the path that a request takes as it travels through a distributed system Follow the procedures to enable and instrument distributed tracing. Time a call to an external resource (such as an external service, database server, or message queue) Follow the procedures to Instrument calls to external segments. Collect or log errors The C SDK detects errors automatically. If you want to change the way it reports errors to New Relic, change the error collector configuration. If you want to... Use this method... Set logging levels for your app Use newrelic_configure_log() to configure the C SDK logs and command-line flags to configure the C daemon logs. For more information, see the C SDK logging documentation. Report an error newrelic_notice_error() Send custom data from your app To record custom data with the C SDK, you can use any of the following methods: Custom events: At New Relic, event data is a fundamental data type. Event data represents a record of a single event at a particular moment in time. This is useful to view or query specific details. Custom event attributes: To include additional metadata about the event, you can add key/value pairs (custom-attributes) to your custom event. Custom metrics: Metric timeslice data is the statistical measure of data that New Relic aggregates so that you can view it in the UI and chart it. Typically metric data has a longer retention period than event data. Custom events The C SDK provides a custom events API that allows you to send custom events to New Relic. To send an event, start a transaction and use the newrelic_create_custom_event and newrelic_record_custom_event functions. For example: // txn is a newrelic_txn_t*, created via newrelic_start_web_transaction newrelic_custom_event_t* custom_event=0; custom_event = newrelic_create_custom_event(\"aTypeForYourEvent\"); newrelic_record_custom_event(txn, &custom_event); Copy Be sure to review the custom data requirements and limits for guidance on what values are and are not allowed inside your custom event. For more information, see Custom events in APM. Tip If you created a custom event but need to remove it before the transaction has ended, use newrelic_discard_custom_event(&custom_event);. Custom event attributes You can also add int, long, double, and char* (string) attributes to your custom event by using the newrelic_custom_event_add_* family of functions. For example: // Example custom attributes: newrelic_custom_event_t* custom_event=0; custom_event = newrelic_create_custom_event(\"aTypeForYourEvent\"); newrelic_custom_event_add_attribute_int(custom_event, \"keya\", 42); newrelic_custom_event_add_attribute_long(custom_event, \"keyb\", 84); newrelic_custom_event_add_attribute_double(custom_event, \"keyc\", 42.42); newrelic_custom_event_add_attribute_string(custom_event, \"keyd\", \"A string\"); newrelic_record_custom_event(txn, &custom_event); Copy For more information, see the documentation about custom attributes. Custom metrics The C SDK provides the newrelic_record_custom_metric() function. This allows you to record time-based performance data using an API call, such as: Transaction timing data Computer resource data Subscription or purchasing data To create a custom metric, provide a name or other identifier and an amount of time in milliseconds to the function, along with the active transaction. Important Always prefix custom metric names with Custom/. For example: // txn is a newrelic_txn_t*, created via newrelic_start_web_transaction // Record a metric value of 100ms in the transaction txn newrelic_record_custom_metric(txn, \"Custom/MyMetric/My_label\", 100); Copy For more information, see Collect custom metrics. Here are some ways to use your custom data. For code details and examples for these options, see the New Relic globals documentation on GitHub. If you want to... Use this method... Create a custom event to populate with a timestamp and attributes. newrelic_create_custom_event() Timestamp and add the custom event to the current transaction so you can query or visualize it. newrelic_record_custom_event() Enhance your custom event with additional metadata. Add custom event attributes to your custom event based on type: newrelic_custom_event_add_attribute_double() newrelic_custom_event_add_attribute_int() newrelic_custom_event_add_attribute_long() newrelic_custom_event_add_attribute_string() Discard a custom event after it was created, but before its transaction has ended, to avoid reporting it to New Relic. newrelic_discard_custom_event This is necessary to free the allocated memory for your unwanted custom event in order to avoid leaks in your program. Report a custom performance duration that you can search or chart. newrelic_record_custom_metric() Monitor desktop browser performance To monitor desktop browser performance for your application, install the browser agent using the copy/paste method. Change configuration settings Typically the default settings for your application's configuration do not need to be changed. However, when necessary, you can adjust some of the settings. For more information, see the C SDK configuration documentation.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.45944,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Guide to using <em>the</em> <em>C</em> <em>SDK</em> API",
        "sections": "Guide to using <em>the</em> <em>C</em> <em>SDK</em> API",
        "tags": "<em>C</em> <em>SDK</em>",
        "body": "New Relic&#x27;s <em>C</em> <em>SDK</em> monitors your applications and microservices to help you identify and solve performance issues. <em>C</em> applications run from a compiled, native binary file. In order to monitor <em>transactions</em>, you must manually <em>instrument</em> your code by adding New Relic methods to it. This guide helps you"
      },
      "id": "603ec04928ccbc252beba785"
    }
  ],
  "/docs/instrumentation-editor-instrument-net-ui": [
    {
      "sections": [
        "Introduction to distributed tracing",
        "Why it matters",
        "Instrumentation: The key to distributed tracing",
        "What you can see in the New Relic UI",
        "Next steps"
      ],
      "title": "Introduction to distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "ac173988a6503674b4411c9c2efe6713912c37f2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/introduction-distributed-tracing/",
      "published_at": "2021-06-20T06:58:34Z",
      "updated_at": "2021-06-20T06:58:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Distributed tracing tracks and observes service requests as they flow through distributed systems. With distributed tracing data, you can quickly pinpoint failures or performance issues and fix them. Distributed tracing systems collect data as the requests go from one service to another, recording each segment of the journey as a span. These spans contain important details about each segment of the request and are combined into one trace. The completed trace gives you a picture of the entire request. Here is an example a web transaction where agents measure the time spent in each service. Agents then send that timing information to New Relic as spans where they are combined into one distributed trace. Why it matters A request might pass through various microservices to reach completion. The microservices or functions could be located in multiple containers, serverless environments, virtual machines, different cloud providers, on-premises, or any combination of these. For example, let's say that you're in a situation where a slow-running request affects the experience of a set of customers: The request is distributed across multiple microservices and serverless functions. Several different teams own and monitor the various services that are involved in the request. None of the teams have reported any performance issues with their microservices. Without a way to view the performance of the entire request across the different services, it’s nearly impossible to pinpoint where and why the high latency is occurring and which team should address the issue. Instrumentation: The key to distributed tracing Distributed tracing starts with the instrumentation of your services to enable data collection and correlation across the entire distributed system. Instrumention means either manually adding code to services or installing agents that automatically track trace data. Many of our New Relic solutions automatically instrument your services for a large number of programming languages and frameworks. You can also use open source tools and open instrumentation standards to instrument your environment. OpenTelemetry, part of the Cloud Native Computing Foundation (CNCF), is becoming the one standard for open source instrumentation and telemetry collection. What you can see in the New Relic UI After the data is collected, you can visualize it to see service dependencies, performance, and any anomalous events such as errors or unusual latency. Here are some examples of what you can do with your data: What you can do Description Detect anomalous spans Spans that are slow in comparison to typical behavior are marked as anomalous, with charts comparing them to typical performance. See your errors and logs Frontend and backend errors appear right in the context of your traces. Everything you need to troubleshoot is in one place. Filter results You can filter charts using many data points, so you can analyze trace data in different ways. Customize queries and dashboards You can create custom queries of your trace data and create custom data dashboards. See data across accounts See a global view of traces from across all your accounts and applications in New Relic One. Query traces programmatically Query distributed trace data by using GraphQL in our NerdGraph API explorer. Next steps Here are some tasks to consider: To instrument your services, check out our Quick start. To learn more about what's happening under the hood, see How distributed tracing works.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.26016,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Instrumentation</em>: The key to distributed tracing",
        "body": " to <em>instrument</em> your environment. OpenTelemetry, part of the Cloud Native Computing Foundation (CNCF), is becoming the one standard for open source <em>instrumentation</em> and telemetry collection. What you can see in the New Relic <em>UI</em> After the data is collected, you can visualize it to see service dependencies"
      },
      "id": "6072a767e7b9d231f1a5c64c"
    },
    {
      "sections": [
        "Ruby agent configuration",
        "Configuration methods and precedence",
        "View and edit config file options",
        "Update the config file",
        "Important",
        "General",
        "license_key",
        "agent_enabled",
        "app_name",
        "entity_guid",
        "monitor_mode",
        "log_level",
        "high_security",
        "security_policies_token",
        "proxy_host",
        "proxy_port",
        "proxy_user",
        "proxy_pass",
        "capture_params",
        "config_path",
        "apdex_t",
        "sync_startup",
        "send_data_on_exit",
        "timeout",
        "force_install_exit_handler",
        "log_file_name",
        "log_file_path",
        "prepend_active_record_instrumentation",
        "capture_memcache_keys",
        "message_tracer.segment_parameters.enabled",
        "marshaller",
        "backport_fast_active_record_connection_lookup",
        "labels",
        "ca_bundle_path",
        "datastore_tracer.instance_reporting.enabled",
        "datastore_tracer.database_name_reporting.enabled",
        "clear_transaction_state_after_fork",
        "exclude_newrelic_header",
        "infinite_tracing.trace_observer.host",
        "infinite_tracing.trace_observer.port",
        "Transaction Tracer",
        "transaction_tracer.enabled",
        "transaction_tracer.transaction_threshold",
        "transaction_tracer.record_sql",
        "transaction_tracer.record_redis_arguments",
        "transaction_tracer.capture_attributes",
        "transaction_tracer.explain_threshold",
        "transaction_tracer.explain_enabled",
        "transaction_tracer.stack_trace_threshold",
        "transaction_tracer.limit_segments",
        "Error Collector",
        "error_collector.enabled",
        "error_collector.capture_attributes",
        "error_collector.ignore_errors",
        "Caution",
        "error_collector.max_backtrace_frames",
        "error_collector.capture_events",
        "error_collector.max_event_samples_stored",
        "Browser Monitoring",
        "browser_monitoring.auto_instrument",
        "browser_monitoring.capture_attributes",
        "Analytics Events",
        "analytics_events.enabled",
        "analytics_events.max_samples_stored",
        "analytics_events.capture_attributes",
        "Attributes",
        "attributes.enabled",
        "transaction_tracer.attributes.enabled",
        "transaction_events.attributes.enabled",
        "error_collector.attributes.enabled",
        "browser_monitoring.attributes.enabled",
        "span_events.attributes.enabled",
        "transaction_segments.attributes.enabled",
        "attributes.exclude",
        "transaction_tracer.attributes.exclude",
        "transaction_events.attributes.exclude",
        "error_collector.attributes.exclude",
        "browser_monitoring.attributes.exclude",
        "span_events.attributes.exclude",
        "transaction_segments.attributes.exclude",
        "attributes.include",
        "transaction_tracer.attributes.include",
        "transaction_events.attributes.include",
        "error_collector.attributes.include",
        "browser_monitoring.attributes.include",
        "span_events.attributes.include",
        "transaction_segments.attributes.include",
        "Audit Log",
        "audit_log.enabled",
        "audit_log.path",
        "audit_log.endpoints",
        "Autostart",
        "autostart.denylisted_constants",
        "autostart.denylisted_executables",
        "autostart.denylisted_rake_tasks",
        "Cross Application Tracer",
        "cross_application_tracer.enabled",
        "Custom Attributes",
        "custom_attributes.enabled",
        "Custom Insights Events",
        "custom_insights_events.enabled",
        "custom_insights_events.max_samples_stored",
        "Disabling",
        "disable_rake",
        "disable_samplers",
        "disable_resque",
        "disable_sidekiq",
        "disable_dj",
        "disable_sinatra",
        "disable_sinatra_auto_middleware",
        "disable_view_instrumentation",
        "disable_activerecord_instrumentation",
        "disable_data_mapper",
        "disable_activejob",
        "disable_action_cable_instrumentation",
        "disable_active_storage",
        "disable_memcached",
        "disable_memcache_client",
        "disable_dalli",
        "disable_dalli_cas_client",
        "disable_memcache_instrumentation",
        "disable_gc_profiler",
        "disable_sequel_instrumentation",
        "disable_database_instrumentation",
        "disable_mongo",
        "disable_redis",
        "disable_vm_sampler",
        "disable_memory_sampler",
        "disable_cpu_sampler",
        "disable_delayed_job_sampler",
        "disable_active_record_notifications",
        "disable_bunny",
        "disable_curb",
        "disable_excon",
        "disable_httpclient",
        "disable_net_http",
        "disable_rack",
        "disable_rack_urlmap",
        "disable_puma_rack",
        "disable_puma_rack_urlmap",
        "disable_typhoeus",
        "disable_httprb",
        "disable_middleware_instrumentation",
        "disable_grape",
        "Distributed Tracing",
        "distributed_tracing.enabled",
        "Heroku",
        "heroku.use_dyno_names",
        "heroku.dyno_name_prefixes_to_shorten",
        "Instrumentation",
        "instrumentation.net_http",
        "instrumentation.typhoeus",
        "instrumentation.bunny",
        "instrumentation.httprb",
        "instrumentation.resque",
        "instrumentation.redis",
        "instrumentation.rake",
        "instrumentation.mongo",
        "instrumentation.delayed_job",
        "instrumentation.httpclient",
        "instrumentation.curb",
        "instrumentation.sinatra",
        "instrumentation.rack",
        "instrumentation.rack_urlmap",
        "instrumentation.puma_rack",
        "instrumentation.puma_rack_urlmap",
        "instrumentation.memcached",
        "instrumentation.memcache_client",
        "instrumentation.memcache",
        "instrumentation.excon",
        "instrumentation.grape",
        "Mongo",
        "mongo.capture_queries",
        "mongo.obfuscate_queries",
        "Process Host",
        "process_host.display_name",
        "Rake",
        "rake.tasks",
        "rake.connect_timeout",
        "Resque",
        "resque.capture_params",
        "Rules",
        "rules.ignore_url_regexes",
        "Sidekiq",
        "sidekiq.capture_params",
        "Slow SQL",
        "slow_sql.enabled",
        "slow_sql.explain_threshold",
        "slow_sql.explain_enabled",
        "slow_sql.record_sql",
        "slow_sql.use_longer_sql_id",
        "Span Events",
        "span_events.enabled",
        "span_events.queue_size",
        "span_events.max_samples_stored",
        "Strip Exception Messages",
        "strip_exception_messages.enabled",
        "strip_exception_messages.allowed_classes",
        "Thread Profiler",
        "thread_profiler.enabled",
        "Utilization",
        "utilization.detect_aws",
        "utilization.detect_azure",
        "utilization.detect_gcp",
        "utilization.detect_pcf",
        "utilization.detect_docker",
        "utilization.detect_kubernetes",
        "For more help"
      ],
      "title": "Ruby agent configuration",
      "type": "docs",
      "tags": [
        "Agents",
        "Ruby agent",
        "Configuration"
      ],
      "external_id": "ee72f1c59d456c5e5a089cfa81bfbde6064d7cb0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/ruby-agent/configuration/ruby-agent-configuration/",
      "published_at": "2021-06-20T18:19:23Z",
      "updated_at": "2021-06-20T18:19:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure the New Relic Ruby agent with settings in a configuration file, environment variables, or programmatically with server-side configuration. This document summarizes the configuration options available for the Ruby agent. If the default value for a configuration option is (Dynamic), this means the Ruby agent calculates the default at runtime. The value for the config setting defaults to the value of another setting as appropriate. Configuration methods and precedence The primary (default) method to configure the Ruby agent is via the configuration file (newrelic.yml) in the config subdirectory. To set configuration values using environment variables: Add the prefix NEW_RELIC_ to the setting's name. Replace any periods . with underscores _. You can also configure a few values in the UI via server-side configuration. The Ruby agent follows this order of precedence for configuration: Environment variables Server-side configuration Configuration file (newrelic.yml) Default configuration settings In other words, environment variables override all other configuration settings and info, server-side configuration overrides the configuration file and default config settings, and so on. View and edit config file options The Ruby agent's newrelic.yml is a standard YAML configuration file. It typically includes a Defaults section at the top, plus sections below for each application environment; for example, Development, Testing, and Production. The Ruby agent determines which section of the newrelic.yml config file to read from by looking at certain environment variables to derive the application's environment. This can be useful, for example, when you want to use info for the log_level config setting in your production environment, and you want more verbose log_level config settings (such as debug in your development environment. Here is an example newrelic.yml config file: common: &default_settings license_key: 'YOUR_LICENSE_KEY' app_name: 'My Application Name' production: <<: *default_settings log_level: info development: <<: *default_settings log_level: debug Copy For non-Rails apps, the Ruby agent looks for the following environment variables, in this order, to determine the application environment: NEW_RELIC_ENV RUBY_ENV RAILS_ENV APP_ENV RACK_ENV If the Ruby agent does not detect values for any of those environment variables, it will default the application environment to development and read from the development section of the newrelic.yml config file. When running the Ruby agent in a Rails app, the agent first looks for the NEW_RELIC_ENV environment variable to determine the application environment and which section of the newrelic.yml to use. If NEW_RELIC_ENV is not present, the agent uses the Rails environment (RAILS_ENV or RAILS.env, depending on the version of Rails) . When you edit the config file, be sure to: Indent only with two spaces. Indent only where relevant, in stanzas such as error_collector. If you do not indent correctly, the agent may throw an Unable to parse configuration file error on startup. To view the most current list of available Ruby agent configuration options, use the rake newrelic:config:docs command. This document describes the most common options. Update the config file This documentation applies to the Ruby agent's latest release. For details on earlier versions, refer to the comments in newrelic.yml itself. To update newrelic.yml file after a new release, use the template in the base directory of the agent gem. When you update to new gem versions, examine or diff config/newrelic.yml and newrelic.yml in the installation directory to take advantage of new configuration options. Important Updating the gem does not automatically update config/newrelic.yml. General These settings are available for agent configuration. Some settings depend on your New Relic subscription level. license_key Type String Default \"\" Environ variable NEW_RELIC_LICENSE_KEY Your New Relic license key. agent_enabled Type Boolean Default (Dynamic) Environ variable NEW_RELIC_AGENT_ENABLED If true, allows the Ruby agent to run. app_name Type String Default (Dynamic) Environ variable NEW_RELIC_APP_NAME Specify the application name used to aggregate data in the New Relic UI. To report data to multiple apps at the same time, specify a list of names separated by a semicolon ;. For example, MyApp or MyStagingApp;Instance1. entity_guid Type String Default nil Environ variable NEW_RELIC_ENTITY_GUID The Entity GUID for the entity running this agent. monitor_mode Type Boolean Default (Dynamic) Environ variable NEW_RELIC_MONITOR_MODE When true, the agent transmits data about your app to the New Relic collector. log_level Type String Default \"info\" Environ variable NEW_RELIC_LOG_LEVEL Sets the level of detail of log messages. Possible log levels, in increasing verbosity, are: error, warn, info or debug. high_security Type Boolean Default false Environ variable NEW_RELIC_HIGH_SECURITY If true, enables high security mode. Ensure you understand the implications of high security mode before enabling this setting. security_policies_token Type String Default \"\" Environ variable NEW_RELIC_SECURITY_POLICIES_TOKEN Applies Language Agent Security Policy settings. proxy_host Type String Default nil Environ variable NEW_RELIC_PROXY_HOST Defines a host for communicating with the New Relic collector via a proxy server. proxy_port Type Integer Default 8080 Environ variable NEW_RELIC_PROXY_PORT Defines a port for communicating with the New Relic collector via a proxy server. proxy_user Type String Default nil Environ variable NEW_RELIC_PROXY_USER Defines a user for communicating with the New Relic collector via a proxy server. proxy_pass Type String Default nil Environ variable NEW_RELIC_PROXY_PASS Defines a password for communicating with the New Relic collector via a proxy server. capture_params Type Boolean Default false Environ variable NEW_RELIC_CAPTURE_PARAMS When true, the agent captures HTTP request parameters and attaches them to transaction traces, traced errors, and TransactionError events When using the capture_params setting, the Ruby agent will not attempt to filter secret information. Recommendation: To filter secret information from request parameters, use the attributes.include setting instead. For more information, see the Ruby attribute examples. config_path Type String Default (Dynamic) Environ variable NEW_RELIC_CONFIG_PATH Path to newrelic.yml. If undefined, the agent checks the following directories (in order): config/newrelic.yml, newrelic.yml, $HOME/.newrelic/newrelic.yml and $HOME/newrelic.yml. apdex_t Type Float Default 0.5 Environ variable NEW_RELIC_APDEX_T DEPRECATED Deprecated. For agent versions 3.5.0 or higher, set your Apdex T via the New Relic UI. sync_startup Type Boolean Default false Environ variable NEW_RELIC_SYNC_STARTUP When set to true, forces a synchronous connection to the New Relic collector during application startup. For very short-lived processes, this helps ensure the New Relic agent has time to report. send_data_on_exit Type Boolean Default true Environ variable NEW_RELIC_SEND_DATA_ON_EXIT If true, enables the exit handler that sends data to the New Relic collector before shutting down. timeout Type Integer Default 120 Environ variable NEW_RELIC_TIMEOUT Defines the maximum number of seconds the agent should spend attempting to connect to the collector. force_install_exit_handler Type Boolean Default false Environ variable NEW_RELIC_FORCE_INSTALL_EXIT_HANDLER Forces the exit handler that sends all cached data to collector before shutting down to be installed regardless of detecting scenarios where it generally should not be. Known use-case for this option is where Sinatra is running as an embedded service within another framework and the agent is detecting the Sinatra app and skipping the at_exit handler as a result. Sinatra classically runs the entire application in an at_exit block and would otherwise misbehave if the Agent's at_exit handler was also installed in those circumstances. Note: send_data_on_exit should also be set to true in tandem with this setting. log_file_name Type String Default \"newrelic_agent.log\" Environ variable NEW_RELIC_LOG_FILE_NAME Defines a name for the log file. log_file_path Type String Default \"log/\" Environ variable NEW_RELIC_LOG_FILE_PATH Defines a path to the agent log file, excluding the filename. prepend_active_record_instrumentation Type Boolean Default false Environ variable NEW_RELIC_PREPEND_ACTIVE_RECORD_INSTRUMENTATION If true, uses Module#prepend rather than alias_method for ActiveRecord instrumentation. capture_memcache_keys Type Boolean Default false Environ variable NEW_RELIC_CAPTURE_MEMCACHE_KEYS Enable or disable the capture of memcache keys from transaction traces. message_tracer.segment_parameters.enabled Type Boolean Default true Environ variable NEW_RELIC_MESSAGE_TRACER_SEGMENT_PARAMETERS_ENABLED If true, the agent will collect metadata about messages and attach them as segment parameters. marshaller Type String Default \"json\" Environ variable NEW_RELIC_MARSHALLER Specifies a marshaller for transmitting data to the New Relic collector. Currently json is the only valid value for this setting. backport_fast_active_record_connection_lookup Type Boolean Default false Environ variable NEW_RELIC_BACKPORT_FAST_ACTIVE_RECORD_CONNECTION_LOOKUP Backports the faster ActiveRecord connection lookup introduced in Rails 6, which improves agent performance when instrumenting ActiveRecord. Note that this setting may not be compatible with other gems that patch ActiveRecord. labels Type String Default \"\" Environ variable NEW_RELIC_LABELS A dictionary of label names and values that will be applied to the data sent from this agent. May also be expressed as a semicolon-delimited ; string of colon-separated : pairs. For example, <var>Server</var>:<var>One</var>;<var>Data Center</var>:<var>Primary</var>. ca_bundle_path Type String Default nil Environ variable NEW_RELIC_CA_BUNDLE_PATH Manual override for the path to your local CA bundle. This CA bundle will be used to validate the SSL certificate presented by New Relic's data collection service. datastore_tracer.instance_reporting.enabled Type Boolean Default true Environ variable NEW_RELIC_DATASTORE_TRACER_INSTANCE_REPORTING_ENABLED If false, the agent will not report datastore instance metrics, nor add host or port_path_or_id parameters to transaction or slow SQL traces. datastore_tracer.database_name_reporting.enabled Type Boolean Default true Environ variable NEW_RELIC_DATASTORE_TRACER_DATABASE_NAME_REPORTING_ENABLED If false, the agent will not add database_name parameter to transaction or slow sql traces. clear_transaction_state_after_fork Type Boolean Default false Environ variable NEW_RELIC_CLEAR_TRANSACTION_STATE_AFTER_FORK If true, the agent will clear Tracer::State in Agent.drop_buffered_data. exclude_newrelic_header Type Boolean Default false Environ variable NEW_RELIC_EXCLUDE_NEWRELIC_HEADER Allows newrelic distributed tracing headers to be suppressed on outbound requests. infinite_tracing.trace_observer.host Type String Default \"\" Environ variable NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST Configures the hostname for the Trace Observer Host. When configured, enables tail-based sampling by sending all recorded spans to a Trace Observer for further sampling decisions, irrespective of any usual agent sampling decision. infinite_tracing.trace_observer.port Type Integer Default 443 Environ variable NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_PORT Configures the TCP/IP port for the Trace Observer Host Transaction Tracer The transaction traces feature collects detailed information from a selection of transactions, including a summary of the calling sequence, a breakdown of time spent, and a list of SQL queries and their query plans (on mysql and postgresql). Available features depend on your New Relic subscription level. transaction_tracer.enabled Type Boolean Default true Environ variable NEW_RELIC_TRANSACTION_TRACER_ENABLED If true, enables collection of transaction traces. transaction_tracer.transaction_threshold Type Float Default (Dynamic) Environ variable NEW_RELIC_TRANSACTION_TRACER_TRANSACTION_THRESHOLD Specify a threshold in seconds. Transactions with a duration longer than this threshold are eligible for transaction traces. Specify a float value or the string apdex_f. transaction_tracer.record_sql Type String Default \"obfuscated\" Environ variable NEW_RELIC_TRANSACTION_TRACER_RECORD_SQL Obfuscation level for SQL queries reported in transaction trace nodes. By default, this is set to obfuscated, which strips out the numeric and string literals. If you do not want the agent to capture query information, set this to none. If you want the agent to capture all query information in its original form, set this to raw. When you enable high security mode, this is automatically set to obfuscated. transaction_tracer.record_redis_arguments Type Boolean Default false Environ variable NEW_RELIC_TRANSACTION_TRACER_RECORD_REDIS_ARGUMENTS If true, the agent records Redis command arguments in transaction traces. transaction_tracer.capture_attributes Type Boolean Default true Environ variable NEW_RELIC_TRANSACTION_TRACER_CAPTURE_ATTRIBUTES DEPRECATED Deprecated; use transaction_tracer.attributes.enabled instead. transaction_tracer.explain_threshold Type Float Default 0.5 Environ variable NEW_RELIC_TRANSACTION_TRACER_EXPLAIN_THRESHOLD Threshold (in seconds) above which the agent will collect explain plans. Relevant only when explain_enabled is true. transaction_tracer.explain_enabled Type Boolean Default true Environ variable NEW_RELIC_TRANSACTION_TRACER_EXPLAIN_ENABLED If true, enables the collection of explain plans in transaction traces. This setting will also apply to explain plans in slow SQL traces if slow_sql.explain_enabled is not set separately. transaction_tracer.stack_trace_threshold Type Float Default 0.5 Environ variable NEW_RELIC_TRANSACTION_TRACER_STACK_TRACE_THRESHOLD Specify a threshold in seconds. The agent includes stack traces in transaction trace nodes when the stack trace duration exceeds this threshold. transaction_tracer.limit_segments Type Integer Default 4000 Environ variable NEW_RELIC_TRANSACTION_TRACER_LIMIT_SEGMENTS Maximum number of transaction trace nodes to record in a single transaction trace. Error Collector The agent collects and reports all uncaught exceptions by default. These configuration options allow you to customize the error collection. error_collector.enabled Type Boolean Default true Environ variable NEW_RELIC_ERROR_COLLECTOR_ENABLED If true, the agent captures traced errors and error count metrics. error_collector.capture_attributes Type Boolean Default true Environ variable NEW_RELIC_ERROR_COLLECTOR_CAPTURE_ATTRIBUTES DEPRECATED Deprecated; use error_collector.attributes.enabled instead. error_collector.ignore_errors Type String Default \"ActionController::RoutingError,Sinatra::NotFound\" Environ variable NEW_RELIC_ERROR_COLLECTOR_IGNORE_ERRORS Specify a comma-delimited list of error classes that the agent should ignore. Caution Server side configuration takes precedence for this setting over all environment configurations. This differs from all other configuration settings where environment variable take precedence over server side configuration. error_collector.max_backtrace_frames Type Integer Default 50 Environ variable NEW_RELIC_ERROR_COLLECTOR_MAX_BACKTRACE_FRAMES Defines the maximum number of frames in an error backtrace. Backtraces over this amount are truncated at the beginning and end. error_collector.capture_events Type Boolean Default (Dynamic) Environ variable NEW_RELIC_ERROR_COLLECTOR_CAPTURE_EVENTS If true, the agent collects TransactionError events. error_collector.max_event_samples_stored Type Integer Default 100 Environ variable NEW_RELIC_ERROR_COLLECTOR_MAX_EVENT_SAMPLES_STORED Defines the maximum number of TransactionError events sent to Insights per harvest cycle. Browser Monitoring The Browser monitoring page load timing feature (sometimes referred to as real user monitoring or RUM) gives you insight into the performance real users are experiencing with your website. This is accomplished by measuring the time it takes for your users' browsers to download and render your web pages by injecting a small amount of JavaScript code into the header and footer of each page. browser_monitoring.auto_instrument Type Boolean Default (Dynamic) Environ variable NEW_RELIC_BROWSER_MONITORING_AUTO_INSTRUMENT This is true by default, this enables auto-injection of the JavaScript header for page load timing (sometimes referred to as real user monitoring or RUM). browser_monitoring.capture_attributes Type Boolean Default false Environ variable NEW_RELIC_BROWSER_MONITORING_CAPTURE_ATTRIBUTES DEPRECATED Deprecated; use browser_monitoring.attributes.enabled instead. Analytics Events New Relic dashboards is a resource to gather and visualize data about your software and what it says about your business. With it you can quickly and easily create real-time dashboards to get immediate answers about end-user experiences, clickstreams, mobile activities, and server transactions. analytics_events.enabled Type Boolean Default true Environ variable NEW_RELIC_ANALYTICS_EVENTS_ENABLED If true, enables analytics event sampling. analytics_events.max_samples_stored Type Integer Default 1200 Environ variable NEW_RELIC_ANALYTICS_EVENTS_MAX_SAMPLES_STORED Defines the maximum number of request events reported from a single harvest. analytics_events.capture_attributes Type Boolean Default true Environ variable NEW_RELIC_ANALYTICS_EVENTS_CAPTURE_ATTRIBUTES DEPRECATED Deprecated; use transaction_events.attributes.enabled instead. Attributes Attributes are key-value pairs containing information that determines the properties of an event or transaction. These key-value pairs can be viewed within transaction traces in APM, traced errors in APM, transaction events in dashboards, and page views in dashboards. You can customize exactly which attributes will be sent to each of these destinations attributes.enabled Type Boolean Default true Environ variable NEW_RELIC_ATTRIBUTES_ENABLED If true, enables capture of attributes for all destinations. transaction_tracer.attributes.enabled Type Boolean Default (Dynamic) Environ variable NEW_RELIC_TRANSACTION_TRACER_ATTRIBUTES_ENABLED If true, the agent captures attributes from transaction traces. transaction_events.attributes.enabled Type Boolean Default (Dynamic) Environ variable NEW_RELIC_TRANSACTION_EVENTS_ATTRIBUTES_ENABLED If true, the agent captures attributes from transaction events. error_collector.attributes.enabled Type Boolean Default (Dynamic) Environ variable NEW_RELIC_ERROR_COLLECTOR_ATTRIBUTES_ENABLED If true, the agent captures attributes from error collection. browser_monitoring.attributes.enabled Type Boolean Default (Dynamic) Environ variable NEW_RELIC_BROWSER_MONITORING_ATTRIBUTES_ENABLED If true, the agent captures attributes from browser monitoring. span_events.attributes.enabled Type Boolean Default true Environ variable NEW_RELIC_SPAN_EVENTS_ATTRIBUTES_ENABLED If true, the agent captures attributes on span events. transaction_segments.attributes.enabled Type Boolean Default true Environ variable NEW_RELIC_TRANSACTION_SEGMENTS_ATTRIBUTES_ENABLED If true, the agent captures attributes on transaction segments. attributes.exclude Type Array Default [] Environ variable NEW_RELIC_ATTRIBUTES_EXCLUDE Prefix of attributes to exclude from all destinations. Allows * as wildcard at end. transaction_tracer.attributes.exclude Type Array Default [] Environ variable NEW_RELIC_TRANSACTION_TRACER_ATTRIBUTES_EXCLUDE Prefix of attributes to exclude from transaction traces. Allows * as wildcard at end. transaction_events.attributes.exclude Type Array Default [] Environ variable NEW_RELIC_TRANSACTION_EVENTS_ATTRIBUTES_EXCLUDE Prefix of attributes to exclude from transaction events. Allows * as wildcard at end. error_collector.attributes.exclude Type Array Default [] Environ variable NEW_RELIC_ERROR_COLLECTOR_ATTRIBUTES_EXCLUDE Prefix of attributes to exclude from error collection. Allows * as wildcard at end. browser_monitoring.attributes.exclude Type Array Default [] Environ variable NEW_RELIC_BROWSER_MONITORING_ATTRIBUTES_EXCLUDE Prefix of attributes to exclude from browser monitoring. Allows * as wildcard at end. span_events.attributes.exclude Type Array Default [] Environ variable NEW_RELIC_SPAN_EVENTS_ATTRIBUTES_EXCLUDE Prefix of attributes to exclude from span events. Allows * as wildcard at end. transaction_segments.attributes.exclude Type Array Default [] Environ variable NEW_RELIC_TRANSACTION_SEGMENTS_ATTRIBUTES_EXCLUDE Prefix of attributes to exclude from transaction segments. Allows * as wildcard at end. attributes.include Type Array Default [] Environ variable NEW_RELIC_ATTRIBUTES_INCLUDE Prefix of attributes to include in all destinations. Allows * as wildcard at end. transaction_tracer.attributes.include Type Array Default [] Environ variable NEW_RELIC_TRANSACTION_TRACER_ATTRIBUTES_INCLUDE Prefix of attributes to include in transaction traces. Allows * as wildcard at end. transaction_events.attributes.include Type Array Default [] Environ variable NEW_RELIC_TRANSACTION_EVENTS_ATTRIBUTES_INCLUDE Prefix of attributes to include in transaction events. Allows * as wildcard at end. error_collector.attributes.include Type Array Default [] Environ variable NEW_RELIC_ERROR_COLLECTOR_ATTRIBUTES_INCLUDE Prefix of attributes to include in error collection. Allows * as wildcard at end. browser_monitoring.attributes.include Type Array Default [] Environ variable NEW_RELIC_BROWSER_MONITORING_ATTRIBUTES_INCLUDE Prefix of attributes to include in browser monitoring. Allows * as wildcard at end. span_events.attributes.include Type Array Default [] Environ variable NEW_RELIC_SPAN_EVENTS_ATTRIBUTES_INCLUDE Prefix of attributes to include on span events. Allows * as wildcard at end. transaction_segments.attributes.include Type Array Default [] Environ variable NEW_RELIC_TRANSACTION_SEGMENTS_ATTRIBUTES_INCLUDE Prefix of attributes to include on transaction segments. Allows * as wildcard at end. Audit Log audit_log.enabled Type Boolean Default false Environ variable NEW_RELIC_AUDIT_LOG_ENABLED If true, enables an audit log which logs communications with the New Relic collector. audit_log.path Type String Default (Dynamic) Environ variable NEW_RELIC_AUDIT_LOG_PATH Specifies a path to the audit log file (including the filename). audit_log.endpoints Type Array Default [\".*\"] Environ variable NEW_RELIC_AUDIT_LOG_ENDPOINTS List of allowed endpoints to include in audit log Autostart autostart.denylisted_constants Type String Default \"Rails::Console\" Environ variable NEW_RELIC_AUTOSTART_DENYLISTED_CONSTANTS Specify a list of constants that should prevent the agent from starting automatically. Separate individual constants with a comma ,. For example, Rails::Console,UninstrumentedBackgroundJob. autostart.denylisted_executables Type String Default \"irb,rspec\" Environ variable NEW_RELIC_AUTOSTART_DENYLISTED_EXECUTABLES Defines a comma-delimited list of executables that the agent should not instrument. For example, rake,my_ruby_script.rb. autostart.denylisted_rake_tasks Type String Default \"about,assets:clean,assets:clobber,assets:environment,assets:precompile,assets:precompile:all,db:create,db:drop,db:fixtures:load,db:migrate,db:migrate:status,db:rollback,db:schema:cache:clear,db:schema:cache:dump,db:schema:dump,db:schema:load,db:seed,db:setup,db:structure:dump,db:version,doc:app,log:clear,middleware,notes,notes:custom,rails:template,rails:update,routes,secret,spec,spec:features,spec:requests,spec:controllers,spec:helpers,spec:models,spec:views,spec:routing,spec:rcov,stats,test,test:all,test:all:db,test:recent,test:single,test:uncommitted,time:zones:all,tmp:clear,tmp:create,webpacker:compile\" Environ variable NEW_RELIC_AUTOSTART_DENYLISTED_RAKE_TASKS Defines a comma-delimited list of Rake tasks that the agent should not instrument. For example, assets:precompile,db:migrate. Cross Application Tracer cross_application_tracer.enabled Type Boolean Default (Dynamic) Environ variable NEW_RELIC_CROSS_APPLICATION_TRACER_ENABLED If true, enables cross-application tracing. Custom Attributes custom_attributes.enabled Type Boolean Default true Environ variable NEW_RELIC_CUSTOM_ATTRIBUTES_ENABLED If false, custom attributes will not be sent on Insights events. Custom Insights Events custom_insights_events.enabled Type Boolean Default true Environ variable NEW_RELIC_CUSTOM_INSIGHTS_EVENTS_ENABLED If true, the agent captures New Relic Insights custom events. custom_insights_events.max_samples_stored Type Integer Default 1000 Environ variable NEW_RELIC_CUSTOM_INSIGHTS_EVENTS_MAX_SAMPLES_STORED Specify a maximum number of custom Insights events to buffer in memory at a time. Disabling Use these settings to toggle instrumentation types during agent startup. disable_rake Type Boolean Default false Environ variable NEW_RELIC_DISABLE_RAKE DEPRECATED Please see: instrumentation.rake. If true, disables Rake instrumentation. disable_samplers Type Boolean Default false Environ variable NEW_RELIC_DISABLE_SAMPLERS If true, disables the collection of sampler metrics. Sampler metrics are metrics that are not event-based (such as CPU time or memory usage). disable_resque Type Boolean Default false Environ variable NEW_RELIC_DISABLE_RESQUE DEPRECATED Please see: instrumentation.resque. If true, disables Resque instrumentation. disable_sidekiq Type Boolean Default false Environ variable NEW_RELIC_DISABLE_SIDEKIQ If true, disables Sidekiq instrumentation. disable_dj Type Boolean Default false Environ variable NEW_RELIC_DISABLE_DJ DEPRECATED Please see: instrumentation.delayed_job. If true, disables Delayed::Job instrumentation. disable_sinatra Type Boolean Default false Environ variable NEW_RELIC_DISABLE_SINATRA DEPRECATED Please see: instrumentation.sinatra. If true , disables Sinatra instrumentation. disable_sinatra_auto_middleware Type Boolean Default false Environ variable NEW_RELIC_DISABLE_SINATRA_AUTO_MIDDLEWARE If true, disables agent middleware for Sinatra. This middleware is responsible for advanced feature support such as cross application tracing, page load timing, and error collection. disable_view_instrumentation Type Boolean Default false Environ variable NEW_RELIC_DISABLE_VIEW_INSTRUMENTATION If true, disables view instrumentation. disable_activerecord_instrumentation Type Boolean Default (Dynamic) Environ variable NEW_RELIC_DISABLE_ACTIVERECORD_INSTRUMENTATION If true, disables active record instrumentation. disable_data_mapper Type Boolean Default false Environ variable NEW_RELIC_DISABLE_DATA_MAPPER If true, disables DataMapper instrumentation. disable_activejob Type Boolean Default false Environ variable NEW_RELIC_DISABLE_ACTIVEJOB If true, disables ActiveJob instrumentation. disable_action_cable_instrumentation Type Boolean Default false Environ variable NEW_RELIC_DISABLE_ACTION_CABLE_INSTRUMENTATION If true, disables Action Cable instrumentation. disable_active_storage Type Boolean Default false Environ variable NEW_RELIC_DISABLE_ACTIVE_STORAGE If true, disables ActiveStorage instrumentation. disable_memcached Type Boolean Default (Dynamic) Environ variable NEW_RELIC_DISABLE_MEMCACHED DEPRECATED Please see: instrumentation.memcached. If true, disables instrumentation for the memcached gem. disable_memcache_client Type Boolean Default (Dynamic) Environ variable NEW_RELIC_DISABLE_MEMCACHE_CLIENT DEPRECATED Please see: instrumentation.memcache-client. If true, disables instrumentation for the memcache-client gem. disable_dalli Type Boolean Default (Dynamic) Environ variable NEW_RELIC_DISABLE_DALLI DEPRECATED Please see: instrumentation.memcache. If true, disables instrumentation for the dalli gem. disable_dalli_cas_client Type Boolean Default (Dynamic) Environ variable NEW_RELIC_DISABLE_DALLI_CAS_CLIENT DEPRECATED Please see: instrumentation.memcache. If true, disables instrumentation for the dalli gem's additional CAS client support. disable_memcache_instrumentation Type Boolean Default false Environ variable NEW_RELIC_DISABLE_MEMCACHE_INSTRUMENTATION DEPRECATED Please see: instrumentation.memcache. If true, disables memcache instrumentation. disable_gc_profiler Type Boolean Default false Environ variable NEW_RELIC_DISABLE_GC_PROFILER If true, disables the use of GC::Profiler to measure time spent in garbage collection disable_sequel_instrumentation Type Boolean Default false Environ variable NEW_RELIC_DISABLE_SEQUEL_INSTRUMENTATION If true, disables Sequel instrumentation. disable_database_instrumentation Type Boolean Default false Environ variable NEW_RELIC_DISABLE_DATABASE_INSTRUMENTATION DEPRECATED Deprecated; use disable_sequel_instrumentation instead. disable_mongo Type Boolean Default false Environ variable NEW_RELIC_DISABLE_MONGO DEPRECATED Please see: instrumentation.mongo. If true, the agent won't install instrumentation for the Mongo gem. disable_redis Type Boolean Default false Environ variable NEW_RELIC_DISABLE_REDIS DEPRECATED Please see: instrumentation.redis. If true, the agent won't install instrumentation for Redis. disable_vm_sampler Type Boolean Default false Environ variable NEW_RELIC_DISABLE_VM_SAMPLER If true, the agent won't sample performance measurements from the Ruby VM. disable_memory_sampler Type Boolean Default false Environ variable NEW_RELIC_DISABLE_MEMORY_SAMPLER If true, the agent won't sample the memory usage of the host process. disable_cpu_sampler Type Boolean Default false Environ variable NEW_RELIC_DISABLE_CPU_SAMPLER If true, the agent won't sample the CPU usage of the host process. disable_delayed_job_sampler Type Boolean Default false Environ variable NEW_RELIC_DISABLE_DELAYED_JOB_SAMPLER If true, the agent won't measure the depth of Delayed Job queues. disable_active_record_notifications Type Boolean Default false Environ variable NEW_RELIC_DISABLE_ACTIVE_RECORD_NOTIFICATIONS If true, disables instrumentation for ActiveRecord 4, 5, and 6. disable_bunny Type Boolean Default false Environ variable NEW_RELIC_DISABLE_BUNNY DEPRECATED Please see: instrumentation.bunny. If true, disables instrumentation for the bunny gem. disable_curb Type Boolean Default false Environ variable NEW_RELIC_DISABLE_CURB DEPRECATED Please see: instrumentation.curb. If true, disables instrumentation for the curb gem. disable_excon Type Boolean Default false Environ variable NEW_RELIC_DISABLE_EXCON DEPRECATED Please see: instrumentation.excon. If true, disables instrumentation for the excon gem. disable_httpclient Type Boolean Default false Environ variable NEW_RELIC_DISABLE_HTTPCLIENT DEPRECATED Please see: instrumentation.httpclient. If true, disables instrumentation for the httpclient gem. disable_net_http Type Boolean Default false Environ variable NEW_RELIC_DISABLE_NET_HTTP DEPRECATED Please see: instrumentation.net_http. If true, disables instrumentation for Net::HTTP. disable_rack Type Boolean Default false Environ variable NEW_RELIC_DISABLE_RACK DEPRECATED Please see: instrumentation.rack. If true, prevents the agent from hooking into the to_app method in Rack::Builder to find gems to instrument during application startup. disable_rack_urlmap Type Boolean Default false Environ variable NEW_RELIC_DISABLE_RACK_URLMAP DEPRECATED Please see: instrumentation.rack_urlmap. If true, prevents the agent from hooking into Rack::URLMap to install middleware tracing. disable_puma_rack Type Boolean Default (Dynamic) Environ variable NEW_RELIC_DISABLE_PUMA_RACK DEPRECATED Please see: instrumentation.puma_rack. If true, prevents the agent from hooking into the to_app method in Puma::Rack::Builder to find gems to instrument during application startup. disable_puma_rack_urlmap Type Boolean Default (Dynamic) Environ variable NEW_RELIC_DISABLE_PUMA_RACK_URLMAP DEPRECATED Please see: instrumentation.puma_rack_urlmap. If true, prevents the agent from hooking into Puma::Rack::URLMap to install middleware tracing. disable_typhoeus Type Boolean Default false Environ variable NEW_RELIC_DISABLE_TYPHOEUS DEPRECATED Please see: instrumentation.typhoeus. If true, the agent won't install instrumentation for the typhoeus gem. disable_httprb Type Boolean Default false Environ variable NEW_RELIC_DISABLE_HTTPRB DEPRECATED Please see: instrumentation.httprb. If true, the agent won't install instrumentation for the http.rb gem. disable_middleware_instrumentation Type Boolean Default false Environ variable NEW_RELIC_DISABLE_MIDDLEWARE_INSTRUMENTATION If true, the agent won't wrap third-party middlewares in instrumentation (regardless of whether they are installed via Rack::Builder or Rails). disable_grape Type Boolean Default false Environ variable NEW_RELIC_DISABLE_GRAPE DEPRECATED Please see: instrumentation.grape. If true, the agent won't install Grape instrumentation. Distributed Tracing distributed_tracing.enabled Type Boolean Default false Environ variable NEW_RELIC_DISTRIBUTED_TRACING_ENABLED Distributed tracing lets you see the path that a request takes through your distributed system. Enabling distributed tracing changes the behavior of some New Relic features, so carefully consult the transition guide before you enable this feature. Heroku heroku.use_dyno_names Type Boolean Default true Environ variable NEW_RELIC_HEROKU_USE_DYNO_NAMES If true, the agent uses Heroku dyno names as the hostname. heroku.dyno_name_prefixes_to_shorten Type Array Default [\"scheduler\", \"run\"] Environ variable NEW_RELIC_HEROKU_DYNO_NAME_PREFIXES_TO_SHORTEN Ordinarily the agent reports dyno names with a trailing dot and process ID (for example, worker.3). You can remove this trailing data by specifying the prefixes you want to report without trailing data (for example, worker). Instrumentation instrumentation.net_http Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_NET_HTTP Controls auto-instrumentation of Net::HTTP at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.typhoeus Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_TYPHOEUS Controls auto-instrumentation of Typhoeus at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.bunny Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_BUNNY Controls auto-instrumentation of bunny at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.httprb Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_HTTPRB Controls auto-instrumentation of http.rb gem at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.resque Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_RESQUE Controls auto-instrumentation of resque at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.redis Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_REDIS Controls auto-instrumentation of Redis at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.rake Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_RAKE Controls auto-instrumentation of rake at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.mongo Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_MONGO Controls auto-instrumentation of Mongo at start up. May be one of [enabled|disabled] . instrumentation.delayed_job Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_DELAYED_JOB Controls auto-instrumentation of Delayed Job at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.httpclient Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_HTTPCLIENT Controls auto-instrumentation of HTTPClient at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.curb Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_CURB Controls auto-instrumentation of Curb at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.sinatra Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_SINATRA Controls auto-instrumentation of Sinatra at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.rack Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_RACK Controls auto-instrumentation of Rack. When enabled, the agent hooks into the to_app method in Rack::Builder to find gems to instrument during application startup. May be one of [auto|prepend|chain|disabled] . instrumentation.rack_urlmap Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_RACK_URLMAP Controls auto-instrumentation of Rack::URLMap at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.puma_rack Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_PUMA_RACK Controls auto-instrumentation of Puma::Rack. When enabled, the agent hooks into the to_app method in Puma::Rack::Builder to find gems to instrument during application startup. May be one of [auto|prepend|chain|disabled] . instrumentation.puma_rack_urlmap Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_PUMA_RACK_URLMAP Controls auto-instrumentation of Puma::Rack::URLMap at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.memcached Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_MEMCACHED Controls auto-instrumentation of memcached gem for Memcache at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.memcache_client Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_MEMCACHE_CLIENT Controls auto-instrumentation of memcache-client gem for Memcache at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.memcache Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_MEMCACHE Controls auto-instrumentation of dalli gem for Memcache at start up. May be one of [auto|prepend|chain|disabled] . instrumentation.excon Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_EXCON Controls auto-instrumentation of Excon at start up. May be one of [enabled|disabled] . instrumentation.grape Type String Default (Dynamic) Environ variable NEW_RELIC_INSTRUMENTATION_GRAPE Controls auto-instrumentation of Grape at start up. May be one of [auto|prepend|chain|disabled] . Mongo mongo.capture_queries Type Boolean Default true Environ variable NEW_RELIC_MONGO_CAPTURE_QUERIES If true, the agent captures Mongo queries in transaction traces. mongo.obfuscate_queries Type Boolean Default true Environ variable NEW_RELIC_MONGO_OBFUSCATE_QUERIES If true, the agent obfuscates Mongo queries in transaction traces. Process Host process_host.display_name Type String Default (Dynamic) Environ variable NEW_RELIC_PROCESS_HOST_DISPLAY_NAME Specify a custom host name for display in the New Relic UI. Rake rake.tasks Type Array Default [] Environ variable NEW_RELIC_RAKE_TASKS Specify an array of Rake tasks to automatically instrument. rake.connect_timeout Type Integer Default 10 Environ variable NEW_RELIC_RAKE_CONNECT_TIMEOUT Timeout for waiting on connect to complete before a rake task Resque resque.capture_params Type Boolean Default false Environ variable NEW_RELIC_RESQUE_CAPTURE_PARAMS DEPRECATED If true, enables the capture of job arguments for transaction traces and traced errors in Resque. Rules rules.ignore_url_regexes Type Array Default [] Environ variable NEW_RELIC_RULES_IGNORE_URL_REGEXES Define transactions you want the agent to ignore, by specifying a list of patterns matching the URI you want to ignore. Sidekiq sidekiq.capture_params Type Boolean Default false Environ variable NEW_RELIC_SIDEKIQ_CAPTURE_PARAMS DEPRECATED If true, enables the capture of job arguments for transaction traces and traced errors in Sidekiq. Slow SQL slow_sql.enabled Type Boolean Default (Dynamic) Environ variable NEW_RELIC_SLOW_SQL_ENABLED If true, the agent collects slow SQL queries. slow_sql.explain_threshold Type Float Default (Dynamic) Environ variable NEW_RELIC_SLOW_SQL_EXPLAIN_THRESHOLD Specify a threshold in seconds. The agent collects slow SQL queries and explain plans that exceed this threshold. slow_sql.explain_enabled Type Boolean Default (Dynamic) Environ variable NEW_RELIC_SLOW_SQL_EXPLAIN_ENABLED If true, the agent collects explain plans in slow SQL queries. If this setting is omitted, the transaction_tracer.explain_enabled setting will be applied as the default setting for explain plans in slow SQL as well. slow_sql.record_sql Type String Default (Dynamic) Environ variable NEW_RELIC_SLOW_SQL_RECORD_SQL Defines an obfuscation level for slow SQL queries. Valid options are obfuscated, raw, or none). slow_sql.use_longer_sql_id Type Boolean Default false Environ variable NEW_RELIC_SLOW_SQL_USE_LONGER_SQL_ID Generate a longer sql_id for slow SQL traces. sql_id is used for aggregation of similar queries. Span Events span_events.enabled Type Boolean Default true Environ variable NEW_RELIC_SPAN_EVENTS_ENABLED If true, enables span event sampling. span_events.queue_size Type Integer Default 10000 Environ variable NEW_RELIC_SPAN_EVENTS_QUEUE_SIZE Sets the maximum number of span events to buffer when streaming to the trace observer. span_events.max_samples_stored Type Integer Default 1000 Environ variable NEW_RELIC_SPAN_EVENTS_MAX_SAMPLES_STORED Defines the maximum number of span events reported from a single harvest. Strip Exception Messages strip_exception_messages.enabled Type Boolean Default (Dynamic) Environ variable NEW_RELIC_STRIP_EXCEPTION_MESSAGES_ENABLED If true, the agent strips messages from all exceptions except those in the allowlist. Enabled automatically in high security mode. strip_exception_messages.allowed_classes Type String Default \"\" Environ variable NEW_RELIC_STRIP_EXCEPTION_MESSAGES_ALLOWED_CLASSES Specify a list of exceptions you do not want the agent to strip when strip_exception_messages is true. Separate exceptions with a comma. For example, \"ImportantException,PreserveMessageException\". Thread Profiler thread_profiler.enabled Type Boolean Default (Dynamic) Environ variable NEW_RELIC_THREAD_PROFILER_ENABLED If true, enables use of the thread profiler. Utilization utilization.detect_aws Type Boolean Default true Environ variable NEW_RELIC_UTILIZATION_DETECT_AWS If true, the agent automatically detects that it is running in an AWS environment. utilization.detect_azure Type Boolean Default true Environ variable NEW_RELIC_UTILIZATION_DETECT_AZURE If true, the agent automatically detects that it is running in an Azure environment. utilization.detect_gcp Type Boolean Default true Environ variable NEW_RELIC_UTILIZATION_DETECT_GCP If true, the agent automatically detects that it is running in an Google Cloud Platform environment. utilization.detect_pcf Type Boolean Default true Environ variable NEW_RELIC_UTILIZATION_DETECT_PCF If true, the agent automatically detects that it is running in a Pivotal Cloud Foundry environment. utilization.detect_docker Type Boolean Default true Environ variable NEW_RELIC_UTILIZATION_DETECT_DOCKER If true, the agent automatically detects that it is running in Docker. utilization.detect_kubernetes Type Boolean Default true Environ variable NEW_RELIC_UTILIZATION_DETECT_KUBERNETES If true, the agent automatically detects that it is running in Kubernetes. For more help Additional documentation resources include: New Relic for Ruby (compatibility and requirements, installation, configuration, troubleshooting, known issues, advanced features and configuration, beta releases) Transaction traces and Configuring transaction traces (detailed information about New Relic's Transaction Traces feature)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 138.34393,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Instrumentation</em>",
        "body": " NEW_RELIC_DISABLE_RACK DEPRECATED Please see: <em>instrumentation</em>.rack. If true, prevents the agent <em>from</em> hooking into the to_app method in Rack::Builder to find gems to <em>instrument</em> during application startup. disable_rack_urlmap Type Boolean Default false Environ variable NEW_RELIC_DISABLE_RACK_URLMAP DEPRECATED"
      },
      "id": "603eb6f4e7b9d22a5f2f7c73"
    },
    {
      "sections": [
        "Custom instrumentation editor: Instrument from UI",
        "Requirements",
        "Define custom instrumentation",
        "Caution",
        "Manual instrumentation using the editor",
        "Important",
        "Deploy changes manually",
        "Page functions",
        "Instrumentation options",
        "Results with \"start\" option"
      ],
      "title": "Custom instrumentation editor: Instrument from UI",
      "type": "docs",
      "tags": [
        "Agents",
        "Java agent",
        "Custom instrumentation"
      ],
      "external_id": "979a3d068ba665b13e8e9e18c432356286d61248",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/java-agent/custom-instrumentation/custom-instrumentation-editor-instrument-ui/",
      "published_at": "2021-06-20T02:19:10Z",
      "updated_at": "2021-03-16T02:40:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's custom instrumentation editor allows Java app users to implement custom instrumentation via the New Relic user interface. The editor is the preferred choice when you cannot modify your application code and don't have that many methods to instrument. See Java custom instrumentation for other instrumentation options and the reasons for using each. To use the custom instrumentation editor: Go to one.newrelic.com > APM > (select a Java app) > Settings > Instrumentation. Use the custom instrumentation editor to: Instrument an unsupported framework. Gain additional insight into uninstrumented methods. Ignore particular transactions. Requirements To use the custom instrumentation editor, you must meet the following requirements: Requirement Comments Agent Java agent version 3.17.0 or higher Security Users of high security mode must export their instrumentation and manually import it to their app server. Define custom instrumentation To define custom instrumentation from the New Relic user interface, use a thread profiling session to collect detailed stack traces of each thread in your application. If possible, test your custom instrumentation in a pre-production environment before changing the instrumentation rules in your production app. In either environment, use the custom instrumentation editor to define the methods you want instrumented, and apply your changes: Create a new thread profiler session. To ensure you collect sufficient data, set the length of the session to at least two minutes. Go to one.newrelic.com > APM > (select an app) > Settings > Instrumentation. Scroll down to the bottom of the page until you see the Recently collected thread profiles list, then select the most recent thread profile. Expand individual methods to locate uninstrumented methods. To define instrumentation rules for particular nodes, select Instrument or Ignore, and customize the rules if necessary. To save your settings, select Confirm instrumentation changes. Deploy your changes from the Instrumentation page: To deploy your changes automatically, select Deploy instrumentation changes. To deploy your changes manually, select Export XML, and see exporting your instrumentation. Caution Avoid over-instrumenting whenever possible. With each additional method that is instrumented, the agent will be using more resources and your application will incur more overhead. In addition, deploying your instrumentation will cause a brief period of higher overhead. This can noticeably slow application requests for several seconds. If you applied your changes from the UI, the agent will begin instrumenting your methods within a few harvest cycles (typically a few minutes). Manual instrumentation using the editor You can also create instrumentation points directly in the editor without using a thread profile: From the custom instrumentation editor, select Add manual instrumentation to manually enter a class and method to be instrumented or ignored. Follow the custom instrumentation by XML rules when defining your instrumentation points. Deploy your changes from the instrumentation editor. Using this method to add instrumentation exposes additional functionality beyond what is available from a thread profile. In addition to matching methods by signature, you can also instrument methods by return type, methods on interfaces, and by Java annotation. These more complex instrumentation types can be created and deleted in the editor, but not edited. Important If a method is marked Instrumentation not allowed, follow New Relic's troubleshooting procedures for custom instrumentation. Deploy changes manually You can also use the custom instrumentation editor to build a custom instrumentation set, then export an instrumentation file and manually import it to your app server. This is required for users of high security mode. To export your instrumentation, define custom instrumentation via the UI. Then select Export xml from the Instrumentation page, and import the file on your app server. Page functions The Instrumentation page supports the following features: If you want to... Do this... Pause or disable custom instrumentation Select Disable instrumentation to temporarily disable all UI-defined custom instrumentation. Select Enable instrumentation to re-enable your instrumentation settings. Import existing instrumentation You can import an existing custom instrumentation xml file by selecting Import xml. You can also Export xml if you do not want to deploy your changes automatically. Edit or delete instrumentation points You cannot edit manual instrumentation, only delete it. Select Remove to stop instrumenting a particular method. Select Edit to change the instrumentation rules. View instrumentation history You can view each previous iteration of your custom instrumentation from the Instrumentation history tab, including who deployed changes and when. You can restore an old version by selecting export to download a copy of the custom instrumentation file, then importing it to the instrumentation editor. Instrumentation options You can define the following options with the custom instrumentation editor: Instrumentation options Comments Instrument methods Begin instrumenting the selected method. Instrumented methods will be visible in the New Relic UI. Instrument supports the following child options: Name the transaction (transaction name): Override the standard transaction name, defined by the automatic naming rules. The UI will instead use the listed name. Start the transaction when this method executes: Rather than including metrics from this metric inside its parent transaction, create a new transaction for this method. Agent behavior with this option depends on whether there is a pre-existing transaction on the thread. Report custom attributes Method parameters can be captured as attributes on a transaction. New Relic reports these attributes to transaction traces, traced errors, and New Relic One Transaction events. For security reasons, capturing custom attributes using the Custom Instrumentation Editor is disabled by default and cannot be enabled while you are using high security mode. If you want to report custom attributes using the custom instrumentation editor and you do not want the Java agent to be in High security mode, disable High security mode and then add the following text in the common: block of your newrelic.yml: reinstrument: attributes_enabled: true Copy Ignore transactions Ignore this method entirely. The agent will not report metrics from this method, and the method will not contribute to Apdex calculations. Results with \"start\" option If you select Instrument methods > Start the transaction when this method executes, agent behavior depends on whether there is a pre-existing transaction on the thread. When the class or method is instrumented: Is the \"Start the transaction\" flag checked? Yes No If a pre-existing transaction is on that thread and the Start the transaction flag is checked: The agent ignores the Start the transaction flag. The agent includes the class/method into the pre-existing transaction. If a pre-existing transaction is on that thread and the Start the transaction flag is not checked, the agent includes the class/method into the pre-existing transaction. If a transaction is not on that thread and the Start the transaction flag is checked: The agent discovers there is no current transaction. The agent creates a new transaction starting with the class/method you have instrumented. If a transaction is not on that thread and the Start the transaction flag is not checked: The agent looks for a transaction on that thread and does not find one. The metric is dropped.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.85419,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Custom <em>instrumentation</em> <em>editor</em>: <em>Instrument</em> <em>from</em> <em>UI</em>",
        "sections": "Custom <em>instrumentation</em> <em>editor</em>: <em>Instrument</em> <em>from</em> <em>UI</em>",
        "tags": "Custom <em>instrumentation</em>",
        "body": " define the following options with the custom <em>instrumentation</em> <em>editor</em>: <em>Instrumentation</em> options Comments <em>Instrument</em> methods Begin instrumenting the selected method. Instrumented methods will be visible in the New Relic <em>UI</em>. <em>Instrument</em> supports the following child options: Name the transaction (transaction"
      },
      "id": "603ed428e7b9d2b9b52a07e6"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/amazon-cognito-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48486,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19846,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84793,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/amazon-sqs-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48462,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19824,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84787,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/amazon-transit-gateway-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48462,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19824,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84787,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-albnlb-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.4844,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19806,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.8478,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-api-gateway-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.4844,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19806,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.8478,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration": [
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19785,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84772,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    },
    {
      "sections": [
        "AWS Auto Scaling monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Inventory data",
        "/aws/autoscaling/group inventory",
        "/aws/autoscaling/instance inventory",
        "/aws/autoscaling/launch-configuration inventory",
        "/aws/autoscaling/policy inventory",
        "/aws/autoscaling/region-limit inventory"
      ],
      "title": "AWS Auto Scaling monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4fc9ef9f703408a92adb47c1511c46c1d1550b21",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-auto-scaling-monitoring-integration/",
      "published_at": "2021-06-20T09:29:29Z",
      "updated_at": "2021-05-16T10:07:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Amazon Auto Scaling service allows launching or terminating Amazon EC2 instances automatically. It helps dynamically adapt Amazon EC2 capacity based on user-defined policies, schedules, and health checks. New Relic infrastructure integrations include an AWS Auto Scaling integration that reports data about groups from your Auto Scaling service to New Relic products. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration: Follow standard procedures to connect AWS services to New Relic. Enable Auto Scaling group metrics so that Amazon Auto Scaling will send sampled data to CloudWatch every minute. To enable group metrics using the console: Open the Amazon EC2 console at console.aws.amazon.com/ec2/. From the navigation pane, select Auto Scaling Groups > (select your group). From the Monitoring tab, select Auto Scaling Metrics > Enable Group Metrics Collection or Display > Auto Scaling. Configuration and polling Default polling information for the AWS Auto Scaling integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute by default To change the polling frequency and filter data, use configuration options. Find and use data To view integration data, go to one.newrelic.com > Infrastructure > AWS, then select one of the AWS Auto Scaling integration links. You can query and explore your data using these event types: AutoScalingInstanceSample AutoScalingGroupSample AutoScalingLaunchConfigurationSample AutoScalingPolicySample AutoScalingRegionLimitSample Metric data This New Relic infrastructure integration collects the following Amazon Auto Scaling data: Metric Description groupDesiredCapacity The number of instances that the Auto Scaling group attempts to maintain. groupInServiceInstances The number of instances that are running as part of the Auto Scaling group. groupMaxSize The maximum size of the Auto Scaling group. groupMinSize The minimum size of the Auto Scaling group. groupPendingInstances The number of instances that are pending and not yet in service. groupStandbyInstances The number of instances that are in a \"standby\" state. groupTerminatingInstances The number of instances that are in the process of terminating. groupTotalInstances The total number of instances in the Auto Scaling group. Inventory data Inventory data provides information about the service's state and configuration. Auto Scaling configuration options are reported as inventory data. /aws/autoscaling/group inventory Name Description autoScalingGroupArn The Amazon Resource Name (ARN) of the group. autoScalingGroupName The name of the group. awsRegion The region of the Auto Scaling group. desiredCapacity The preferred size of the group. enabledMetrics A list of structures representing the metrics enabled for the group. healthCheckPeriod The amount of time, in seconds, that Auto Scaling waits before checking the health status of an EC2 instance that has come into service. healthCheckType The service to use for the health checks. instances A list of structures representing the EC2 instances associated with the group. launchConfiguration The launch configuration associated with the instance. loadBalancers One or more load balancers associated with the group. maxSize The maximum size of the group. minSize The minimum size of the group. status The current state of the group when delete-auto-scaling-group is in progress. tags A list of structures representing the tags for the group. /aws/autoscaling/instance inventory Name Description autoScalingGroupName The group names. awsRegion The region of the Auto Scaling group. id The ID of the instance. launchConfigurationName The launch configuration used to launch the instance. lifeCycle The lifecycle state for the instance. status The last reported health status of this instance. /aws/autoscaling/launch-configuration inventory Name Description awsRegion The AWS region where the launch configuration is defined. createdAt The creation date and time for the launch configuration. kernelId The ID of the kernel associated with the AMI. keyName The name of the key pair. iamInstanceProfile The name or Amazon Resource Name (ARN) of the instance profile associated with the IAM role for the instance. imageId The ID of the Amazon Machine Image (AMI). instanceMonitoring A boolean. It will be true when the instances in this group are launched with detailed monitoring. instanceType The instance type for the instances. launchConfigurationArn The Amazon Resource Name (ARN) of the launch configuration. launchConfigurationName The name of the launch configuration. name The name of the launch configuration. placementTenacy The tenancy of the instance. securityGroups The security groups to associate with the instances. spotPrice The price to bid when launching Spot Instances. /aws/autoscaling/policy inventory Name Description adjustmentType The adjustment type, which specifies how ScalingAdjustment is interpreted. alarms A list of structures representing the CloudWatch alarms related to the policy. autoScalingGroupName The name of the Auto Scaling group associated with this scaling policy. awsRegion The AWS region where the policy is defined. coolDown The amount of time, in seconds, after a scaling activity completes before any further dynamic scaling activities can start. estimatedWarmUp The estimated time, in seconds, until a newly launched instance can contribute to the CloudWatch metrics. metricAggregationType The aggregation type for the CloudWatch metrics. name The name of the Auto Scaling group. policyArn The Amazon Resource Name (ARN) of the policy. policyType The policy type. stepAdjustments list of structures A list of structures representing a set of adjustments that allow scaling based on the size of the alarm breach. /aws/autoscaling/region-limit inventory Name Description accountName AWS account name. autoScalingGroups The current number of groups for the AWS account. awsRegion The AWS region where the region limit is defined. launchConfigurations The current number of launch configurations for the AWS account. maxAutoScalingGroups The maximum number of groups allowed for the AWS account. maxLaunchConfigurations The maximum number of launch configurations allowed for the AWS account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.17056,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> Auto Scaling monitoring <em>integration</em>",
        "sections": "<em>AWS</em> Auto Scaling monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "The <em>Amazon</em> Auto Scaling service allows launching or terminating <em>Amazon</em> EC2 instances automatically. It helps dynamically adapt <em>Amazon</em> EC2 capacity based on user-defined policies, schedules, and health checks. New Relic infrastructure <em>integrations</em> include an <em>AWS</em> Auto Scaling integration that reports"
      },
      "id": "603e863728ccbc8b0aeba779"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-athena-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48419,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19785,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84772,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-auto-scaling-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48395,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19763,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84764,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-billing-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48395,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19763,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84764,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-cloudformation-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48373,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19742,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84756,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-cloudfront-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48373,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19742,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84756,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-cloudtrail-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48352,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.1972,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84749,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-connect-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48352,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.1972,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84749,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-direct-connect-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48328,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19699,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84741,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-documentdb-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48328,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19699,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84741,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-dynamodb-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48306,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19678,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84734,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-ebs-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48285,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19656,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84726,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-ec2-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48285,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19656,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84726,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-ecsecr-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.4826,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84718,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-efs-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.4826,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84718,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-elastic-beanstalk-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48239,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19614,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.8471,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticache-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48239,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19614,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.8471,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48218,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84703,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    },
    {
      "sections": [
        "AWS Auto Scaling monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Inventory data",
        "/aws/autoscaling/group inventory",
        "/aws/autoscaling/instance inventory",
        "/aws/autoscaling/launch-configuration inventory",
        "/aws/autoscaling/policy inventory",
        "/aws/autoscaling/region-limit inventory"
      ],
      "title": "AWS Auto Scaling monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4fc9ef9f703408a92adb47c1511c46c1d1550b21",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-auto-scaling-monitoring-integration/",
      "published_at": "2021-06-20T09:29:29Z",
      "updated_at": "2021-05-16T10:07:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Amazon Auto Scaling service allows launching or terminating Amazon EC2 instances automatically. It helps dynamically adapt Amazon EC2 capacity based on user-defined policies, schedules, and health checks. New Relic infrastructure integrations include an AWS Auto Scaling integration that reports data about groups from your Auto Scaling service to New Relic products. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration: Follow standard procedures to connect AWS services to New Relic. Enable Auto Scaling group metrics so that Amazon Auto Scaling will send sampled data to CloudWatch every minute. To enable group metrics using the console: Open the Amazon EC2 console at console.aws.amazon.com/ec2/. From the navigation pane, select Auto Scaling Groups > (select your group). From the Monitoring tab, select Auto Scaling Metrics > Enable Group Metrics Collection or Display > Auto Scaling. Configuration and polling Default polling information for the AWS Auto Scaling integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute by default To change the polling frequency and filter data, use configuration options. Find and use data To view integration data, go to one.newrelic.com > Infrastructure > AWS, then select one of the AWS Auto Scaling integration links. You can query and explore your data using these event types: AutoScalingInstanceSample AutoScalingGroupSample AutoScalingLaunchConfigurationSample AutoScalingPolicySample AutoScalingRegionLimitSample Metric data This New Relic infrastructure integration collects the following Amazon Auto Scaling data: Metric Description groupDesiredCapacity The number of instances that the Auto Scaling group attempts to maintain. groupInServiceInstances The number of instances that are running as part of the Auto Scaling group. groupMaxSize The maximum size of the Auto Scaling group. groupMinSize The minimum size of the Auto Scaling group. groupPendingInstances The number of instances that are pending and not yet in service. groupStandbyInstances The number of instances that are in a \"standby\" state. groupTerminatingInstances The number of instances that are in the process of terminating. groupTotalInstances The total number of instances in the Auto Scaling group. Inventory data Inventory data provides information about the service's state and configuration. Auto Scaling configuration options are reported as inventory data. /aws/autoscaling/group inventory Name Description autoScalingGroupArn The Amazon Resource Name (ARN) of the group. autoScalingGroupName The name of the group. awsRegion The region of the Auto Scaling group. desiredCapacity The preferred size of the group. enabledMetrics A list of structures representing the metrics enabled for the group. healthCheckPeriod The amount of time, in seconds, that Auto Scaling waits before checking the health status of an EC2 instance that has come into service. healthCheckType The service to use for the health checks. instances A list of structures representing the EC2 instances associated with the group. launchConfiguration The launch configuration associated with the instance. loadBalancers One or more load balancers associated with the group. maxSize The maximum size of the group. minSize The minimum size of the group. status The current state of the group when delete-auto-scaling-group is in progress. tags A list of structures representing the tags for the group. /aws/autoscaling/instance inventory Name Description autoScalingGroupName The group names. awsRegion The region of the Auto Scaling group. id The ID of the instance. launchConfigurationName The launch configuration used to launch the instance. lifeCycle The lifecycle state for the instance. status The last reported health status of this instance. /aws/autoscaling/launch-configuration inventory Name Description awsRegion The AWS region where the launch configuration is defined. createdAt The creation date and time for the launch configuration. kernelId The ID of the kernel associated with the AMI. keyName The name of the key pair. iamInstanceProfile The name or Amazon Resource Name (ARN) of the instance profile associated with the IAM role for the instance. imageId The ID of the Amazon Machine Image (AMI). instanceMonitoring A boolean. It will be true when the instances in this group are launched with detailed monitoring. instanceType The instance type for the instances. launchConfigurationArn The Amazon Resource Name (ARN) of the launch configuration. launchConfigurationName The name of the launch configuration. name The name of the launch configuration. placementTenacy The tenancy of the instance. securityGroups The security groups to associate with the instances. spotPrice The price to bid when launching Spot Instances. /aws/autoscaling/policy inventory Name Description adjustmentType The adjustment type, which specifies how ScalingAdjustment is interpreted. alarms A list of structures representing the CloudWatch alarms related to the policy. autoScalingGroupName The name of the Auto Scaling group associated with this scaling policy. awsRegion The AWS region where the policy is defined. coolDown The amount of time, in seconds, after a scaling activity completes before any further dynamic scaling activities can start. estimatedWarmUp The estimated time, in seconds, until a newly launched instance can contribute to the CloudWatch metrics. metricAggregationType The aggregation type for the CloudWatch metrics. name The name of the Auto Scaling group. policyArn The Amazon Resource Name (ARN) of the policy. policyType The policy type. stepAdjustments list of structures A list of structures representing a set of adjustments that allow scaling based on the size of the alarm breach. /aws/autoscaling/region-limit inventory Name Description accountName AWS account name. autoScalingGroups The current number of groups for the AWS account. awsRegion The AWS region where the region limit is defined. launchConfigurations The current number of launch configurations for the AWS account. maxAutoScalingGroups The maximum number of groups allowed for the AWS account. maxLaunchConfigurations The maximum number of launch configurations allowed for the AWS account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.1705,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> Auto Scaling monitoring <em>integration</em>",
        "sections": "<em>AWS</em> Auto Scaling monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "The <em>Amazon</em> Auto Scaling service allows launching or terminating <em>Amazon</em> EC2 instances automatically. It helps dynamically adapt <em>Amazon</em> EC2 capacity based on user-defined policies, schedules, and health checks. New Relic infrastructure <em>integrations</em> include an <em>AWS</em> Auto Scaling integration that reports"
      },
      "id": "603e863728ccbc8b0aeba779"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-elb-classic-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48218,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19592,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84703,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-elemental-mediaconvert-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48193,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19571,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84695,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-elemental-mediapackage-vod-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48193,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19571,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84695,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-emr-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48172,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19553,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84688,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-fsx-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48172,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19553,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84688,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-glue-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.4815,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19531,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.8468,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-health-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.4815,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19531,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.8468,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-iam-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48126,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.1951,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84673,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-iot-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48126,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.1951,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84673,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-kinesis-data-analytics-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48105,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19489,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84665,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-kinesis-data-firehose-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48105,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19489,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84665,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-kinesis-data-streams-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48083,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19467,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84657,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-lambda-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48083,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19467,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84657,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-managed-kafka-msk-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48059,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19446,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.8465,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48059,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19446,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "AWS Auto Scaling monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Inventory data",
        "/aws/autoscaling/group inventory",
        "/aws/autoscaling/instance inventory",
        "/aws/autoscaling/launch-configuration inventory",
        "/aws/autoscaling/policy inventory",
        "/aws/autoscaling/region-limit inventory"
      ],
      "title": "AWS Auto Scaling monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4fc9ef9f703408a92adb47c1511c46c1d1550b21",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-auto-scaling-monitoring-integration/",
      "published_at": "2021-06-20T09:29:29Z",
      "updated_at": "2021-05-16T10:07:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Amazon Auto Scaling service allows launching or terminating Amazon EC2 instances automatically. It helps dynamically adapt Amazon EC2 capacity based on user-defined policies, schedules, and health checks. New Relic infrastructure integrations include an AWS Auto Scaling integration that reports data about groups from your Auto Scaling service to New Relic products. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration: Follow standard procedures to connect AWS services to New Relic. Enable Auto Scaling group metrics so that Amazon Auto Scaling will send sampled data to CloudWatch every minute. To enable group metrics using the console: Open the Amazon EC2 console at console.aws.amazon.com/ec2/. From the navigation pane, select Auto Scaling Groups > (select your group). From the Monitoring tab, select Auto Scaling Metrics > Enable Group Metrics Collection or Display > Auto Scaling. Configuration and polling Default polling information for the AWS Auto Scaling integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute by default To change the polling frequency and filter data, use configuration options. Find and use data To view integration data, go to one.newrelic.com > Infrastructure > AWS, then select one of the AWS Auto Scaling integration links. You can query and explore your data using these event types: AutoScalingInstanceSample AutoScalingGroupSample AutoScalingLaunchConfigurationSample AutoScalingPolicySample AutoScalingRegionLimitSample Metric data This New Relic infrastructure integration collects the following Amazon Auto Scaling data: Metric Description groupDesiredCapacity The number of instances that the Auto Scaling group attempts to maintain. groupInServiceInstances The number of instances that are running as part of the Auto Scaling group. groupMaxSize The maximum size of the Auto Scaling group. groupMinSize The minimum size of the Auto Scaling group. groupPendingInstances The number of instances that are pending and not yet in service. groupStandbyInstances The number of instances that are in a \"standby\" state. groupTerminatingInstances The number of instances that are in the process of terminating. groupTotalInstances The total number of instances in the Auto Scaling group. Inventory data Inventory data provides information about the service's state and configuration. Auto Scaling configuration options are reported as inventory data. /aws/autoscaling/group inventory Name Description autoScalingGroupArn The Amazon Resource Name (ARN) of the group. autoScalingGroupName The name of the group. awsRegion The region of the Auto Scaling group. desiredCapacity The preferred size of the group. enabledMetrics A list of structures representing the metrics enabled for the group. healthCheckPeriod The amount of time, in seconds, that Auto Scaling waits before checking the health status of an EC2 instance that has come into service. healthCheckType The service to use for the health checks. instances A list of structures representing the EC2 instances associated with the group. launchConfiguration The launch configuration associated with the instance. loadBalancers One or more load balancers associated with the group. maxSize The maximum size of the group. minSize The minimum size of the group. status The current state of the group when delete-auto-scaling-group is in progress. tags A list of structures representing the tags for the group. /aws/autoscaling/instance inventory Name Description autoScalingGroupName The group names. awsRegion The region of the Auto Scaling group. id The ID of the instance. launchConfigurationName The launch configuration used to launch the instance. lifeCycle The lifecycle state for the instance. status The last reported health status of this instance. /aws/autoscaling/launch-configuration inventory Name Description awsRegion The AWS region where the launch configuration is defined. createdAt The creation date and time for the launch configuration. kernelId The ID of the kernel associated with the AMI. keyName The name of the key pair. iamInstanceProfile The name or Amazon Resource Name (ARN) of the instance profile associated with the IAM role for the instance. imageId The ID of the Amazon Machine Image (AMI). instanceMonitoring A boolean. It will be true when the instances in this group are launched with detailed monitoring. instanceType The instance type for the instances. launchConfigurationArn The Amazon Resource Name (ARN) of the launch configuration. launchConfigurationName The name of the launch configuration. name The name of the launch configuration. placementTenacy The tenancy of the instance. securityGroups The security groups to associate with the instances. spotPrice The price to bid when launching Spot Instances. /aws/autoscaling/policy inventory Name Description adjustmentType The adjustment type, which specifies how ScalingAdjustment is interpreted. alarms A list of structures representing the CloudWatch alarms related to the policy. autoScalingGroupName The name of the Auto Scaling group associated with this scaling policy. awsRegion The AWS region where the policy is defined. coolDown The amount of time, in seconds, after a scaling activity completes before any further dynamic scaling activities can start. estimatedWarmUp The estimated time, in seconds, until a newly launched instance can contribute to the CloudWatch metrics. metricAggregationType The aggregation type for the CloudWatch metrics. name The name of the Auto Scaling group. policyArn The Amazon Resource Name (ARN) of the policy. policyType The policy type. stepAdjustments list of structures A list of structures representing a set of adjustments that allow scaling based on the size of the alarm breach. /aws/autoscaling/region-limit inventory Name Description accountName AWS account name. autoScalingGroups The current number of groups for the AWS account. awsRegion The AWS region where the region limit is defined. launchConfigurations The current number of launch configurations for the AWS account. maxAutoScalingGroups The maximum number of groups allowed for the AWS account. maxLaunchConfigurations The maximum number of launch configurations allowed for the AWS account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.17045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> Auto Scaling monitoring <em>integration</em>",
        "sections": "<em>AWS</em> Auto Scaling monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "The <em>Amazon</em> Auto Scaling service allows launching or terminating <em>Amazon</em> EC2 instances automatically. It helps dynamically adapt <em>Amazon</em> EC2 capacity based on user-defined policies, schedules, and health checks. New Relic infrastructure <em>integrations</em> include an <em>AWS</em> Auto Scaling integration that reports"
      },
      "id": "603e863728ccbc8b0aeba779"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-mq-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48038,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19424,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84642,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-neptune-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48038,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19424,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84642,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-qldb-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.48016,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19403,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84634,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-rds-enhanced-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.47992,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19382,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84628,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-rds-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.47992,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19382,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84628,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-redshift-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.4797,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.1936,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84619,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-route-53-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.4797,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.1936,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84619,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-route53-resolver-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.47949,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19339,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84613,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-s3-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.47949,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19339,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.8464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-simple-email-service-ses-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.47925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.1932,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84633,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-sns-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.47925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.1932,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84633,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-step-functions-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.47903,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.193,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-trusted-advisor-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.47903,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.193,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-vpc-flow-logs-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.47879,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19278,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84618,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-vpc-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.47879,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19278,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.84618,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-waf-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.47858,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19257,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.8461,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/aws-x-ray-monitoring-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.47858,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "sections": "<em>AWS</em> AppSync monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the <em>AWS</em> AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.19257,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.8461,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>AWS</em> <em>integrations</em> <em>list</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with <em>AWS</em> to collect performance metrics and metadata for more than 50 <em>AWS</em> services. With the new <em>AWS</em> Metric Streams integration, you only need a single service, <em>AWS</em> CloudWatch, to gather all <em>AWS</em> metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/aws-integrations-list/rate-limit-alerts-amazon": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.431725,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 122.232285,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.32126,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": ". In order to continue collecting that additional data, keep those enabled as polling <em>integrations</em>. <em>Troubleshooting</em> No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/get-started/aws-integrations-metrics": [
    {
      "sections": [
        "Use integration data in New Relic dashboards",
        "Get started with integration data",
        "Example NRQL queries",
        "AWS EBS query example",
        "Azure Service Bus query example",
        "Azure Functions query example",
        "Azure VMs query example",
        "NGINX query example",
        "MySQL query example",
        "Inventory change query example",
        "Tip",
        "Tips for using different data types",
        "Metric data tips",
        "Event data tips",
        "Inventory data tips"
      ],
      "title": "Use integration data in New Relic dashboards",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "b236b0fae29853de085d0430fdec27fba74c15d4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards/",
      "published_at": "2021-06-20T19:34:58Z",
      "updated_at": "2021-06-20T19:34:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Most data generated by integrations is available in New Relic One dashboards, where you can query your data using NRQL and build custom dashboards. The following tips and sample queries were created for New Relic-built integrations, but most will also apply to integrations built with the Integrations SDK. For a general look at how to find and use integration data, see New Relic data types. Get started with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com > Infrastructure > Third-party services page, select an integration dashboard. There, you can view the NRQL queries that generated a chart. For examples of NRQL queries for integration data, see the example queries. Use the data explorer or the dashboards to explore and understand the available data. Read the documentation for a specific integration to learn about the reported data. When you create a useful query you'd like to add to your Insights dashboard, select Add to dashboard. Example NRQL queries Here are some examples of NRQL queries that use integration data: AWS EBS query example Here's a NRQL query for the AWS EBS service, showing the total write time metric, faceted by entityName: SELECT sum('provider.volumeTotalWriteTime.Sum') FROM BlockDeviceSample WHERE provider = 'EbsVolume' FACET entityName Copy Azure Service Bus query example Here's an Insights NRQL query for the maximum number of messages in an Azure Service Bus topic queue, faceted by resource group: SELECT max(activeMessages.Maximum) FROM AzureServiceBusTopicSample FACET resourceGroupName Copy Azure Functions query example Here's an Insights NRQL query for Azure Functions, showing the count of executed functions over the past six hours by region over time: SELECT sum(functionExecutionCount.Total) FROM AzureFunctionsAppSample FACET regionName TIMESERIES SINCE 6 hours ago Copy Azure VMs query example Here's an Insights NRQL query for Azure VMs that compares the count of VM events over the past thirty minutes with the same time a week ago: SELECT uniqueCount(vMName) FROM AzureVirtualMachineScaleSetSample FACET name SINCE 30 minutes ago COMPARE WITH 1 week ago Copy NGINX query example Here's an example of a query that you might run on your NGINX integration data and place in an Insights dashboard. This query creates a chart showing the average value of NGINX requests per second over time: SELECT average(net.requestsPerSecond) FROM NginxSample TIMESERIES Copy For more on how to create queries, see NRQL syntax. MySQL query example Here's an example of a query that you might run on your MySQL integration data. This query generates a chart showing the maximum number of used MySQL connections: SELECT max(net.maxUsedConnections) FROM MysqlSample Copy For more on how to create queries, see NRQL syntax. Inventory change query example Here's an example of a query that groups inventory change events from the last day by the type of change: SELECT count(*) FROM InfrastructureEvent WHERE format='inventoryChange' FACET changeType SINCE 1 DAY AGO Copy Tip You can also perform these queries using dimensional metrics. Tips for using different data types Integrations can generate metric, event, and inventory data, all of which are available for querying in New Relic Insights. Here are some tips for using the different types of integration data in Insights: Metric data tips Tips for finding and using integration metric data in Insights: All integration data is attached to a data type known as an event (not to be confused with events reported by integrations, which represent important activity in your host/service). This means that all integration data can be found via the data explorer. For more about these two basic New Relic data types, see New Relic data collection. Metric values are treated as attributes: key-value pairs attached to an event. For example, the MySQL integration has an 'active connections' metric; this would be found by querying the connectionsActive attribute of the MysqlSample event. For general information about metrics, see Integration metric data. Event data tips Here are some tips for finding and using integration event data in Insights: Most integration events are inventory change events. When inventory is changed, it generates an InfrastructureEvent with a format value of inventoryChange. Integration data can be found via the data explorer. For general information about events, see Event data. Inventory data tips Here are some tips for finding and using integration inventory data in Insights: For general information about inventory data, see Integration inventory data. Some inventory data is added as attributes (key-value pairs) to Insights data. For example, the AWS EC2 integration collects awsRegion as inventory data; this would be found in Insights by querying the awsRegion attribute of the ComputeSample event type and provider Ec2Instance. When inventory data changes, an Insights InfrastructureEvent event type is generated with a format value of inventoryChange. See the query examples for an example of querying this data. For more on using NRQL queries, see Intro to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.82668,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>integration</em> data in New Relic dashboards",
        "sections": "<em>Get</em> <em>started</em> with <em>integration</em> data",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " with the <em>Integrations</em> SDK. For a general look at how to find and use integration data, see New Relic data types. <em>Get</em> <em>started</em> with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com &gt; Infrastructure &gt; Third-party services page, select"
      },
      "id": "60450a39196a67d7dc960f7c"
    },
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.86671,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-06-20T12:42:18Z",
      "updated_at": "2021-06-20T12:42:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.15749,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate Azure <em>integrations</em>",
        "sections": "Activate Azure <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The Microsoft Azure <em>integrations</em> report data from various Azure platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    }
  ],
  "/docs/integrations/amazon-integrations/get-started/connect-aws-govcloud-new-relic": [
    {
      "sections": [
        "Use integration data in New Relic dashboards",
        "Get started with integration data",
        "Example NRQL queries",
        "AWS EBS query example",
        "Azure Service Bus query example",
        "Azure Functions query example",
        "Azure VMs query example",
        "NGINX query example",
        "MySQL query example",
        "Inventory change query example",
        "Tip",
        "Tips for using different data types",
        "Metric data tips",
        "Event data tips",
        "Inventory data tips"
      ],
      "title": "Use integration data in New Relic dashboards",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "b236b0fae29853de085d0430fdec27fba74c15d4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards/",
      "published_at": "2021-06-20T19:34:58Z",
      "updated_at": "2021-06-20T19:34:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Most data generated by integrations is available in New Relic One dashboards, where you can query your data using NRQL and build custom dashboards. The following tips and sample queries were created for New Relic-built integrations, but most will also apply to integrations built with the Integrations SDK. For a general look at how to find and use integration data, see New Relic data types. Get started with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com > Infrastructure > Third-party services page, select an integration dashboard. There, you can view the NRQL queries that generated a chart. For examples of NRQL queries for integration data, see the example queries. Use the data explorer or the dashboards to explore and understand the available data. Read the documentation for a specific integration to learn about the reported data. When you create a useful query you'd like to add to your Insights dashboard, select Add to dashboard. Example NRQL queries Here are some examples of NRQL queries that use integration data: AWS EBS query example Here's a NRQL query for the AWS EBS service, showing the total write time metric, faceted by entityName: SELECT sum('provider.volumeTotalWriteTime.Sum') FROM BlockDeviceSample WHERE provider = 'EbsVolume' FACET entityName Copy Azure Service Bus query example Here's an Insights NRQL query for the maximum number of messages in an Azure Service Bus topic queue, faceted by resource group: SELECT max(activeMessages.Maximum) FROM AzureServiceBusTopicSample FACET resourceGroupName Copy Azure Functions query example Here's an Insights NRQL query for Azure Functions, showing the count of executed functions over the past six hours by region over time: SELECT sum(functionExecutionCount.Total) FROM AzureFunctionsAppSample FACET regionName TIMESERIES SINCE 6 hours ago Copy Azure VMs query example Here's an Insights NRQL query for Azure VMs that compares the count of VM events over the past thirty minutes with the same time a week ago: SELECT uniqueCount(vMName) FROM AzureVirtualMachineScaleSetSample FACET name SINCE 30 minutes ago COMPARE WITH 1 week ago Copy NGINX query example Here's an example of a query that you might run on your NGINX integration data and place in an Insights dashboard. This query creates a chart showing the average value of NGINX requests per second over time: SELECT average(net.requestsPerSecond) FROM NginxSample TIMESERIES Copy For more on how to create queries, see NRQL syntax. MySQL query example Here's an example of a query that you might run on your MySQL integration data. This query generates a chart showing the maximum number of used MySQL connections: SELECT max(net.maxUsedConnections) FROM MysqlSample Copy For more on how to create queries, see NRQL syntax. Inventory change query example Here's an example of a query that groups inventory change events from the last day by the type of change: SELECT count(*) FROM InfrastructureEvent WHERE format='inventoryChange' FACET changeType SINCE 1 DAY AGO Copy Tip You can also perform these queries using dimensional metrics. Tips for using different data types Integrations can generate metric, event, and inventory data, all of which are available for querying in New Relic Insights. Here are some tips for using the different types of integration data in Insights: Metric data tips Tips for finding and using integration metric data in Insights: All integration data is attached to a data type known as an event (not to be confused with events reported by integrations, which represent important activity in your host/service). This means that all integration data can be found via the data explorer. For more about these two basic New Relic data types, see New Relic data collection. Metric values are treated as attributes: key-value pairs attached to an event. For example, the MySQL integration has an 'active connections' metric; this would be found by querying the connectionsActive attribute of the MysqlSample event. For general information about metrics, see Integration metric data. Event data tips Here are some tips for finding and using integration event data in Insights: Most integration events are inventory change events. When inventory is changed, it generates an InfrastructureEvent with a format value of inventoryChange. Integration data can be found via the data explorer. For general information about events, see Event data. Inventory data tips Here are some tips for finding and using integration inventory data in Insights: For general information about inventory data, see Integration inventory data. Some inventory data is added as attributes (key-value pairs) to Insights data. For example, the AWS EC2 integration collects awsRegion as inventory data; this would be found in Insights by querying the awsRegion attribute of the ComputeSample event type and provider Ec2Instance. When inventory data changes, an Insights InfrastructureEvent event type is generated with a format value of inventoryChange. See the query examples for an example of querying this data. For more on using NRQL queries, see Intro to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.82654,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>integration</em> data in New Relic dashboards",
        "sections": "<em>Get</em> <em>started</em> with <em>integration</em> data",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " with the <em>Integrations</em> SDK. For a general look at how to find and use integration data, see New Relic data types. <em>Get</em> <em>started</em> with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com &gt; Infrastructure &gt; Third-party services page, select"
      },
      "id": "60450a39196a67d7dc960f7c"
    },
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.8666,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-06-20T12:42:18Z",
      "updated_at": "2021-06-20T12:42:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.15738,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate Azure <em>integrations</em>",
        "sections": "Activate Azure <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The Microsoft Azure <em>integrations</em> report data from various Azure platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    }
  ],
  "/docs/integrations/amazon-integrations/get-started/connect-aws-new-relic-infrastructure-monitoring": [
    {
      "sections": [
        "Use integration data in New Relic dashboards",
        "Get started with integration data",
        "Example NRQL queries",
        "AWS EBS query example",
        "Azure Service Bus query example",
        "Azure Functions query example",
        "Azure VMs query example",
        "NGINX query example",
        "MySQL query example",
        "Inventory change query example",
        "Tip",
        "Tips for using different data types",
        "Metric data tips",
        "Event data tips",
        "Inventory data tips"
      ],
      "title": "Use integration data in New Relic dashboards",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "b236b0fae29853de085d0430fdec27fba74c15d4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards/",
      "published_at": "2021-06-20T19:34:58Z",
      "updated_at": "2021-06-20T19:34:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Most data generated by integrations is available in New Relic One dashboards, where you can query your data using NRQL and build custom dashboards. The following tips and sample queries were created for New Relic-built integrations, but most will also apply to integrations built with the Integrations SDK. For a general look at how to find and use integration data, see New Relic data types. Get started with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com > Infrastructure > Third-party services page, select an integration dashboard. There, you can view the NRQL queries that generated a chart. For examples of NRQL queries for integration data, see the example queries. Use the data explorer or the dashboards to explore and understand the available data. Read the documentation for a specific integration to learn about the reported data. When you create a useful query you'd like to add to your Insights dashboard, select Add to dashboard. Example NRQL queries Here are some examples of NRQL queries that use integration data: AWS EBS query example Here's a NRQL query for the AWS EBS service, showing the total write time metric, faceted by entityName: SELECT sum('provider.volumeTotalWriteTime.Sum') FROM BlockDeviceSample WHERE provider = 'EbsVolume' FACET entityName Copy Azure Service Bus query example Here's an Insights NRQL query for the maximum number of messages in an Azure Service Bus topic queue, faceted by resource group: SELECT max(activeMessages.Maximum) FROM AzureServiceBusTopicSample FACET resourceGroupName Copy Azure Functions query example Here's an Insights NRQL query for Azure Functions, showing the count of executed functions over the past six hours by region over time: SELECT sum(functionExecutionCount.Total) FROM AzureFunctionsAppSample FACET regionName TIMESERIES SINCE 6 hours ago Copy Azure VMs query example Here's an Insights NRQL query for Azure VMs that compares the count of VM events over the past thirty minutes with the same time a week ago: SELECT uniqueCount(vMName) FROM AzureVirtualMachineScaleSetSample FACET name SINCE 30 minutes ago COMPARE WITH 1 week ago Copy NGINX query example Here's an example of a query that you might run on your NGINX integration data and place in an Insights dashboard. This query creates a chart showing the average value of NGINX requests per second over time: SELECT average(net.requestsPerSecond) FROM NginxSample TIMESERIES Copy For more on how to create queries, see NRQL syntax. MySQL query example Here's an example of a query that you might run on your MySQL integration data. This query generates a chart showing the maximum number of used MySQL connections: SELECT max(net.maxUsedConnections) FROM MysqlSample Copy For more on how to create queries, see NRQL syntax. Inventory change query example Here's an example of a query that groups inventory change events from the last day by the type of change: SELECT count(*) FROM InfrastructureEvent WHERE format='inventoryChange' FACET changeType SINCE 1 DAY AGO Copy Tip You can also perform these queries using dimensional metrics. Tips for using different data types Integrations can generate metric, event, and inventory data, all of which are available for querying in New Relic Insights. Here are some tips for using the different types of integration data in Insights: Metric data tips Tips for finding and using integration metric data in Insights: All integration data is attached to a data type known as an event (not to be confused with events reported by integrations, which represent important activity in your host/service). This means that all integration data can be found via the data explorer. For more about these two basic New Relic data types, see New Relic data collection. Metric values are treated as attributes: key-value pairs attached to an event. For example, the MySQL integration has an 'active connections' metric; this would be found by querying the connectionsActive attribute of the MysqlSample event. For general information about metrics, see Integration metric data. Event data tips Here are some tips for finding and using integration event data in Insights: Most integration events are inventory change events. When inventory is changed, it generates an InfrastructureEvent with a format value of inventoryChange. Integration data can be found via the data explorer. For general information about events, see Event data. Inventory data tips Here are some tips for finding and using integration inventory data in Insights: For general information about inventory data, see Integration inventory data. Some inventory data is added as attributes (key-value pairs) to Insights data. For example, the AWS EC2 integration collects awsRegion as inventory data; this would be found in Insights by querying the awsRegion attribute of the ComputeSample event type and provider Ec2Instance. When inventory data changes, an Insights InfrastructureEvent event type is generated with a format value of inventoryChange. See the query examples for an example of querying this data. For more on using NRQL queries, see Intro to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.82654,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>integration</em> data in New Relic dashboards",
        "sections": "<em>Get</em> <em>started</em> with <em>integration</em> data",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " with the <em>Integrations</em> SDK. For a general look at how to find and use integration data, see New Relic data types. <em>Get</em> <em>started</em> with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com &gt; Infrastructure &gt; Third-party services page, select"
      },
      "id": "60450a39196a67d7dc960f7c"
    },
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.8666,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-06-20T12:42:18Z",
      "updated_at": "2021-06-20T12:42:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.15738,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate Azure <em>integrations</em>",
        "sections": "Activate Azure <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The Microsoft Azure <em>integrations</em> report data from various Azure platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    }
  ],
  "/docs/integrations/amazon-integrations/get-started/integrations-managed-policies": [
    {
      "sections": [
        "Use integration data in New Relic dashboards",
        "Get started with integration data",
        "Example NRQL queries",
        "AWS EBS query example",
        "Azure Service Bus query example",
        "Azure Functions query example",
        "Azure VMs query example",
        "NGINX query example",
        "MySQL query example",
        "Inventory change query example",
        "Tip",
        "Tips for using different data types",
        "Metric data tips",
        "Event data tips",
        "Inventory data tips"
      ],
      "title": "Use integration data in New Relic dashboards",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "b236b0fae29853de085d0430fdec27fba74c15d4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards/",
      "published_at": "2021-06-20T19:34:58Z",
      "updated_at": "2021-06-20T19:34:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Most data generated by integrations is available in New Relic One dashboards, where you can query your data using NRQL and build custom dashboards. The following tips and sample queries were created for New Relic-built integrations, but most will also apply to integrations built with the Integrations SDK. For a general look at how to find and use integration data, see New Relic data types. Get started with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com > Infrastructure > Third-party services page, select an integration dashboard. There, you can view the NRQL queries that generated a chart. For examples of NRQL queries for integration data, see the example queries. Use the data explorer or the dashboards to explore and understand the available data. Read the documentation for a specific integration to learn about the reported data. When you create a useful query you'd like to add to your Insights dashboard, select Add to dashboard. Example NRQL queries Here are some examples of NRQL queries that use integration data: AWS EBS query example Here's a NRQL query for the AWS EBS service, showing the total write time metric, faceted by entityName: SELECT sum('provider.volumeTotalWriteTime.Sum') FROM BlockDeviceSample WHERE provider = 'EbsVolume' FACET entityName Copy Azure Service Bus query example Here's an Insights NRQL query for the maximum number of messages in an Azure Service Bus topic queue, faceted by resource group: SELECT max(activeMessages.Maximum) FROM AzureServiceBusTopicSample FACET resourceGroupName Copy Azure Functions query example Here's an Insights NRQL query for Azure Functions, showing the count of executed functions over the past six hours by region over time: SELECT sum(functionExecutionCount.Total) FROM AzureFunctionsAppSample FACET regionName TIMESERIES SINCE 6 hours ago Copy Azure VMs query example Here's an Insights NRQL query for Azure VMs that compares the count of VM events over the past thirty minutes with the same time a week ago: SELECT uniqueCount(vMName) FROM AzureVirtualMachineScaleSetSample FACET name SINCE 30 minutes ago COMPARE WITH 1 week ago Copy NGINX query example Here's an example of a query that you might run on your NGINX integration data and place in an Insights dashboard. This query creates a chart showing the average value of NGINX requests per second over time: SELECT average(net.requestsPerSecond) FROM NginxSample TIMESERIES Copy For more on how to create queries, see NRQL syntax. MySQL query example Here's an example of a query that you might run on your MySQL integration data. This query generates a chart showing the maximum number of used MySQL connections: SELECT max(net.maxUsedConnections) FROM MysqlSample Copy For more on how to create queries, see NRQL syntax. Inventory change query example Here's an example of a query that groups inventory change events from the last day by the type of change: SELECT count(*) FROM InfrastructureEvent WHERE format='inventoryChange' FACET changeType SINCE 1 DAY AGO Copy Tip You can also perform these queries using dimensional metrics. Tips for using different data types Integrations can generate metric, event, and inventory data, all of which are available for querying in New Relic Insights. Here are some tips for using the different types of integration data in Insights: Metric data tips Tips for finding and using integration metric data in Insights: All integration data is attached to a data type known as an event (not to be confused with events reported by integrations, which represent important activity in your host/service). This means that all integration data can be found via the data explorer. For more about these two basic New Relic data types, see New Relic data collection. Metric values are treated as attributes: key-value pairs attached to an event. For example, the MySQL integration has an 'active connections' metric; this would be found by querying the connectionsActive attribute of the MysqlSample event. For general information about metrics, see Integration metric data. Event data tips Here are some tips for finding and using integration event data in Insights: Most integration events are inventory change events. When inventory is changed, it generates an InfrastructureEvent with a format value of inventoryChange. Integration data can be found via the data explorer. For general information about events, see Event data. Inventory data tips Here are some tips for finding and using integration inventory data in Insights: For general information about inventory data, see Integration inventory data. Some inventory data is added as attributes (key-value pairs) to Insights data. For example, the AWS EC2 integration collects awsRegion as inventory data; this would be found in Insights by querying the awsRegion attribute of the ComputeSample event type and provider Ec2Instance. When inventory data changes, an Insights InfrastructureEvent event type is generated with a format value of inventoryChange. See the query examples for an example of querying this data. For more on using NRQL queries, see Intro to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.82642,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>integration</em> data in New Relic dashboards",
        "sections": "<em>Get</em> <em>started</em> with <em>integration</em> data",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " with the <em>Integrations</em> SDK. For a general look at how to find and use integration data, see New Relic data types. <em>Get</em> <em>started</em> with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com &gt; Infrastructure &gt; Third-party services page, select"
      },
      "id": "60450a39196a67d7dc960f7c"
    },
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.86646,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-06-20T12:42:18Z",
      "updated_at": "2021-06-20T12:42:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.15726,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate Azure <em>integrations</em>",
        "sections": "Activate Azure <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The Microsoft Azure <em>integrations</em> report data from various Azure platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    }
  ],
  "/docs/integrations/amazon-integrations/get-started/introduction-aws-integrations": [
    {
      "sections": [
        "Use integration data in New Relic dashboards",
        "Get started with integration data",
        "Example NRQL queries",
        "AWS EBS query example",
        "Azure Service Bus query example",
        "Azure Functions query example",
        "Azure VMs query example",
        "NGINX query example",
        "MySQL query example",
        "Inventory change query example",
        "Tip",
        "Tips for using different data types",
        "Metric data tips",
        "Event data tips",
        "Inventory data tips"
      ],
      "title": "Use integration data in New Relic dashboards",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "b236b0fae29853de085d0430fdec27fba74c15d4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards/",
      "published_at": "2021-06-20T19:34:58Z",
      "updated_at": "2021-06-20T19:34:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Most data generated by integrations is available in New Relic One dashboards, where you can query your data using NRQL and build custom dashboards. The following tips and sample queries were created for New Relic-built integrations, but most will also apply to integrations built with the Integrations SDK. For a general look at how to find and use integration data, see New Relic data types. Get started with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com > Infrastructure > Third-party services page, select an integration dashboard. There, you can view the NRQL queries that generated a chart. For examples of NRQL queries for integration data, see the example queries. Use the data explorer or the dashboards to explore and understand the available data. Read the documentation for a specific integration to learn about the reported data. When you create a useful query you'd like to add to your Insights dashboard, select Add to dashboard. Example NRQL queries Here are some examples of NRQL queries that use integration data: AWS EBS query example Here's a NRQL query for the AWS EBS service, showing the total write time metric, faceted by entityName: SELECT sum('provider.volumeTotalWriteTime.Sum') FROM BlockDeviceSample WHERE provider = 'EbsVolume' FACET entityName Copy Azure Service Bus query example Here's an Insights NRQL query for the maximum number of messages in an Azure Service Bus topic queue, faceted by resource group: SELECT max(activeMessages.Maximum) FROM AzureServiceBusTopicSample FACET resourceGroupName Copy Azure Functions query example Here's an Insights NRQL query for Azure Functions, showing the count of executed functions over the past six hours by region over time: SELECT sum(functionExecutionCount.Total) FROM AzureFunctionsAppSample FACET regionName TIMESERIES SINCE 6 hours ago Copy Azure VMs query example Here's an Insights NRQL query for Azure VMs that compares the count of VM events over the past thirty minutes with the same time a week ago: SELECT uniqueCount(vMName) FROM AzureVirtualMachineScaleSetSample FACET name SINCE 30 minutes ago COMPARE WITH 1 week ago Copy NGINX query example Here's an example of a query that you might run on your NGINX integration data and place in an Insights dashboard. This query creates a chart showing the average value of NGINX requests per second over time: SELECT average(net.requestsPerSecond) FROM NginxSample TIMESERIES Copy For more on how to create queries, see NRQL syntax. MySQL query example Here's an example of a query that you might run on your MySQL integration data. This query generates a chart showing the maximum number of used MySQL connections: SELECT max(net.maxUsedConnections) FROM MysqlSample Copy For more on how to create queries, see NRQL syntax. Inventory change query example Here's an example of a query that groups inventory change events from the last day by the type of change: SELECT count(*) FROM InfrastructureEvent WHERE format='inventoryChange' FACET changeType SINCE 1 DAY AGO Copy Tip You can also perform these queries using dimensional metrics. Tips for using different data types Integrations can generate metric, event, and inventory data, all of which are available for querying in New Relic Insights. Here are some tips for using the different types of integration data in Insights: Metric data tips Tips for finding and using integration metric data in Insights: All integration data is attached to a data type known as an event (not to be confused with events reported by integrations, which represent important activity in your host/service). This means that all integration data can be found via the data explorer. For more about these two basic New Relic data types, see New Relic data collection. Metric values are treated as attributes: key-value pairs attached to an event. For example, the MySQL integration has an 'active connections' metric; this would be found by querying the connectionsActive attribute of the MysqlSample event. For general information about metrics, see Integration metric data. Event data tips Here are some tips for finding and using integration event data in Insights: Most integration events are inventory change events. When inventory is changed, it generates an InfrastructureEvent with a format value of inventoryChange. Integration data can be found via the data explorer. For general information about events, see Event data. Inventory data tips Here are some tips for finding and using integration inventory data in Insights: For general information about inventory data, see Integration inventory data. Some inventory data is added as attributes (key-value pairs) to Insights data. For example, the AWS EC2 integration collects awsRegion as inventory data; this would be found in Insights by querying the awsRegion attribute of the ComputeSample event type and provider Ec2Instance. When inventory data changes, an Insights InfrastructureEvent event type is generated with a format value of inventoryChange. See the query examples for an example of querying this data. For more on using NRQL queries, see Intro to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.82642,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>integration</em> data in New Relic dashboards",
        "sections": "<em>Get</em> <em>started</em> with <em>integration</em> data",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " with the <em>Integrations</em> SDK. For a general look at how to find and use integration data, see New Relic data types. <em>Get</em> <em>started</em> with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com &gt; Infrastructure &gt; Third-party services page, select"
      },
      "id": "60450a39196a67d7dc960f7c"
    },
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.86646,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-06-20T12:42:18Z",
      "updated_at": "2021-06-20T12:42:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.15726,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate Azure <em>integrations</em>",
        "sections": "Activate Azure <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The Microsoft Azure <em>integrations</em> report data from various Azure platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    }
  ],
  "/docs/integrations/amazon-integrations/get-started/polling-intervals-aws-integrations": [
    {
      "sections": [
        "Use integration data in New Relic dashboards",
        "Get started with integration data",
        "Example NRQL queries",
        "AWS EBS query example",
        "Azure Service Bus query example",
        "Azure Functions query example",
        "Azure VMs query example",
        "NGINX query example",
        "MySQL query example",
        "Inventory change query example",
        "Tip",
        "Tips for using different data types",
        "Metric data tips",
        "Event data tips",
        "Inventory data tips"
      ],
      "title": "Use integration data in New Relic dashboards",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "b236b0fae29853de085d0430fdec27fba74c15d4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards/",
      "published_at": "2021-06-20T19:34:58Z",
      "updated_at": "2021-06-20T19:34:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Most data generated by integrations is available in New Relic One dashboards, where you can query your data using NRQL and build custom dashboards. The following tips and sample queries were created for New Relic-built integrations, but most will also apply to integrations built with the Integrations SDK. For a general look at how to find and use integration data, see New Relic data types. Get started with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com > Infrastructure > Third-party services page, select an integration dashboard. There, you can view the NRQL queries that generated a chart. For examples of NRQL queries for integration data, see the example queries. Use the data explorer or the dashboards to explore and understand the available data. Read the documentation for a specific integration to learn about the reported data. When you create a useful query you'd like to add to your Insights dashboard, select Add to dashboard. Example NRQL queries Here are some examples of NRQL queries that use integration data: AWS EBS query example Here's a NRQL query for the AWS EBS service, showing the total write time metric, faceted by entityName: SELECT sum('provider.volumeTotalWriteTime.Sum') FROM BlockDeviceSample WHERE provider = 'EbsVolume' FACET entityName Copy Azure Service Bus query example Here's an Insights NRQL query for the maximum number of messages in an Azure Service Bus topic queue, faceted by resource group: SELECT max(activeMessages.Maximum) FROM AzureServiceBusTopicSample FACET resourceGroupName Copy Azure Functions query example Here's an Insights NRQL query for Azure Functions, showing the count of executed functions over the past six hours by region over time: SELECT sum(functionExecutionCount.Total) FROM AzureFunctionsAppSample FACET regionName TIMESERIES SINCE 6 hours ago Copy Azure VMs query example Here's an Insights NRQL query for Azure VMs that compares the count of VM events over the past thirty minutes with the same time a week ago: SELECT uniqueCount(vMName) FROM AzureVirtualMachineScaleSetSample FACET name SINCE 30 minutes ago COMPARE WITH 1 week ago Copy NGINX query example Here's an example of a query that you might run on your NGINX integration data and place in an Insights dashboard. This query creates a chart showing the average value of NGINX requests per second over time: SELECT average(net.requestsPerSecond) FROM NginxSample TIMESERIES Copy For more on how to create queries, see NRQL syntax. MySQL query example Here's an example of a query that you might run on your MySQL integration data. This query generates a chart showing the maximum number of used MySQL connections: SELECT max(net.maxUsedConnections) FROM MysqlSample Copy For more on how to create queries, see NRQL syntax. Inventory change query example Here's an example of a query that groups inventory change events from the last day by the type of change: SELECT count(*) FROM InfrastructureEvent WHERE format='inventoryChange' FACET changeType SINCE 1 DAY AGO Copy Tip You can also perform these queries using dimensional metrics. Tips for using different data types Integrations can generate metric, event, and inventory data, all of which are available for querying in New Relic Insights. Here are some tips for using the different types of integration data in Insights: Metric data tips Tips for finding and using integration metric data in Insights: All integration data is attached to a data type known as an event (not to be confused with events reported by integrations, which represent important activity in your host/service). This means that all integration data can be found via the data explorer. For more about these two basic New Relic data types, see New Relic data collection. Metric values are treated as attributes: key-value pairs attached to an event. For example, the MySQL integration has an 'active connections' metric; this would be found by querying the connectionsActive attribute of the MysqlSample event. For general information about metrics, see Integration metric data. Event data tips Here are some tips for finding and using integration event data in Insights: Most integration events are inventory change events. When inventory is changed, it generates an InfrastructureEvent with a format value of inventoryChange. Integration data can be found via the data explorer. For general information about events, see Event data. Inventory data tips Here are some tips for finding and using integration inventory data in Insights: For general information about inventory data, see Integration inventory data. Some inventory data is added as attributes (key-value pairs) to Insights data. For example, the AWS EC2 integration collects awsRegion as inventory data; this would be found in Insights by querying the awsRegion attribute of the ComputeSample event type and provider Ec2Instance. When inventory data changes, an Insights InfrastructureEvent event type is generated with a format value of inventoryChange. See the query examples for an example of querying this data. For more on using NRQL queries, see Intro to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.8263,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>integration</em> data in New Relic dashboards",
        "sections": "<em>Get</em> <em>started</em> with <em>integration</em> data",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " with the <em>Integrations</em> SDK. For a general look at how to find and use integration data, see New Relic data types. <em>Get</em> <em>started</em> with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com &gt; Infrastructure &gt; Third-party services page, select"
      },
      "id": "60450a39196a67d7dc960f7c"
    },
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.86633,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-06-20T12:42:18Z",
      "updated_at": "2021-06-20T12:42:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.15714,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate Azure <em>integrations</em>",
        "sections": "Activate Azure <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The Microsoft Azure <em>integrations</em> report data from various Azure platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/authentication-issues": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.43137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 122.231964,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.32112,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": ". In order to continue collecting that additional data, keep those enabled as polling <em>integrations</em>. <em>Troubleshooting</em> No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/aws-service-specific-api-rate-limiting": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.43126,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 122.23185,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.321075,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": ". In order to continue collecting that additional data, keep those enabled as polling <em>integrations</em>. <em>Troubleshooting</em> No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/cannot-create-alert-condition-infrastructure-integration": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.43126,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 122.23185,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.321075,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": ". In order to continue collecting that additional data, keep those enabled as polling <em>integrations</em>. <em>Troubleshooting</em> No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/cloudwatch-billing-increase": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.431145,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 122.23174,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.32103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": ". In order to continue collecting that additional data, keep those enabled as polling <em>integrations</em>. <em>Troubleshooting</em> No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/invalid-principal-error-unsupported-aws-regions": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.431145,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 122.23174,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.32103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": ". In order to continue collecting that additional data, keep those enabled as polling <em>integrations</em>. <em>Troubleshooting</em> No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/metric-data-delays-amazon-aws-integrations": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.43103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 122.231636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.320984,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": ". In order to continue collecting that additional data, keep those enabled as polling <em>integrations</em>. <em>Troubleshooting</em> No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/no-data-appears-aws-integrations": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.43103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 122.231636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.320984,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": ". In order to continue collecting that additional data, keep those enabled as polling <em>integrations</em>. <em>Troubleshooting</em> No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/partial-or-missing-logs-rds-vpc-aws-lambda": [
    {
      "sections": [
        "AWS AppSync monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "AppSync Api data"
      ],
      "title": "AWS AppSync monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "0ea2f07ae74c1b4184b647164010bd152ad53cb7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-appsync-monitoring-integration/",
      "published_at": "2021-06-20T20:54:39Z",
      "updated_at": "2021-06-20T20:54:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your AWS AppSync data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select an integration. You can query and explore your data using this event type: Entity Event Type Provider Api AwsAppSyncApiSample AwsAppSyncApi For more on how to use your data, see New Relic data types. Metric data This integration collects AWS AppSync data for Api. AppSync Api data Metric Unit Description 4XXError Count The number of errors captured as a result of invalid requests due to incorrect client configuration. Typically, these errors happen anywhere outside of the GraphQL execution. For example, this could be an incorrect JSON payload or an incorrect query in the request, when the service is throttled, or even a potential misconfiguration on the Auth settings. 5XXError Count Errors encountered during the execution of a GraphQL query. For example, this could occur when a query request is initiated for an empty or incorrect schema, if the Amazon Cognito user pool ID or AWS Region is invalid. Alternatively, this could also happen if AWS AppSync encounters an issue during an execution of a request. Latency Milliseconds The time between when AWS AppSync receives a request from a client and when it returns a response to the client. This doesn't include the network latency encountered for a response to reach the end devices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.430916,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS AppSync monitoring <em>integration</em>",
        "sections": "AWS AppSync monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS AppSync integration: New Relic polling interval: 5 minutes <em>Amazon</em> CloudWatch data interval: 1 minute Find and use data To find your integration data in Infrastructure"
      },
      "id": "603eaf99196a67d294a83dbf"
    },
    {
      "sections": [
        "Amazon Elasticsearch monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Configuration",
        "eBSOptions",
        "snapshotOptions",
        "elasticsearchClusterConfig"
      ],
      "title": "Amazon Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "66a8a74032ef636e2f8646447174aadf280d7b60",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elasticsearch-monitoring-integration/",
      "published_at": "2021-06-20T15:26:04Z",
      "updated_at": "2021-06-20T15:26:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting Amazon Elasticsearch data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features Amazon Elasticsearch Service is a fully managed service that delivers Elasticsearch’s easy-to-use APIs and real-time capabilities along with the availability, scalability, and security required by production workloads. New Relic's Elasticsearch monitoring integration allows you to track cluster status, CPU utilization, read/write latency, throughput, and other metrics, at specific points in time. Elasticsearch data is also available to query, analyze, and chart your data. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Amazon Elasticsearch integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the Elasticsearch integration links. To query and explore your data, use the DatastoreSample event type with the appropriate provider value: ElasticsearchCluster for clusters ElasticsearchNode for nodes Metric data The Elasticsearch integration collects these metrics for clusters: Name Relevant statistics Description ClusterStatus.green Minimum, Maximum Indicates that all index shards are allocated to nodes in the cluster. ClusterStatus.yellow Minimum, Maximum Indicates that the primary shards for all indices are allocated to nodes in a cluster, but the replica shards for at least one index are not. Single node clusters always initialize with this cluster status because there is no second node to which a replica can be assigned. You can either increase your node count to obtain a green cluster status, or you can use the Amazon ES API to set the number_of_replicas setting for your index to 0. For more information, see Amazon's documentation for Updating indices settings. ClusterStatus.red Minimum, Maximum Indicates that the primary and replica shards of at least one index are not allocated to nodes in a cluster. For more information, see Amazon's documentation on Red Cluster Status. Nodes Minimum, Maximum, Average The number of nodes in the Amazon ES cluster. SearchableDocuments Minimum, Maximum, Average The total number of searchable documents across all indices in the cluster. DeletedDocuments Minimum, Maximum, Average The total number of deleted documents across all indices in the cluster. CPUUtilization Minimum, Maximum, Average The maximum percentage of CPU resources used for data nodes in the cluster. FreeStorageSpace Minimum The free space, in megabytes, for all data nodes in the cluster. ClusterUsedSpace Minimum, Maximum The total used space, in megabytes, for a cluster. ClusterIndexWritesBlocked Maximum Indicates whether your cluster is accepting or blocking incoming write requests. A value of 0 means that the cluster is accepting requests. A value of 1 means that it is blocking requests. JVMMemoryPressure Maximum The maximum percentage of the Java heap used for all data nodes in the cluster. AutomatedSnapshotFailure Minimum, Maximum The number of failed automated snapshots for the cluster. A value of 1 indicates that no automated snapshot was taken for the domain in the previous 36 hours. CPUCreditBalance Minimum The remaining CPU credits available for data nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metrics is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. KibanaHealthyNodes Minimum A health check for Kibana. A value of 1 indicates normal behavior. A value of 0 indicates that Kibana is inaccessible. In most cases, the health of Kibana mirrors the health of the cluster. KMSKeyError Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been disabled. To restore the domain to normal operations, re-enable the key. KMSKeyInaccessible Minimum, Maximum A value of 1 indicates that the KMS customer master key used to encrypt data at rest has been deleted or revoked its grants to Amazon ES. You can't recover domains that are in this state. If you have a manual snapshot, though, you can use it to migrate the domain's data to a new domain. InvalidHostHeaderRequests Sum The number of HTTP requests made to the Elasticsearch cluster that included an invalid (or missing) host header. ElasticsearchRequests Sum The number of requests made to the Elasticsearch cluster. RequestCount Sum The number of requests to a domain and the HTTP response code (2xx, 3xx, 4xx, 5xx) for each request. MasterCPUUtilization Average The maximum percentage of CPU resources used by the dedicated master nodes. We recommend increasing the size of the instance type when this metric reaches 60 percent. MasterJVMMemoryPressure Maximum The maximum percentage of the Java heap used for all dedicated master nodes in the cluster. We recommend moving to a larger instance type when this metric reaches 85 percent. MasterCPUCreditBalance Minimum The remaining CPU credits available for dedicated master nodes in the cluster. A CPU credit provides the performance of a full CPU core for one minute. This metric is available only for the t2.micro.elasticsearch, t2.small.elasticsearch, and t2.medium.elasticsearch instance types. MasterReachableFromNode Minimum A health check for MasterNotDiscovered exceptions. A value of 1 indicates normal behavior. A value of 0 indicates that /_cluster/health/ is failing. Failures mean that the master node stopped or is not reachable. They are usually the result of a network connectivity issue or AWS dependency problem. ReadLatency Minimum, Maximum, Average The latency, in seconds, for read operations on EBS volumes. WriteLatency Minimum, Maximum, Average The latency, in seconds, for write operations on EBS volumes. ReadThroughput Minimum, Maximum, Average The throughput, in bytes per second, for read operations on EBS volumes. WriteThroughput Minimum, Maximum, Average The throughput, in bytes per second, for write operations on EBS volumes. DiskQueueDepth Minimum, Maximum, Average The number of pending input and output (I/O) requests for an EBS volume. ReadIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for read operations on EBS volumes. WriteIOPS Minimum, Maximum, Average The number of input and output (I/O) operations per second for write operations on EBS volumes. The following metrics are collected for Elasticsearch clusters, and optionally for each instance or node in a domain as well: Name Relevant statistics Description IndexingLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete an indexing operation. IndexingRate For nodes: Average For clusters: Average, Maximum, Sum The number of indexing operations per minute. SearchLatency For nodes: Average For clusters: Average, Maximum The average time, in milliseconds, that it takes a shard to complete a search operation. SearchRate For nodes: Average For clusters: Average, Maximum, Sum The total number of search requests per minute for all shards on a node. SysMemoryUtilization Minimum, Maximum, Average The percentage of the instance's memory that is in use. JVMGCYoungCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"young generation\" garbage collection has run. A large, ever-growing number of runs is a normal part of cluster operations. JVMGCYoungCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"young generation\" garbage collection. JVMGCOldCollectionCount For nodes: Maximum For clusters: Sum, Maximum, Average The number of times that \"old generation\" garbage collection has run. In a cluster with sufficient resources, this number should remain small and grow infrequently. JVMGCOldCollectionTime For nodes: Maximum For clusters: Sum, Maximum, Average The amount of time, in milliseconds, that the cluster has spent performing \"old generation\" garbage collection. ThreadpoolForce_mergeQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the force merge thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolForce_mergeRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the force merge thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolForce_mergeThreads For nodes: Maximum For clusters: Sum, Average The size of the force merge thread pool. ThreadpoolIndexQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the index thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum index queue size is 200. ThreadpoolIndexRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the index thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolIndexThreads For nodes: Maximum For clusters: Sum, Average The size of the index thread pool. ThreadpoolSearchQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the search thread pool. If the queue size is consistently high, consider scaling your cluster. The maximum search queue size is 1000. ThreadpoolSearchRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the search thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolSearchThreads For nodes: Maximum For clusters: Sum, Average The size of the search thread pool. ThreadpoolBulkQueue For nodes: Maximum For clusters: Sum, Maximum, Average The number of queued tasks in the bulk thread pool. If the queue size is consistently high, consider scaling your cluster. ThreadpoolBulkRejected For nodes: Maximum For clusters: Sum The number of rejected tasks in the bulk thread pool. If this number continually grows, consider scaling your cluster. ThreadpoolBulkThreads For nodes: Maximum For clusters: Sum, Average The size of the bulk thread pool. Inventory data The integration collects this ElasticSearch data as inventory data. Configuration The integration collects this data from aws/elasticsearch/cluster/config: Inventory Description aRN The Amazon resource name (ARN) of an Elasticsearch domain. accessPolicies The IAM access policy. created The domain creation status. True if the creation of an Elasticsearch domain is complete. False if domain creation is still in progress. deleted The domain deletion status. True if a delete request has been received for the domain but resource cleanup is still in progress. False if the domain has not been deleted. domainId The unique identifier for the specified Elasticsearch domain. domainName Name of the Elasticsearch domain. elasticsearchVersion Elasticsearch version. endpoint The Elasticsearch domain endpoint that you use to submit index and search requests. processing The status of the Elasticsearch domain configuration. True if Amazon Elasticsearch Service is processing configuration changes. False if the configuration is active. upgradeProcessing The status of an Elasticsearch domain version upgrade. True if Amazon Elasticsearch Service is undergoing a version upgrade. False if the configuration is active. eBSOptions The integration collects this data from aws/elasticsearch/cluster/config/eBSOptions: Name Description eBSEnabled Specifies whether EBS-based storage is enabled. iops Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). volumeSize Integer to specify the size of an EBS volume. volumeType Specifies the volume type for EBS-based storage. snapshotOptions The integration collects this data from aws/elasticsearch/cluster/config/snapshotOptions: Name Description automatedSnapshotStartHour Specifies the time, in UTC format, when the service takes a daily automated snapshot of the specified Elasticsearch domain. elasticsearchClusterConfig The integration collects this data from aws/elasticsearch/cluster/config/elasticsearchClusterConfig: Name Description dedicatedMasterCount Total number of dedicated master nodes, active and on standby, for the cluster. dedicatedMasterEnabled A boolean value to indicate whether a dedicated master node is enabled. dedicatedMasterType The instance type for a dedicated master node. instanceCount The number of instances in the specified domain cluster. instanceType The instance type for an Elasticsearch cluster. zoneAwarenessEnabled A boolean value to indicate whether zone awareness is enabled.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 122.23153,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "sections": "<em>Amazon</em> Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> include an integration for reporting <em>Amazon</em> Elasticsearch data to New Relic. This document explains the integration&#x27;s features, how to activate it, and what data can be reported. Features <em>Amazon</em> Elasticsearch Service is a fully managed service that delivers"
      },
      "id": "603ea15d196a67aa5ba83dc9"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-06-20T22:52:04Z",
      "updated_at": "2021-06-15T11:35:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Entities in New Relic Explorer might be duplicated for up to 24 hours. The AWS CloudWatch Metric Streams integration only collects metrics. Integrations such as AWS Health, AWS Billing, and AWS CloudTrail collect additional instrumentation that's not available for metric streams. In order to continue collecting that additional data, keep those enabled as polling integrations. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with “eu” then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and it’s not the case, please verify the following: Make sure there’s no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.32094,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": ". In order to continue collecting that additional data, keep those enabled as polling <em>integrations</em>. <em>Troubleshooting</em> No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow"
      },
      "id": "606a036de7b9d2bfef9445f2"
    }
  ],
  "/docs/integrations/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration": [
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-06-20T14:55:11Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.5088,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No data appears",
        "sections": "ECS <em>integration</em> troubleshooting: No data appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, use the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    },
    {
      "sections": [
        "Install the ECS integration",
        "Tip",
        "Install overview",
        "Install using CloudFormation",
        "EC2 launch type",
        "Fargate launch type",
        "Install with automatic script",
        "Manual install",
        "AWS resources created"
      ],
      "title": "Install the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "857b78b6e7de76449f3f9569cee4700705b7d7fe",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/install-ecs-integration/",
      "published_at": "2021-06-20T23:01:35Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's ECS integration reports and displays performance data from your Amazon ECS environment. This document explains how to install this integration. Tip To use ECS integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Install overview Before you install our ECS integration, we recommend reviewing the requirements. Here's a brief overview of what happens during the install process: For EC2 launch type: The infrastructure agent (newrelic-infra) gets deployed onto an ECS cluster as a service using the daemon scheduling strategy. This deployment installs the infrastructure agent in all the container instances of the cluster. The infrastructure agent then monitors ECS and Docker containers. For Fargate launch type: The infrastructure agent (newrelic-infra) gets deployed as a sidecar in every task to monitor. Install options: Install using AWS CloudFormation Install using automatic script Install manually Install using CloudFormation One install option is using AWS CloudFormation. We provide some CloudFormation templates that install the ECS integration onto your AWS account for both EC2 and Fargate launch types: To register the New Relic's ECS integration task, deploy this stack. Ensure you’re deploying the stack to your desired region(s). This stack creates the following resources: A secret that stores the license key. A policy to access the license key. An instance role to be used as an ECS task ExecutionRole, with access to the license key. For EC2 launch type: Registers the New Relic Infrastructure ECS integration task. Follow the additional instructions for your launch type: EC2 launch type Additional steps for EC2 launch type: To create a service that runs the task on every container instance, deploy this stack. Fargate launch type Additional steps for Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Install with automatic script One install option is using our install script. To use the automatic install script: Download the ECS integration installer: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-installer.sh Copy Add execute permissions to the installer: chmod +x newrelic-infra-ecs-installer.sh Copy Execute it with -h to see the documentation and requirements: ./newrelic-infra-ecs-installer.sh -h Copy Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Execute the installer, specifying your license key and cluster name. EC2 launch type: ./newrelic-infra-ecs-installer.sh -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Fargate launch type: ./newrelic-infra-ecs-installer.sh -fargate -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Additional steps for Fargate launch type (not EC2 launch type): Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the single container in this task definition as a sidecar to the task definitions you want monitored. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Manual install One install option is to manually do the steps that are done by the automatic installer script. We will describe how this is done using the awscli tool: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Save your New Relic license key as a Systems Manager (SSM) parameter: aws ssm put-parameter \\ --name \"/newrelic-infra/ecs/license-key\" \\ --type SecureString \\ --description 'New Relic license key for ECS monitoring' \\ --value \"NEW_RELIC_LICENSE_KEY\" Copy Create an IAM policy to access the license key parameter: aws iam create-policy \\ --policy-name \"NewRelicSSMLicenseKeyReadAccess\" \\ --policy-document \"{\"Version\"\\\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"ssm:GetParameters\"],\"Resource\":[\"ARN_OF_LICENSE_KEY_PARAMETER\"]}]}\" --description \"Provides read access to the New Relic SSM license key parameter\" Copy Create an IAM role to be used as the task execution role: aws iam create-role \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --assume-role-policy-document '{\"Version\":\"2008-10-17\",\"Statement\":[{\"Sid\":\"\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"ecs-tasks.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}' \\ --description \"ECS task execution role for New Relic infrastructure\" Copy Attach the policies NewRelicSSMLicenseKeyReadAccess, AmazonEC2ContainerServiceforEC2Role, and AmazonECSTaskExecutionRolePolicy to the role: aws iam attach-role-policy \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --policy-arn \"POLICY_ARN\" Copy Choose your launch type for more instructions: EC2 launch type Additional steps for EC2 launch type: Download the New Relic ECS integration task definition template file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-ec2-latest.json Copy Replace the task execution role in the template file with the newly created role: \"executionRoleArn\": \"NewRelicECSTaskExecutionRole\", Copy Replace the valueFrom attribute of the secret with the name of the Systems Manager parameter: secrets\": [ { \"valueFrom\": \"/newrelic-infra/ecs/license-key\", \"name\": \"NRIA_LICENSE_KEY\" } ], Copy Register the task definition file: aws ecs register-task-definition --cli-input-json file://newrelic-infra-ecs-ec2-latest.json Copy Create a service with the daemon scheduling strategy for the registered task: aws ecs create-service --cluster \"YOUR_CLUSTER_NAME\" --service-name \"newrelic-infra\" --task-definition \"newrelic-infra\" --scheduling-strategy DAEMON Copy Fargate launch type Additional steps for the Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, a separate integration which gets you supplementary ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. AWS resources created When you install the ECS integration using default/recommended values, it does the following in AWS: Creates Systems Manager (SSM) parameter /newrelic-infra/ecs/license-key. This system parameter contains the New Relic license key. Creates IAM policy NewRelicSSMLicenseKeyReadAccess, which enables access to the SSM parameter with the license key. Creates IAM role NewRelicECSTaskExecutionRole used as the task execution role. Policies attached to the role: NewRelicSSMLicenseKeyReadAccess (created by the installer). AmazonEC2ContainerServiceforEC2Role AmazonECSTaskExecutionRolePolicy For EC2 launch type, this is also done: Registers the newrelic-infra ECS task definition. Creates the service newrelic-infra for the registered task using a daemon scheduling strategy.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 148.28981,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the ECS <em>integration</em>",
        "sections": "Install the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. This document explains how to install this <em>integration</em>. Tip To use ECS <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign"
      },
      "id": "603e9e76196a676684a83de9"
    },
    {
      "sections": [
        "Uninstall the ECS integration",
        "Uninstall",
        "CloudFormation uninstall",
        "Automatic uninstall",
        "Manual uninstall"
      ],
      "title": "Uninstall the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "78bfa3ecb2059e2641be8e22cd8ebb025da625a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/uninstall-ecs-integration/",
      "published_at": "2021-06-20T23:01:35Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this integration. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script Manual uninstall CloudFormation uninstall To uninstall the ECS integration using the CloudFormation templates: Go to the list of stacks in your AWS console. For each New Relic stack: Select the stack Click the delete button Click the delete stack button on the confirmation pop-up. Automatic uninstall To uninstall the ECS integration using the installer script: For EC2 launch type: run $ ./newrelic-infrastructure-ecs-installer.sh -u -c YOUR_CLUSTER_NAME Copy For Fargate launch type: $ ./newrelic-infrastructure-ecs-installer.sh -f -u -c YOUR_CLUSTER_NAME Copy You only need to execute the command once, regardless of the number of nodes in your cluster. The command will delete the AWS resources created during the install procedure. The installer provides a dry run mode that shows you the awscli commands that are going to be executed. The dry run mode for the uninstall process is activated by passing the -d flag to the command: $ ./newrelic-infrastructure-ecs-installer.sh -d -u -c YOUR_CLUSTER_NAME Copy Manual uninstall To uninstall manually, you must delete all the AWS resources related to the integration. To do this: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Delete the Systems Manager (SSM) parameter that stores the New Relic license key: aws ssm delete-parameter --name \"/newrelic-infra/ecs/license-key\" Copy Before deleting the IAM role, you need to detach all of its policies. To get a list of the attached policies: aws iam list-attached-role-policies --role-name \"NewRelicECSTaskExecutionRole\" --output text --query 'AttachedPolicies[*].PolicyArn' Copy Detach all the policies returned in the previous step from the IAM role: aws iam detach-role-policy --role-name \"NewRelicECSTaskExecutionRole\" --policy-arn \"POLICY_ARN\" Copy Delete the IAM role: aws iam delete-role --role-name \"NewRelicECSTaskExecutionRole\" Copy Delete the IAM policy NewRelicSSMLicenseKeyReadAccess, which grants System Manager license key access: aws iam delete-policy --policy-arn \"POLICY_ARN\" Copy The remaining steps are only for EC2 launch type, and not Fargate: Delete the service: aws ecs delete-service --service \"newrelic-infra\" --cluster \"YOUR_CLUSTER_NAME\" Copy List the task definition for the newrelic-infra family of tasks: aws ecs list-task-definitions \\ --family-prefix newrelic-infra \\ --output text \\ --query taskDefinitionArns Copy Deregister the tasks: aws ecs deregister-task-definition --task-definition \"TASK_DEFINITION_ARN\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.27095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Uninstall the ECS <em>integration</em>",
        "sections": "Uninstall the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this <em>integration</em>. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script"
      },
      "id": "603e9e7464441fd9cf4e885b"
    }
  ],
  "/docs/integrations/elastic-container-service-integration/installation/install-ecs-integration": [
    {
      "sections": [
        "Uninstall the ECS integration",
        "Uninstall",
        "CloudFormation uninstall",
        "Automatic uninstall",
        "Manual uninstall"
      ],
      "title": "Uninstall the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "78bfa3ecb2059e2641be8e22cd8ebb025da625a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/uninstall-ecs-integration/",
      "published_at": "2021-06-20T23:01:35Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this integration. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script Manual uninstall CloudFormation uninstall To uninstall the ECS integration using the CloudFormation templates: Go to the list of stacks in your AWS console. For each New Relic stack: Select the stack Click the delete button Click the delete stack button on the confirmation pop-up. Automatic uninstall To uninstall the ECS integration using the installer script: For EC2 launch type: run $ ./newrelic-infrastructure-ecs-installer.sh -u -c YOUR_CLUSTER_NAME Copy For Fargate launch type: $ ./newrelic-infrastructure-ecs-installer.sh -f -u -c YOUR_CLUSTER_NAME Copy You only need to execute the command once, regardless of the number of nodes in your cluster. The command will delete the AWS resources created during the install procedure. The installer provides a dry run mode that shows you the awscli commands that are going to be executed. The dry run mode for the uninstall process is activated by passing the -d flag to the command: $ ./newrelic-infrastructure-ecs-installer.sh -d -u -c YOUR_CLUSTER_NAME Copy Manual uninstall To uninstall manually, you must delete all the AWS resources related to the integration. To do this: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Delete the Systems Manager (SSM) parameter that stores the New Relic license key: aws ssm delete-parameter --name \"/newrelic-infra/ecs/license-key\" Copy Before deleting the IAM role, you need to detach all of its policies. To get a list of the attached policies: aws iam list-attached-role-policies --role-name \"NewRelicECSTaskExecutionRole\" --output text --query 'AttachedPolicies[*].PolicyArn' Copy Detach all the policies returned in the previous step from the IAM role: aws iam detach-role-policy --role-name \"NewRelicECSTaskExecutionRole\" --policy-arn \"POLICY_ARN\" Copy Delete the IAM role: aws iam delete-role --role-name \"NewRelicECSTaskExecutionRole\" Copy Delete the IAM policy NewRelicSSMLicenseKeyReadAccess, which grants System Manager license key access: aws iam delete-policy --policy-arn \"POLICY_ARN\" Copy The remaining steps are only for EC2 launch type, and not Fargate: Delete the service: aws ecs delete-service --service \"newrelic-infra\" --cluster \"YOUR_CLUSTER_NAME\" Copy List the task definition for the newrelic-infra family of tasks: aws ecs list-task-definitions \\ --family-prefix newrelic-infra \\ --output text \\ --query taskDefinitionArns Copy Deregister the tasks: aws ecs deregister-task-definition --task-definition \"TASK_DEFINITION_ARN\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.6318,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Uninstall the ECS <em>integration</em>",
        "sections": "Uninstall the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this <em>integration</em>. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script"
      },
      "id": "603e9e7464441fd9cf4e885b"
    },
    {
      "sections": [
        "Introduction to the Amazon ECS integration",
        "Features",
        "Important",
        "Compatibility and requirements",
        "Install",
        "Check the source code"
      ],
      "title": "Introduction to the Amazon ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Get started"
      ],
      "external_id": "a2af5484b25f8595032cc1937210c9a41024a138",
      "image": "https://docs.newrelic.com/static/986bdb22950fdd8b222a850e205882a9/c1b63/new-relic-ecs-integration-dashboards_0.png",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration/",
      "published_at": "2021-06-20T23:01:36Z",
      "updated_at": "2021-03-30T21:13:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our ECS integration reports and displays performance data from your Amazon ECS environment. The ECS integration works well with other integrations, so you can also monitor services running on ECS. Features Amazon Elastic Container Service (ECS) is a scalable container management service that makes it easy to run, stop, and manage Docker containers on Amazon EC2 clusters. Our ECS integration instruments the underlying container instance (EC2 launch type) and the container layer by reporting metrics from ECS objects. The integration gives you insight into your ECS instances, tasks, services, and containers. one.newrelic.com > Explorer > ECS dashboard: The ECS integration reports performance data about your Amazon ECS containers. Features include: View your data in pre-built dashboards for immediate insight into your ECS environment. Create your own queries and charts in the query builder from automatically reported data. Create alert conditions on ECS data. Explore entities using the New Relic Explorer. Important New Relic also offers an ECS cloud integration, which reports a different data set than our on-host integration. For complete ECS monitoring, we recommend enabling both integrations. Compatibility and requirements Requirements: Amazon ECS container agent 1.21.0 or higher. Windows not supported. This integration uses our infrastructure agent and our Docker instrumentation: applicable requirements and restrictions of those systems apply. Install To install, see Install integration. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.70364,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Amazon ECS <em>integration</em>",
        "sections": "Introduction to the Amazon ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "Our ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. The ECS <em>integration</em> works well with other <em>integrations</em>, so you can also monitor services running on ECS. Features Amazon <em>Elastic</em> <em>Container</em> <em>Service</em> (ECS) is a scalable <em>container</em> management <em>service</em> that makes"
      },
      "id": "603eb04b196a6752b5a83dc8"
    },
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-06-20T14:55:11Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.5088,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No data appears",
        "sections": "ECS <em>integration</em> troubleshooting: No data appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, use the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    }
  ],
  "/docs/integrations/elastic-container-service-integration/installation/uninstall-ecs-integration": [
    {
      "sections": [
        "Install the ECS integration",
        "Tip",
        "Install overview",
        "Install using CloudFormation",
        "EC2 launch type",
        "Fargate launch type",
        "Install with automatic script",
        "Manual install",
        "AWS resources created"
      ],
      "title": "Install the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "857b78b6e7de76449f3f9569cee4700705b7d7fe",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/install-ecs-integration/",
      "published_at": "2021-06-20T23:01:35Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's ECS integration reports and displays performance data from your Amazon ECS environment. This document explains how to install this integration. Tip To use ECS integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Install overview Before you install our ECS integration, we recommend reviewing the requirements. Here's a brief overview of what happens during the install process: For EC2 launch type: The infrastructure agent (newrelic-infra) gets deployed onto an ECS cluster as a service using the daemon scheduling strategy. This deployment installs the infrastructure agent in all the container instances of the cluster. The infrastructure agent then monitors ECS and Docker containers. For Fargate launch type: The infrastructure agent (newrelic-infra) gets deployed as a sidecar in every task to monitor. Install options: Install using AWS CloudFormation Install using automatic script Install manually Install using CloudFormation One install option is using AWS CloudFormation. We provide some CloudFormation templates that install the ECS integration onto your AWS account for both EC2 and Fargate launch types: To register the New Relic's ECS integration task, deploy this stack. Ensure you’re deploying the stack to your desired region(s). This stack creates the following resources: A secret that stores the license key. A policy to access the license key. An instance role to be used as an ECS task ExecutionRole, with access to the license key. For EC2 launch type: Registers the New Relic Infrastructure ECS integration task. Follow the additional instructions for your launch type: EC2 launch type Additional steps for EC2 launch type: To create a service that runs the task on every container instance, deploy this stack. Fargate launch type Additional steps for Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Install with automatic script One install option is using our install script. To use the automatic install script: Download the ECS integration installer: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-installer.sh Copy Add execute permissions to the installer: chmod +x newrelic-infra-ecs-installer.sh Copy Execute it with -h to see the documentation and requirements: ./newrelic-infra-ecs-installer.sh -h Copy Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Execute the installer, specifying your license key and cluster name. EC2 launch type: ./newrelic-infra-ecs-installer.sh -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Fargate launch type: ./newrelic-infra-ecs-installer.sh -fargate -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Additional steps for Fargate launch type (not EC2 launch type): Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the single container in this task definition as a sidecar to the task definitions you want monitored. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Manual install One install option is to manually do the steps that are done by the automatic installer script. We will describe how this is done using the awscli tool: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Save your New Relic license key as a Systems Manager (SSM) parameter: aws ssm put-parameter \\ --name \"/newrelic-infra/ecs/license-key\" \\ --type SecureString \\ --description 'New Relic license key for ECS monitoring' \\ --value \"NEW_RELIC_LICENSE_KEY\" Copy Create an IAM policy to access the license key parameter: aws iam create-policy \\ --policy-name \"NewRelicSSMLicenseKeyReadAccess\" \\ --policy-document \"{\"Version\"\\\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"ssm:GetParameters\"],\"Resource\":[\"ARN_OF_LICENSE_KEY_PARAMETER\"]}]}\" --description \"Provides read access to the New Relic SSM license key parameter\" Copy Create an IAM role to be used as the task execution role: aws iam create-role \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --assume-role-policy-document '{\"Version\":\"2008-10-17\",\"Statement\":[{\"Sid\":\"\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"ecs-tasks.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}' \\ --description \"ECS task execution role for New Relic infrastructure\" Copy Attach the policies NewRelicSSMLicenseKeyReadAccess, AmazonEC2ContainerServiceforEC2Role, and AmazonECSTaskExecutionRolePolicy to the role: aws iam attach-role-policy \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --policy-arn \"POLICY_ARN\" Copy Choose your launch type for more instructions: EC2 launch type Additional steps for EC2 launch type: Download the New Relic ECS integration task definition template file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-ec2-latest.json Copy Replace the task execution role in the template file with the newly created role: \"executionRoleArn\": \"NewRelicECSTaskExecutionRole\", Copy Replace the valueFrom attribute of the secret with the name of the Systems Manager parameter: secrets\": [ { \"valueFrom\": \"/newrelic-infra/ecs/license-key\", \"name\": \"NRIA_LICENSE_KEY\" } ], Copy Register the task definition file: aws ecs register-task-definition --cli-input-json file://newrelic-infra-ecs-ec2-latest.json Copy Create a service with the daemon scheduling strategy for the registered task: aws ecs create-service --cluster \"YOUR_CLUSTER_NAME\" --service-name \"newrelic-infra\" --task-definition \"newrelic-infra\" --scheduling-strategy DAEMON Copy Fargate launch type Additional steps for the Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, a separate integration which gets you supplementary ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. AWS resources created When you install the ECS integration using default/recommended values, it does the following in AWS: Creates Systems Manager (SSM) parameter /newrelic-infra/ecs/license-key. This system parameter contains the New Relic license key. Creates IAM policy NewRelicSSMLicenseKeyReadAccess, which enables access to the SSM parameter with the license key. Creates IAM role NewRelicECSTaskExecutionRole used as the task execution role. Policies attached to the role: NewRelicSSMLicenseKeyReadAccess (created by the installer). AmazonEC2ContainerServiceforEC2Role AmazonECSTaskExecutionRolePolicy For EC2 launch type, this is also done: Registers the newrelic-infra ECS task definition. Creates the service newrelic-infra for the registered task using a daemon scheduling strategy.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.65068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the ECS <em>integration</em>",
        "sections": "<em>Install</em> the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. This document explains how to install this <em>integration</em>. Tip To use ECS <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign"
      },
      "id": "603e9e76196a676684a83de9"
    },
    {
      "sections": [
        "Introduction to the Amazon ECS integration",
        "Features",
        "Important",
        "Compatibility and requirements",
        "Install",
        "Check the source code"
      ],
      "title": "Introduction to the Amazon ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Get started"
      ],
      "external_id": "a2af5484b25f8595032cc1937210c9a41024a138",
      "image": "https://docs.newrelic.com/static/986bdb22950fdd8b222a850e205882a9/c1b63/new-relic-ecs-integration-dashboards_0.png",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration/",
      "published_at": "2021-06-20T23:01:36Z",
      "updated_at": "2021-03-30T21:13:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our ECS integration reports and displays performance data from your Amazon ECS environment. The ECS integration works well with other integrations, so you can also monitor services running on ECS. Features Amazon Elastic Container Service (ECS) is a scalable container management service that makes it easy to run, stop, and manage Docker containers on Amazon EC2 clusters. Our ECS integration instruments the underlying container instance (EC2 launch type) and the container layer by reporting metrics from ECS objects. The integration gives you insight into your ECS instances, tasks, services, and containers. one.newrelic.com > Explorer > ECS dashboard: The ECS integration reports performance data about your Amazon ECS containers. Features include: View your data in pre-built dashboards for immediate insight into your ECS environment. Create your own queries and charts in the query builder from automatically reported data. Create alert conditions on ECS data. Explore entities using the New Relic Explorer. Important New Relic also offers an ECS cloud integration, which reports a different data set than our on-host integration. For complete ECS monitoring, we recommend enabling both integrations. Compatibility and requirements Requirements: Amazon ECS container agent 1.21.0 or higher. Windows not supported. This integration uses our infrastructure agent and our Docker instrumentation: applicable requirements and restrictions of those systems apply. Install To install, see Install integration. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.70364,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Amazon ECS <em>integration</em>",
        "sections": "Introduction to the Amazon ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "Our ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. The ECS <em>integration</em> works well with other <em>integrations</em>, so you can also monitor services running on ECS. Features Amazon <em>Elastic</em> <em>Container</em> <em>Service</em> (ECS) is a scalable <em>container</em> management <em>service</em> that makes"
      },
      "id": "603eb04b196a6752b5a83dc8"
    },
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-06-20T14:55:11Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.5088,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No data appears",
        "sections": "ECS <em>integration</em> troubleshooting: No data appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, use the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    }
  ],
  "/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-generate-verbose-logs": [
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-06-20T14:55:11Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.08029,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> <em>troubleshooting</em>: No data appears",
        "sections": "ECS <em>integration</em> <em>troubleshooting</em>: No data appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". <em>Troubleshoot</em> in the UI To use the UI to <em>troubleshoot</em>: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, use the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    },
    {
      "sections": [
        "Introduction to the Amazon ECS integration",
        "Features",
        "Important",
        "Compatibility and requirements",
        "Install",
        "Check the source code"
      ],
      "title": "Introduction to the Amazon ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Get started"
      ],
      "external_id": "a2af5484b25f8595032cc1937210c9a41024a138",
      "image": "https://docs.newrelic.com/static/986bdb22950fdd8b222a850e205882a9/c1b63/new-relic-ecs-integration-dashboards_0.png",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration/",
      "published_at": "2021-06-20T23:01:36Z",
      "updated_at": "2021-03-30T21:13:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our ECS integration reports and displays performance data from your Amazon ECS environment. The ECS integration works well with other integrations, so you can also monitor services running on ECS. Features Amazon Elastic Container Service (ECS) is a scalable container management service that makes it easy to run, stop, and manage Docker containers on Amazon EC2 clusters. Our ECS integration instruments the underlying container instance (EC2 launch type) and the container layer by reporting metrics from ECS objects. The integration gives you insight into your ECS instances, tasks, services, and containers. one.newrelic.com > Explorer > ECS dashboard: The ECS integration reports performance data about your Amazon ECS containers. Features include: View your data in pre-built dashboards for immediate insight into your ECS environment. Create your own queries and charts in the query builder from automatically reported data. Create alert conditions on ECS data. Explore entities using the New Relic Explorer. Important New Relic also offers an ECS cloud integration, which reports a different data set than our on-host integration. For complete ECS monitoring, we recommend enabling both integrations. Compatibility and requirements Requirements: Amazon ECS container agent 1.21.0 or higher. Windows not supported. This integration uses our infrastructure agent and our Docker instrumentation: applicable requirements and restrictions of those systems apply. Install To install, see Install integration. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.7157,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Amazon ECS <em>integration</em>",
        "sections": "Introduction to the Amazon ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "Our ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. The ECS <em>integration</em> works well with other <em>integrations</em>, so you can also monitor services running on ECS. Features Amazon <em>Elastic</em> <em>Container</em> <em>Service</em> (ECS) is a scalable <em>container</em> management <em>service</em> that makes"
      },
      "id": "603eb04b196a6752b5a83dc8"
    },
    {
      "sections": [
        "Install the ECS integration",
        "Tip",
        "Install overview",
        "Install using CloudFormation",
        "EC2 launch type",
        "Fargate launch type",
        "Install with automatic script",
        "Manual install",
        "AWS resources created"
      ],
      "title": "Install the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "857b78b6e7de76449f3f9569cee4700705b7d7fe",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/install-ecs-integration/",
      "published_at": "2021-06-20T23:01:35Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's ECS integration reports and displays performance data from your Amazon ECS environment. This document explains how to install this integration. Tip To use ECS integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Install overview Before you install our ECS integration, we recommend reviewing the requirements. Here's a brief overview of what happens during the install process: For EC2 launch type: The infrastructure agent (newrelic-infra) gets deployed onto an ECS cluster as a service using the daemon scheduling strategy. This deployment installs the infrastructure agent in all the container instances of the cluster. The infrastructure agent then monitors ECS and Docker containers. For Fargate launch type: The infrastructure agent (newrelic-infra) gets deployed as a sidecar in every task to monitor. Install options: Install using AWS CloudFormation Install using automatic script Install manually Install using CloudFormation One install option is using AWS CloudFormation. We provide some CloudFormation templates that install the ECS integration onto your AWS account for both EC2 and Fargate launch types: To register the New Relic's ECS integration task, deploy this stack. Ensure you’re deploying the stack to your desired region(s). This stack creates the following resources: A secret that stores the license key. A policy to access the license key. An instance role to be used as an ECS task ExecutionRole, with access to the license key. For EC2 launch type: Registers the New Relic Infrastructure ECS integration task. Follow the additional instructions for your launch type: EC2 launch type Additional steps for EC2 launch type: To create a service that runs the task on every container instance, deploy this stack. Fargate launch type Additional steps for Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Install with automatic script One install option is using our install script. To use the automatic install script: Download the ECS integration installer: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-installer.sh Copy Add execute permissions to the installer: chmod +x newrelic-infra-ecs-installer.sh Copy Execute it with -h to see the documentation and requirements: ./newrelic-infra-ecs-installer.sh -h Copy Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Execute the installer, specifying your license key and cluster name. EC2 launch type: ./newrelic-infra-ecs-installer.sh -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Fargate launch type: ./newrelic-infra-ecs-installer.sh -fargate -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Additional steps for Fargate launch type (not EC2 launch type): Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the single container in this task definition as a sidecar to the task definitions you want monitored. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Manual install One install option is to manually do the steps that are done by the automatic installer script. We will describe how this is done using the awscli tool: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Save your New Relic license key as a Systems Manager (SSM) parameter: aws ssm put-parameter \\ --name \"/newrelic-infra/ecs/license-key\" \\ --type SecureString \\ --description 'New Relic license key for ECS monitoring' \\ --value \"NEW_RELIC_LICENSE_KEY\" Copy Create an IAM policy to access the license key parameter: aws iam create-policy \\ --policy-name \"NewRelicSSMLicenseKeyReadAccess\" \\ --policy-document \"{\"Version\"\\\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"ssm:GetParameters\"],\"Resource\":[\"ARN_OF_LICENSE_KEY_PARAMETER\"]}]}\" --description \"Provides read access to the New Relic SSM license key parameter\" Copy Create an IAM role to be used as the task execution role: aws iam create-role \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --assume-role-policy-document '{\"Version\":\"2008-10-17\",\"Statement\":[{\"Sid\":\"\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"ecs-tasks.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}' \\ --description \"ECS task execution role for New Relic infrastructure\" Copy Attach the policies NewRelicSSMLicenseKeyReadAccess, AmazonEC2ContainerServiceforEC2Role, and AmazonECSTaskExecutionRolePolicy to the role: aws iam attach-role-policy \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --policy-arn \"POLICY_ARN\" Copy Choose your launch type for more instructions: EC2 launch type Additional steps for EC2 launch type: Download the New Relic ECS integration task definition template file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-ec2-latest.json Copy Replace the task execution role in the template file with the newly created role: \"executionRoleArn\": \"NewRelicECSTaskExecutionRole\", Copy Replace the valueFrom attribute of the secret with the name of the Systems Manager parameter: secrets\": [ { \"valueFrom\": \"/newrelic-infra/ecs/license-key\", \"name\": \"NRIA_LICENSE_KEY\" } ], Copy Register the task definition file: aws ecs register-task-definition --cli-input-json file://newrelic-infra-ecs-ec2-latest.json Copy Create a service with the daemon scheduling strategy for the registered task: aws ecs create-service --cluster \"YOUR_CLUSTER_NAME\" --service-name \"newrelic-infra\" --task-definition \"newrelic-infra\" --scheduling-strategy DAEMON Copy Fargate launch type Additional steps for the Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, a separate integration which gets you supplementary ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. AWS resources created When you install the ECS integration using default/recommended values, it does the following in AWS: Creates Systems Manager (SSM) parameter /newrelic-infra/ecs/license-key. This system parameter contains the New Relic license key. Creates IAM policy NewRelicSSMLicenseKeyReadAccess, which enables access to the SSM parameter with the license key. Creates IAM role NewRelicECSTaskExecutionRole used as the task execution role. Policies attached to the role: NewRelicSSMLicenseKeyReadAccess (created by the installer). AmazonEC2ContainerServiceforEC2Role AmazonECSTaskExecutionRolePolicy For EC2 launch type, this is also done: Registers the newrelic-infra ECS task definition. Creates the service newrelic-infra for the registered task using a daemon scheduling strategy.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 138.32169,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the ECS <em>integration</em>",
        "sections": "Install the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. This document explains how to install this <em>integration</em>. Tip To use ECS <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign"
      },
      "id": "603e9e76196a676684a83de9"
    }
  ],
  "/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears": [
    {
      "sections": [
        "ECS integration troubleshooting: Generate verbose logs",
        "Problem",
        "Solution",
        "Using task definition environment variable",
        "Retrieve logs via SSH (EC2 launch type only)",
        "Forward logs to CloudWatch and download them with awscli",
        "From running container"
      ],
      "title": "ECS integration troubleshooting: Generate verbose logs",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "06198f1b2e0faa69bd8a7dfb93f18c8955fea83b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-generate-verbose-logs/",
      "published_at": "2021-06-20T23:02:25Z",
      "updated_at": "2021-03-13T03:35:43Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When troubleshooting the on-host ECS integration, you can generate verbose logs for a few minutes to find and investigate errors. This can be useful for conducting your own troubleshooting or when providing information to New Relic support. Verbose logging generates a lot of data very quickly. When finished generating logs, be sure to set verbose: 0 to reduce disk space consumption. You can automate this process by using the newrelic-infra-ctl command. For more information, see Troubleshooting a running agent. Solution Generating verbose log files requires editing your task definition file. For a sample config file that includes all applicable settings, see Infrastructure configuration settings. You have several options for implementing verbose logs: Change the task definition environment variable and do a task restart For EC2 launch type: Retrieve logs via SSH Forward to CloudWatch and download with awscli Run a command from the running container Using task definition environment variable To enable verbose logs by changing the environment variable and doing a task restart: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. Save your task definition. Update your service to use the newly registered task definition. If you chose NRIA_VERBOSE=3 and you're not sending the logs directly to New Relic, you have two options for viewing and downloading the logs: For EC2 launch type: you can retrieve the logs via SSH, or Forward logs to CloudWatch Return your settings to default: Disable verbose logging by editing your task definition and setting NRIA_VERBOSE to 0. Save your task definition. Update your service to the latest version of your task. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log file that contains the ECS integration version: New Relic ECS integration version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your task definition .yml file. Retrieve logs via SSH (EC2 launch type only) To get logs via SSH: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. SSH into one of your container instances. Find the container ID of the New Relic integration container, by running the command docker ps -a. The name of the container should be nri-ecs. Save the logs from the container with the command docker logs NRI_ECS_CONTAINER_ID > logs.txt. Leave the command running for about three minutes to generate sufficient logging data. Continue with the instructions in the enable verbose logs section. Forward logs to CloudWatch and download them with awscli To get logs via CloudWatch: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. We use a CloudWatch log group called /newrelic-infra/ecs to forward the logs to. To see if it already exists, run: aws logs describe-log-groups --log-group-name-prefix /newrelic-infra/ecs Copy If a log group exists with that prefix, you'll get this output: { \"logGroups\": [ { \"logGroupName\": \"/newrelic-infra/ecs\", \"creationTime\": 1585828615225, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:YOUR_REGION:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:*\", \"storedBytes\": 122539356 } ] } Copy Because this command matches log groups with prefixes, ensure the log group name returned is exactly /newrelic-infra/ecs. If the log group doesn't exist, the output will be: { \"logGroups\": [] } Copy If the log group doesn't exist, create it by running: aws logs create-log-group --log-group-name /newrelic-infra/ecs Copy Edit your task definition. In the container definition for the newrelic-infra container, add the following logConfiguration: \"logConfiguration\": { \"logDriver\": \"awslogs\", \"options\": { \"awslogs-group\": \"/newrelic-infra/ecs\", \"awslogs-region\": \"AWS_REGION_OF_YOUR_CLUSTER\", \"awslogs-stream-prefix\": \"verbose\" } } Copy Register the new task version and update your service. Next you'll look for the relevant log stream. If you have multiple instances of the task running, they'll all send their logs to the same log group but each will have its own log stream. Log streams names follow the structure AWSLOGS_STREAM_PREFIX/TASK_FAMILY_NAME/TASK_ID. In this case, it will be verbose/newrelic-infra/TASK_ID. To get all the log streams for a given log group, run this command: aws logs describe-log-streams --log-group-name /newrelic-infra/ecs Copy The following is an example output of a log group with two streams: { \"logStreams\": [ { \"logStreamName\": \"verbose/newrelic-infra/9dfb28114e40415ebc399ec1e53a21b7\", \"creationTime\": 1586166741197, \"firstEventTimestamp\": 1586166742030, \"lastEventTimestamp\": 1586173933472, \"lastIngestionTime\": 1586175101220, \"uploadSequenceToken\": \"49599989655680038369205623273330095416487086853777112338\", \"arn\": \"arn:aws:logs:AWS_REGION_OF_YOUR_CLUSTER:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:log-stream:verbose/newrelic-infra/9dfb28114e40415ebc399ec1e53a21b7\", \"storedBytes\": 0 }, { \"logStreamName\": \"verbose/newrelic-infra/f6ce0be416804bc4bfa658da5514eb00\", \"creationTime\": 1586166745643, \"firstEventTimestamp\": 1586166746491, \"lastEventTimestamp\": 1586173037927, \"lastIngestionTime\": 1586175100660, \"uploadSequenceToken\": \"49605664273821671319096446647846424799651902350804230514\", \"arn\": \"arn:aws:logs:AWS_REGION_OF_YOUR_CLUSTER:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:log-stream:verbose/newrelic-infra/f6ce0be416804bc4bfa658da5514eb00\", \"storedBytes\": 0 } ] } Copy From the previous list of log streams, identify the one with the task ID for which you want to retrieve the logs and use the logStreamName in this command: aws logs get-log-events --log-group-name /newrelic-infra/ecs --log-stream-name \"LOG_STREAM_NAME\" --output text > logs.txt Copy Continue with the enable verbose logs instructions. From running container To enable verbose logs by running a command from the running container: SSH into one of your container instances. Find the container ID of the New Relic integration container by running the command docker ps -a. The name of the container should be nri-ecs. Enable verbose logs for a limited period of time by using newrelic-infra-ctl. Run the command: docker exec INTEGRATION_CONTAINER_ID /usr/bin/newrelic-infra-ctl Copy For more details, see Troubleshoot the agent. Save the logs from the container with the command docker logs INTEGRATION_CONTAINER_ID > logs.txt Copy Leave the command running for about three minutes to generate sufficient logging data. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log file that contains the ECS integration version: New Relic ECS integration version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your task definition .yml file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 156.77708,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> <em>troubleshooting</em>: Generate verbose logs",
        "sections": "ECS <em>integration</em> <em>troubleshooting</em>: Generate verbose logs",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": " by running the command docker ps -a. The name of the <em>container</em> should be nri-ecs. Enable verbose logs for a limited period of time by using newrelic-infra-ctl. Run the command: docker exec <em>INTEGRATION_CONTAINER</em>_ID &#x2F;usr&#x2F;bin&#x2F;newrelic-infra-ctl Copy For more details, see <em>Troubleshoot</em> the agent. Save"
      },
      "id": "604507f9196a67c1ae960f5e"
    },
    {
      "sections": [
        "Introduction to the Amazon ECS integration",
        "Features",
        "Important",
        "Compatibility and requirements",
        "Install",
        "Check the source code"
      ],
      "title": "Introduction to the Amazon ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Get started"
      ],
      "external_id": "a2af5484b25f8595032cc1937210c9a41024a138",
      "image": "https://docs.newrelic.com/static/986bdb22950fdd8b222a850e205882a9/c1b63/new-relic-ecs-integration-dashboards_0.png",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration/",
      "published_at": "2021-06-20T23:01:36Z",
      "updated_at": "2021-03-30T21:13:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our ECS integration reports and displays performance data from your Amazon ECS environment. The ECS integration works well with other integrations, so you can also monitor services running on ECS. Features Amazon Elastic Container Service (ECS) is a scalable container management service that makes it easy to run, stop, and manage Docker containers on Amazon EC2 clusters. Our ECS integration instruments the underlying container instance (EC2 launch type) and the container layer by reporting metrics from ECS objects. The integration gives you insight into your ECS instances, tasks, services, and containers. one.newrelic.com > Explorer > ECS dashboard: The ECS integration reports performance data about your Amazon ECS containers. Features include: View your data in pre-built dashboards for immediate insight into your ECS environment. Create your own queries and charts in the query builder from automatically reported data. Create alert conditions on ECS data. Explore entities using the New Relic Explorer. Important New Relic also offers an ECS cloud integration, which reports a different data set than our on-host integration. For complete ECS monitoring, we recommend enabling both integrations. Compatibility and requirements Requirements: Amazon ECS container agent 1.21.0 or higher. Windows not supported. This integration uses our infrastructure agent and our Docker instrumentation: applicable requirements and restrictions of those systems apply. Install To install, see Install integration. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.7157,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Amazon ECS <em>integration</em>",
        "sections": "Introduction to the Amazon ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "Our ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. The ECS <em>integration</em> works well with other <em>integrations</em>, so you can also monitor services running on ECS. Features Amazon <em>Elastic</em> <em>Container</em> <em>Service</em> (ECS) is a scalable <em>container</em> management <em>service</em> that makes"
      },
      "id": "603eb04b196a6752b5a83dc8"
    },
    {
      "sections": [
        "Install the ECS integration",
        "Tip",
        "Install overview",
        "Install using CloudFormation",
        "EC2 launch type",
        "Fargate launch type",
        "Install with automatic script",
        "Manual install",
        "AWS resources created"
      ],
      "title": "Install the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "857b78b6e7de76449f3f9569cee4700705b7d7fe",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/install-ecs-integration/",
      "published_at": "2021-06-20T23:01:35Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's ECS integration reports and displays performance data from your Amazon ECS environment. This document explains how to install this integration. Tip To use ECS integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Install overview Before you install our ECS integration, we recommend reviewing the requirements. Here's a brief overview of what happens during the install process: For EC2 launch type: The infrastructure agent (newrelic-infra) gets deployed onto an ECS cluster as a service using the daemon scheduling strategy. This deployment installs the infrastructure agent in all the container instances of the cluster. The infrastructure agent then monitors ECS and Docker containers. For Fargate launch type: The infrastructure agent (newrelic-infra) gets deployed as a sidecar in every task to monitor. Install options: Install using AWS CloudFormation Install using automatic script Install manually Install using CloudFormation One install option is using AWS CloudFormation. We provide some CloudFormation templates that install the ECS integration onto your AWS account for both EC2 and Fargate launch types: To register the New Relic's ECS integration task, deploy this stack. Ensure you’re deploying the stack to your desired region(s). This stack creates the following resources: A secret that stores the license key. A policy to access the license key. An instance role to be used as an ECS task ExecutionRole, with access to the license key. For EC2 launch type: Registers the New Relic Infrastructure ECS integration task. Follow the additional instructions for your launch type: EC2 launch type Additional steps for EC2 launch type: To create a service that runs the task on every container instance, deploy this stack. Fargate launch type Additional steps for Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Install with automatic script One install option is using our install script. To use the automatic install script: Download the ECS integration installer: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-installer.sh Copy Add execute permissions to the installer: chmod +x newrelic-infra-ecs-installer.sh Copy Execute it with -h to see the documentation and requirements: ./newrelic-infra-ecs-installer.sh -h Copy Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Execute the installer, specifying your license key and cluster name. EC2 launch type: ./newrelic-infra-ecs-installer.sh -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Fargate launch type: ./newrelic-infra-ecs-installer.sh -fargate -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Additional steps for Fargate launch type (not EC2 launch type): Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the single container in this task definition as a sidecar to the task definitions you want monitored. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Manual install One install option is to manually do the steps that are done by the automatic installer script. We will describe how this is done using the awscli tool: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Save your New Relic license key as a Systems Manager (SSM) parameter: aws ssm put-parameter \\ --name \"/newrelic-infra/ecs/license-key\" \\ --type SecureString \\ --description 'New Relic license key for ECS monitoring' \\ --value \"NEW_RELIC_LICENSE_KEY\" Copy Create an IAM policy to access the license key parameter: aws iam create-policy \\ --policy-name \"NewRelicSSMLicenseKeyReadAccess\" \\ --policy-document \"{\"Version\"\\\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"ssm:GetParameters\"],\"Resource\":[\"ARN_OF_LICENSE_KEY_PARAMETER\"]}]}\" --description \"Provides read access to the New Relic SSM license key parameter\" Copy Create an IAM role to be used as the task execution role: aws iam create-role \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --assume-role-policy-document '{\"Version\":\"2008-10-17\",\"Statement\":[{\"Sid\":\"\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"ecs-tasks.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}' \\ --description \"ECS task execution role for New Relic infrastructure\" Copy Attach the policies NewRelicSSMLicenseKeyReadAccess, AmazonEC2ContainerServiceforEC2Role, and AmazonECSTaskExecutionRolePolicy to the role: aws iam attach-role-policy \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --policy-arn \"POLICY_ARN\" Copy Choose your launch type for more instructions: EC2 launch type Additional steps for EC2 launch type: Download the New Relic ECS integration task definition template file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-ec2-latest.json Copy Replace the task execution role in the template file with the newly created role: \"executionRoleArn\": \"NewRelicECSTaskExecutionRole\", Copy Replace the valueFrom attribute of the secret with the name of the Systems Manager parameter: secrets\": [ { \"valueFrom\": \"/newrelic-infra/ecs/license-key\", \"name\": \"NRIA_LICENSE_KEY\" } ], Copy Register the task definition file: aws ecs register-task-definition --cli-input-json file://newrelic-infra-ecs-ec2-latest.json Copy Create a service with the daemon scheduling strategy for the registered task: aws ecs create-service --cluster \"YOUR_CLUSTER_NAME\" --service-name \"newrelic-infra\" --task-definition \"newrelic-infra\" --scheduling-strategy DAEMON Copy Fargate launch type Additional steps for the Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, a separate integration which gets you supplementary ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. AWS resources created When you install the ECS integration using default/recommended values, it does the following in AWS: Creates Systems Manager (SSM) parameter /newrelic-infra/ecs/license-key. This system parameter contains the New Relic license key. Creates IAM policy NewRelicSSMLicenseKeyReadAccess, which enables access to the SSM parameter with the license key. Creates IAM role NewRelicECSTaskExecutionRole used as the task execution role. Policies attached to the role: NewRelicSSMLicenseKeyReadAccess (created by the installer). AmazonEC2ContainerServiceforEC2Role AmazonECSTaskExecutionRolePolicy For EC2 launch type, this is also done: Registers the newrelic-infra ECS task definition. Creates the service newrelic-infra for the registered task using a daemon scheduling strategy.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 138.32169,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the ECS <em>integration</em>",
        "sections": "Install the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. This document explains how to install this <em>integration</em>. Tip To use ECS <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign"
      },
      "id": "603e9e76196a676684a83de9"
    }
  ],
  "/docs/integrations/elastic-container-service-integration/understand-use-data/ecs-integration-recommended-alert-conditions": [
    {
      "sections": [
        "Understand and use ECS data",
        "View data",
        "Query your data"
      ],
      "title": "Understand and use ECS data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Understand use data"
      ],
      "external_id": "16689cc080d4a8482e802b404df9ae45c4283db2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/understand-use-data/understand-use-ecs-data/",
      "published_at": "2021-06-20T23:03:14Z",
      "updated_at": "2021-03-29T20:30:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Here we explain how to find, understand, and use the data reported by this integration. View data To view the ECS integration dashboard: Go to one.newrelic.com and select Explorer. On the left, search for ECS clusters, or type the name of your ECS cluster in the search bar. To view a dashboard, select the entity name corresponding to your ECS cluster. In addition to the pre-built dashboards, you can also create your own custom queries and charts using the query builder. To learn how to query this data, see Understand data. Query your data Data reported by this integration is displayed in its dashboards and is also available for querying and the creation of custom charts and dashboards. This integration reports an EcsClusterSample event, with attributes clusterName and arn. Other types of data that may be available for querying: Infrastructure agent-reported events, including Docker All the events reported from an ECS cluster contain the attributes ecsClusterName and ecsClusterArn. Here's an example NRQL query that returns the count of containers associated with each Docker image in an ECS cluster named MyClusterName created in us-east-1: SELECT uniqueCount(containerId) FROM ContainerSample WHERE awsRegion = 'us-east-1' AND ecsClusterName = 'MyClusterName' FACET imageName SINCE 1 HOUR AGO Copy To learn more about creating custom queries and charts: How to query New Relic data Introduction to NRQL",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.70541,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Understand</em> and <em>use</em> ECS <em>data</em>",
        "sections": "<em>Understand</em> and <em>use</em> ECS <em>data</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance <em>data</em> from your Amazon ECS environment. Here we explain how to find, <em>understand</em>, and <em>use</em> the <em>data</em> reported by this <em>integration</em>. View <em>data</em> To view the ECS <em>integration</em> dashboard: Go to one.newrelic.com and select Explorer"
      },
      "id": "603e9eb664441fbaad4e889f"
    },
    {
      "sections": [
        "Introduction to the Amazon ECS integration",
        "Features",
        "Important",
        "Compatibility and requirements",
        "Install",
        "Check the source code"
      ],
      "title": "Introduction to the Amazon ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Get started"
      ],
      "external_id": "a2af5484b25f8595032cc1937210c9a41024a138",
      "image": "https://docs.newrelic.com/static/986bdb22950fdd8b222a850e205882a9/c1b63/new-relic-ecs-integration-dashboards_0.png",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration/",
      "published_at": "2021-06-20T23:01:36Z",
      "updated_at": "2021-03-30T21:13:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our ECS integration reports and displays performance data from your Amazon ECS environment. The ECS integration works well with other integrations, so you can also monitor services running on ECS. Features Amazon Elastic Container Service (ECS) is a scalable container management service that makes it easy to run, stop, and manage Docker containers on Amazon EC2 clusters. Our ECS integration instruments the underlying container instance (EC2 launch type) and the container layer by reporting metrics from ECS objects. The integration gives you insight into your ECS instances, tasks, services, and containers. one.newrelic.com > Explorer > ECS dashboard: The ECS integration reports performance data about your Amazon ECS containers. Features include: View your data in pre-built dashboards for immediate insight into your ECS environment. Create your own queries and charts in the query builder from automatically reported data. Create alert conditions on ECS data. Explore entities using the New Relic Explorer. Important New Relic also offers an ECS cloud integration, which reports a different data set than our on-host integration. For complete ECS monitoring, we recommend enabling both integrations. Compatibility and requirements Requirements: Amazon ECS container agent 1.21.0 or higher. Windows not supported. This integration uses our infrastructure agent and our Docker instrumentation: applicable requirements and restrictions of those systems apply. Install To install, see Install integration. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.70364,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Amazon ECS <em>integration</em>",
        "sections": "Introduction to the Amazon ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "Our ECS <em>integration</em> reports and displays performance <em>data</em> from your Amazon ECS environment. The ECS <em>integration</em> works well with other <em>integrations</em>, so you can also monitor services running on ECS. Features Amazon <em>Elastic</em> <em>Container</em> <em>Service</em> (ECS) is a scalable <em>container</em> management <em>service</em> that makes"
      },
      "id": "603eb04b196a6752b5a83dc8"
    },
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-06-20T14:55:11Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.50879,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No <em>data</em> appears",
        "sections": "ECS <em>integration</em> troubleshooting: No <em>data</em> appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To <em>use</em> the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, <em>use</em> the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    }
  ],
  "/docs/integrations/elastic-container-service-integration/understand-use-data/understand-use-ecs-data": [
    {
      "sections": [
        "Recommended ECS alert conditions",
        "Recommended alert conditions"
      ],
      "title": "Recommended ECS alert conditions",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Understand use data"
      ],
      "external_id": "334d80a75b3ef0a7b6125bf2a15f643ea46d7282",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/understand-use-data/ecs-integration-recommended-alert-conditions/",
      "published_at": "2021-06-20T23:02:24Z",
      "updated_at": "2021-03-16T05:41:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's ECS integration reports and displays performance data from your Amazon ECS environment. This document provides some recommended alert conditions for monitoring ECS performance. Recommended alert conditions Here are some recommended ECS alert conditions. To add these alerts, go to the Alerts UI and add the following NRQL alert conditions to an existing or new alert policy: High CPU usage NRQL: FROM ContainerSample SELECT cpuUsed / cpuLimitCores Critical: > 90% for 5 minutes High memory usage NRQL: FROM ContainerSample SELECT memoryUsageBytes / memorySizeLimitBytes Critical: > 80% for 5 minutes Restart count NRQL: FROM ContainerSample SELECT max(restartCount) - min(restartCount) Critical: > 5 for 5 minutes",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 209.69214,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s ECS <em>integration</em> reports and displays performance <em>data</em> from your Amazon ECS environment. This document provides some recommended alert conditions for monitoring ECS performance. Recommended alert conditions Here are some recommended ECS alert conditions. To add these alerts, go"
      },
      "id": "603e7eee64441f0f674e889f"
    },
    {
      "sections": [
        "Introduction to the Amazon ECS integration",
        "Features",
        "Important",
        "Compatibility and requirements",
        "Install",
        "Check the source code"
      ],
      "title": "Introduction to the Amazon ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Get started"
      ],
      "external_id": "a2af5484b25f8595032cc1937210c9a41024a138",
      "image": "https://docs.newrelic.com/static/986bdb22950fdd8b222a850e205882a9/c1b63/new-relic-ecs-integration-dashboards_0.png",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration/",
      "published_at": "2021-06-20T23:01:36Z",
      "updated_at": "2021-03-30T21:13:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our ECS integration reports and displays performance data from your Amazon ECS environment. The ECS integration works well with other integrations, so you can also monitor services running on ECS. Features Amazon Elastic Container Service (ECS) is a scalable container management service that makes it easy to run, stop, and manage Docker containers on Amazon EC2 clusters. Our ECS integration instruments the underlying container instance (EC2 launch type) and the container layer by reporting metrics from ECS objects. The integration gives you insight into your ECS instances, tasks, services, and containers. one.newrelic.com > Explorer > ECS dashboard: The ECS integration reports performance data about your Amazon ECS containers. Features include: View your data in pre-built dashboards for immediate insight into your ECS environment. Create your own queries and charts in the query builder from automatically reported data. Create alert conditions on ECS data. Explore entities using the New Relic Explorer. Important New Relic also offers an ECS cloud integration, which reports a different data set than our on-host integration. For complete ECS monitoring, we recommend enabling both integrations. Compatibility and requirements Requirements: Amazon ECS container agent 1.21.0 or higher. Windows not supported. This integration uses our infrastructure agent and our Docker instrumentation: applicable requirements and restrictions of those systems apply. Install To install, see Install integration. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.70363,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Amazon ECS <em>integration</em>",
        "sections": "Introduction to the Amazon ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "Our ECS <em>integration</em> reports and displays performance <em>data</em> from your Amazon ECS environment. The ECS <em>integration</em> works well with other <em>integrations</em>, so you can also monitor services running on ECS. Features Amazon <em>Elastic</em> <em>Container</em> <em>Service</em> (ECS) is a scalable <em>container</em> management <em>service</em> that makes"
      },
      "id": "603eb04b196a6752b5a83dc8"
    },
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-06-20T14:55:11Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but you’re still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.50879,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No <em>data</em> appears",
        "sections": "ECS <em>integration</em> troubleshooting: No <em>data</em> appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To <em>use</em> the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, <em>use</em> the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-app-engine-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-06-20T23:04:49Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.37698,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-06-20T19:23:11Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.52708,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-06-20T14:56:57Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.3325,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-bigquery-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-06-20T23:04:49Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.37698,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-06-20T19:23:11Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.52708,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-06-20T14:56:57Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.3325,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-bigtable-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-06-20T23:04:49Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.37698,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-06-20T19:23:11Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.52708,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-06-20T14:56:57Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.3325,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-06-20T23:04:49Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.37697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-06-20T19:23:11Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.52707,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-06-20T14:56:57Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.3325,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-dataflow-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-06-20T23:04:49Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.37697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-06-20T19:23:11Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.52707,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-06-20T14:56:57Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.3325,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-dataproc-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-06-20T23:04:49Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.37697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-06-20T19:23:11Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.52707,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-06-20T14:56:57Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.3325,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firebase-database-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-06-20T23:04:49Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.37697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-06-20T19:23:11Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.52707,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-06-20T14:56:57Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.3325,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firebase-hosting-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-06-20T23:04:49Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.37695,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-06-20T19:23:11Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.52707,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-06-20T14:56:57Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.3325,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firebase-storage-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-06-20T23:04:49Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.37695,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-06-20T19:23:11Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.52707,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-06-20T14:56:57Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.3325,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firestore-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-06-20T23:04:49Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.37695,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-06-20T19:23:11Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.52707,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-06-20T14:56:57Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.3325,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    }
  ]
}